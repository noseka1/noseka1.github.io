<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Ales Nosek - The Software Practitioner]]></title>
  <link href="http://alesnosek.com/atom.xml" rel="self"/>
  <link href="http://alesnosek.com/"/>
  <updated>2018-03-08T20:11:04-08:00</updated>
  <id>http://alesnosek.com/</id>
  <author>
    <name><![CDATA[Ales Nosek]]></name>

  </author>
  <generator uri="http://octopress.org/">Octopress</generator>


  <entry>
    <title type="html"><![CDATA[18 Months with OpenStack, Our Experience, Part I]]></title>
    <link href="http://alesnosek.com/blog/2018/02/19/18-months-with-openstack-our-experience-part-i/"/>
    <updated>2018-02-19T21:06:17-08:00</updated>
    <id>http://alesnosek.com/blog/2018/02/19/18-months-with-openstack-our-experience-part-i</id>
    <content type="html"><![CDATA[<p>It has been 18 months since we deployed OpenStack cloud in our company. In this article, I would like to review our time with OpenStack and describe some of the experience we gained. Are you thinking about building an OpenStack-based cloud? This article might provide you with additional insights and tips that will help you succeed.</p>

<!-- more -->


<h2>Introduction</h2>

<p>Right at the beginning, I&rsquo;d like to say that our company is not a cloud provider. We didn&rsquo;t build a cloud to provide a service to customers. Instead, we built a private cloud to support our engineering team in their software development efforts. Our OpenStack is running development machines, build machines and test machines. The requirements on the availability and reliability of our OpenStack cluster are therefore lower than the requirements that a public cloud would have to meet.</p>

<p>We started evaluating OpenStack around the Kilo release and ended up going with Mitaka into production. Around that time, Internet was flooded with articles criticizing the complexity of OpenStack including scary stories of companies failing their OpenStack projects. I can confirm that deploying OpenStack was a real challenge and that as we were working on it, the OpenStack version of the Kennedy&rsquo;s famous words passed through my mind several times:</p>

<blockquote><p>We choose to deploy OpenStack not because it is easy, but because it is hard.</p></blockquote>

<p>However, nothing can scare away a proficent software practitioner. Eventually, we got the job done and the invested effort did pay off.</p>

<h2>OpenStack essentials</h2>

<p><a href="https://www.openstack.org/">OpenStack</a> is an open-source infrastructure-as-a-service (IaaS) cloud platform. It&rsquo;s important to understand that OpenStack itself is only a controlling layer that relies on other software projects to provide the implementation of the underlying functionality. For instance, OpenStack can spin up virtual machines, however, you would not find any code in the OpenStack project that would actually implement a hypervisor. Instead, OpenStack integrates with an existing hypervisor software to do the job. A very popular choice of the hypervisor used along with OpenStack is <a href="https://www.linux-kvm.org">KVM</a> and so when the user creates a virtual machine on OpenStack, OpenStack merely calls the KVM hypervisor to spin up the virtual machine. The same holds true for other areas of OpenStack functionality like networking and storage where OpenStack drives the underlying networking and storage software to do the actual job.</p>

<p>When planning an OpenStack cluster, you will have to choose from a variety of underlying technologies. For instance, in addition to the KVM hypervisor, OpenStack also supports Xen, VMware vSphere and Hyper-V. Most of the time you will just pick the technology you already run at your place and for which you have the staff to manage it. The freedom of choice you have with OpenStack is amazing, however, I would recommend to always look at the most popular choices first because their integration with OpenStack tends to be more solid. In our case, we chose KVM as a hypervisor, <a href="http://www.openvswitch.org/">Open vSwitch</a> as a networking implementation and <a href="https://ceph.com/">Ceph</a> to provide object and block storage.</p>

<p>OpenStack is an umbrella project under which you can find a host of projects each dealing with a different portion of the cloud functionality. The core projects that can find installed in the majority of OpenStack deployments are:</p>

<ul>
<li><a href="https://wiki.openstack.org/wiki/Nova">Nova</a>. Manages virtual machines.</li>
<li><a href="https://wiki.openstack.org/wiki/Neutron">Neutron</a>. Provides networking to virtual machines.</li>
<li><a href="https://wiki.openstack.org/wiki/Cinder">Cinder</a>. Provides block storage that can be attached to virtual machines.</li>
<li><a href="https://wiki.openstack.org/wiki/Glance">Glance</a>. Stores virtual machine images.</li>
<li><a href="https://wiki.openstack.org/wiki/Horizon">Horizon</a>. Web-based user interface to OpenStack services.</li>
<li><a href="https://wiki.openstack.org/wiki/Keystone">Keystone</a>. Identity service.</li>
</ul>


<p>Additionally, project <a href="https://wiki.openstack.org/wiki/Heat">Heat</a>, <a href="https://wiki.openstack.org/wiki/Swift">Swift</a>, and <a href="https://wiki.openstack.org/wiki/Telemetry">Ceilometer</a> are also rather popular. You can find plenty of other OpenStack projects listed on the <a href="https://www.openstack.org/software/project-navigator">Project navigator</a> page. When choosing OpenStack projects for your deployment, you should always consider the adoption and the maturity of the projects. Many projects on the list are still in the early stages of development and not ready for production use.</p>

<p>OpenStack was designed for massive scale deployments as you can tell if you look at the <a href="https://docs.openstack.org/arch-design/design.html">OpenStack architecture diagram</a>. Each OpenStack project consists of multiple services (daemons) that can be deployed on separate physical machines allowing OpenStack to scale out. OpenStack services communicate with each other over the network using RESTful APIs. In addition, some of the projects like Nova, Neutron and Cinder chose to leverage a message broker for internal communication. The high number of services that form an OpenStack deployment contributes to its operational complexity.</p>

<h2>Getting started with OpenStack</h2>

<p>If you are new to OpenStack, a great place to start learning OpenStack is the <a href="https://docs.openstack.org/devstack/latest/">DevStack project</a>. DevStack allows you to create an all-in-one deployment of OpenStack. With DevStack you can access debug logs of individual OpenStack services as well as easily restart OpenStack services after you changed their configuration. It took me a while to figure out which configuration option affects which OpenStack service and how OpenStack services communicate with each other. I learned a lot by re-deploying DevStack many times, trying to make the individual OpenStack features work properly.</p>

<p>OpenStack is a fast moving project with two major releases per year. Especially in the past, the project documentation could not keep up with the many changes packed in each release. The documentation was outdated on many places or was missing altogether. When working with OpenStack I quickly realized that reading the OpenStack&rsquo;s Python code was necessary in order to understand how some of the configuration options worked or when troubleshooting various issues.</p>

<blockquote><p> Ability to read the OpenStack source code was required to succeed.</p></blockquote>

<p>OpenStack is written using a beautiful idiomatic Python code which was most of the time a pleasure to read. At first, I started walking through the code of simpler projects like Glance and learned the patterns that were commonly used in other OpenStack projects, too. Only later I dived deeper into the internals of Nova, the OpenStack&rsquo;s brain that schedules and creates virtual machines. When between OpenStack releases configuration options were renamed or moved to different INI file sections, I just grepped through the source code and learned about the changes avoiding any further frustration.</p>

<p>If you are getting started with OpenStack, prepare for a steep learning curve. Apart from studying the OpenStack project documentation, you will have to refer to the documentation of the technologies that you integrate with OpenStack, too. For instance, I spend quite a bit of time studying the documentation of <a href="https://www.rabbitmq.com/documentation.html">RabbitMQ</a>, <a href="https://libvirt.org/docs.html">libvirt</a>, <a href="http://docs.openvswitch.org">Open vSwitch</a>, and <a href="http://docs.ceph.com">Ceph</a>.</p>

<h2>Choosing an OpenStack distribution</h2>

<p>There are several OpenStack distributions available out there. For us the choice was pretty straight forward. As we are a Red Hat shop, we went with <a href="https://www.rdoproject.org/">RDO</a> installed on top of RHEL7. I spent large amounts of time working with RDO and yeah, it was challenging, at least in its Mitaka release. For further details on our experience with OpenStack RDO, you can refer to articles: <a href="http://alesnosek.com/blog/2016/03/27/tripleo-installer-the-good/">1</a>, <a href="http://alesnosek.com/blog/2017/01/15/tripleo-installer-production-ready">2</a>. Due to the complexity of OpenStack, it is rather difficult to create a tool to manage its life-cycle and I&rsquo;m certain that further development effort will have to be spent before reaching perfection.</p>

<p>By the way, some OpenStack distributions come with a GUI-based installer. As there are dozens of configuration parameters to set during the installation, I don&rsquo;t see the point of using a graphical interface to do this. Instead, a well commented configuration file seems more desirable to me. Does the GUI-based installer enable product managers to make a check mark on their data sheet? I would say yes, but you can safely ignore it when choosing your OpenStack distribution.</p>

<h2>Choosing server hardware</h2>

<p>We started with a small OpenStack deployment comprised of 3 controller nodes, 2 compute nodes and 3 Ceph nodes. Over time, we added further nodes to meet the growing demand and ended up with the current size of the cluster being 3 controller nodes, 13 compute nodes and 7 Ceph nodes. Majority of the nodes are HP ProLiant DL360 Gen9 machines with the following hardware parameters:</p>

<ul>
<li><strong>Controller nodes.</strong> 96GB RAM, 2 x 300GB SAS 10K HDD in RAID1, 1Gbit Ethernet NICs</li>
<li><strong>Compute nodes.</strong> 288GB RAM, 8 x 300GB SAS 10K HDD in RAID10 (for the OS + instance ephemeral storage), 1Gbit and 10Gbit Ethernet NICs</li>
<li><strong>Ceph nodes.</strong> 32GB RAM, 2 x 300GB SAS 10K HDD in RAID1 (for the OS), 6 x 1.2TB SAS 10K HDD (Ceph OSD storage drives), 1Gbit and 10Gbit Ethernet NICs</li>
</ul>


<p>1Gbit Ethernet NICs are used to access OpenStack APIs on the controller nodes. Compute nodes and Ceph storage nodes are interconnected using 10Gbit Ethernet. All network interfaces are bonded and connected to two different switches to avoid a single-point-of-failure. There is a dedicated 1Gbit link attached to each of the OpenStack nodes used for node management via SSH.</p>

<p>From our experience, each of the compute nodes can run up to 40-50 virtual machines using the default OpenStack RDO settings: cpu_allocation_ratio=16.0, ram_allocation_ratio=1.0 and disk_allocation_ratio=1.0. Our current limit preventing us to achieve even higher density is the amount of provisioned RAM on the nodes. In the future, we are considering adding more RAM to the compute nodes or increasing the ram_allocation_ratio.</p>

<h2>Deployment overview</h2>

<p>Finally, we are going to take a look at the high-level overview of our OpenStack deployment. In the diagram below you can see the OpenStack projects that we chose for the deployment:</p>

<p><img src="http://alesnosek.com/images/posts/18_months_with_openstack_components.png" width="800" height="1000" title="OpenStack Components" ></p>

<p>Let me comment on some of the projects we deployed:</p>

<ul>
<li><strong><a href="https://docs.openstack.org/ironic">Ironic</a>.</strong> We deployed Ironic in order to manage baremetal machines that we use for performance testing. Performance tests are more accurate when carried out in an isolated baremetal environment than on the virtual machines that share the resources of the hypervisor. To this date we didn&rsquo;t realize this our plan but we will get back to it in the future.</li>
<li><strong><a href="https://docs.openstack.org/magnum">Magnum</a>.</strong> Magnum project simplifies the deployment of container orchestrators like Kubernetes, Swarm and Mesos on top of OpenStack. To accomplish this, Magnum leverages Heat templates behind the scenes and the actual provisioning is done by Heat. To be honest, we never really started using Magnum. When deploying Kubernetes, we preferred to use Heat templates provided by the Kubernetes project. This approach turned to be more straight forward than involving yet another service like Magnum. You can read about it <a href="http://alesnosek.com/blog/2016/06/26/deploying-kubernetes-on-openstack-using-heat">here</a>.</li>
<li><strong><a href="https://docs.openstack.org/sahara">Sahara</a>.</strong> Sahara project allows you to deploy big data frameworks like Apache Hadoop and Apache Spark on top of OpenStack. We made similar experience with Sahara as we made with Magnum. We just didn&rsquo;t start using it at all. It turned out that there were already pre-existing deployment scripts provided by Hortonworks and others that it made no sense for us to use Sahara. While Hortonworks <a href="https://github.com/hortonworks/ansible-hortonworks">scripts</a> can deploy Hadoop on any of the major clouds, Sahara would be an OpenStack-only solution.</li>
<li><strong><a href="https://docs.openstack.org/manila">Manila</a>.</strong> While not depicted in the diagram, we also deployed OpenStack Manila. Manila is a shared file system service and we use it to provision NFS shares. Manila project started as a code copy of the Cinder project and perhaps that&rsquo;s why it was pretty stable and usable soon after its inception. I wrote an <a href="http://alesnosek.com/blog/2016/05/22/test-driving-openstack-manila/">article</a> about Manila at the time we were evaluating it.</li>
<li><strong><a href="https://docs.openstack.org/designate">Designate</a>.</strong>  Designate is a DNS as a service for OpenStack. After evaluating this project, we realized that for our simple purpose Designate was too involved. We ended up writing a Python script that dynamically registers OpenStack virtual machines with our internal DNS server. This script works reliably ever since and you can read about it in this <a href="http://alesnosek.com/blog/2015/05/31/openstack-dynamic-dns-updates">blog post</a>.</li>
</ul>


<h2>Conclusion</h2>

<p>In this post, we described some of our experience with planning the OpenStack cloud and deploying it. In the second blog post, we are going to share the lessons learned when operating OpenStack.</p>

<p>If you have battle scars from working with OpenStack, I would love to hear from you. Please, feel free to share your comments and stories in the comment section below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Edge Security for Your Cloud Application, Part II]]></title>
    <link href="http://alesnosek.com/blog/2018/01/12/edge-security-for-your-cloud-application-part-ii/"/>
    <updated>2018-01-12T21:21:19-08:00</updated>
    <id>http://alesnosek.com/blog/2018/01/12/edge-security-for-your-cloud-application-part-ii</id>
    <content type="html"><![CDATA[<p>In this article, we&rsquo;re going to create a proof-of-concept deployment featuring a non-TLS client connecting to our cloud application. We are going to leverage the architecture approach discussed in the <a href="http://alesnosek.com/blog/2018/01/10/edge-security-for-your-cloud-application-part-i">previous blog post</a>. A secure communication channel is going to be established between the client and the cloud application including mutual authentication.</p>

<!-- more -->


<h2>Deployment overview</h2>

<p>Before we get our hands dirty, let&rsquo;s gain a better understanding of what we are trying to achieve. The diagram depicting our test deployment looks as follows:</p>

<p><img src="http://alesnosek.com/images/posts/edge_security_for_your_cloud_application_poc_arch.svg" width="1000" title="Architecture Overview" ></p>

<p>We&rsquo;re going to spin up two virtual instances. The edge instance will host our edge service. The client instance will host the client that will be accessing our edge service. For the sake of POC, the edge service is going to be an Apache web server and we&rsquo;re going to use the curl command-line utility in place of the client. The battle-proven HAProxy is going to play the role of the client-side as well as the server-side proxy, securing the client-server communication.</p>

<h2>Getting started</h2>

<p>You can start off with creating two CentOS 7 instances in AWS. Choose a minimalist t2.micro instance type which is sufficient for our proof of concept. In the security groups settings, make sure that in addition to the SSH port you have also enabled access to port 443 (HTTPS) from anywhere.</p>

<p>After the instances booted up, install HAProxy on both instances:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>yum install -y haproxy
</span></code></pre></td></tr></table></div></figure>


<p>And create a directory that will hold the keys and certificates required by HAProxy:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>mkdir /etc/haproxy/ssl
</span></code></pre></td></tr></table></div></figure>


<p>Make sure that you run the above command on both instances, too.</p>

<h2>PKI keys and certificates</h2>

<p>We are going to leverage the TLS protocol to establish a secure, mutually authenticated connection between the two proxies. TLS relies on PKI keys and certificates that we&rsquo;ll need to generate. The PKI setup for our company consists of a root CA, a layer of subordinate CAs and three end-entity certificates.</p>

<p><img src="http://alesnosek.com/images/posts/edge_security_for_your_cloud_application_pki.svg" width="500" height="600" title="PKI" ></p>

<p>It is a common practice to sign the end-entity certificates by one or more subordinate CAs as it prevents the necessity of revoking a root certificate in the case that an end-entity certificate is incorrectly issued or compromised.</p>

<p>In total, we are going to generate three end-entity certificates. <code>Edge Service Certificate</code> is going to be used by the reverse proxy running on the edge instance in order to authenticate itself to the clients. <code>Customer1 Client Certificate</code> and <code>Customer2 Client Certificate</code> are certificates that our company securely distributes to the tenants (customers). Each tenant uses her certificate and the associated private key to authenticate herself when accessing the cloud application.</p>

<p>A PKI certificate can be created in three steps:</p>

<ol>
<li>Generate a private key.</li>
<li>Using the private key, generate a Certificate Signing Request (CSR).</li>
<li>Using a CA certificate along with the respective private key and the CSR, generate the certificate.</li>
</ol>


<p>An exeption from this three-step procedure is the root certificate which is self-signed.</p>

<p>In the following, we are going to generate seven certificates. All the commands are to be issued on the edge instance. Note that you can populate the certificate fields with pretty arbitrary values with one exception: the <code>Common Name</code> field of the end-entity certificates. <code>Common Name</code> of the <code>Edge Service Certificate</code> must match the DNS name of the edge instance. <code>Common Name</code> of the <code>Customer1 Client Certificate</code> and the <code>Customer2 Client Certificate</code> must match the HAProxy configuration. By inspecting the <code>Common Name</code> field, HAProxy is able to recognize which client is trying to access the cloud application and it is able to route the client request to the appropriate backend service.</p>

<h3>Company RootCA certificate</h3>

<p>Let&rsquo;s generate the company&rsquo;s greatest secret - the private key of the Company&rsquo;s RootCA:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl genrsa -out rootca.company.example.key.pem 4096
</span></code></pre></td></tr></table></div></figure>


<p>And create a self-signed RootCA certificate. Below you can see the sample input data:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl req -key rootca.company.example.key.pem -new -x509 -extensions v3_ca -out rootca.company.example.crt.pem
</span><span class='line'>-----
</span><span class='line'>Country Name <span class="o">(</span><span class="m">2</span> letter code<span class="o">)</span> <span class="o">[</span>XX<span class="o">]</span>:US
</span><span class='line'>State or Province Name <span class="o">(</span>full name<span class="o">)</span> <span class="o">[]</span>:CA
</span><span class='line'>Locality Name <span class="o">(</span>eg, city<span class="o">)</span> <span class="o">[</span>Default City<span class="o">]</span>:San Diego
</span><span class='line'>Organization Name <span class="o">(</span>eg, company<span class="o">)</span> <span class="o">[</span>Default Company Ltd<span class="o">]</span>:Company RootCA
</span><span class='line'>Organizational Unit Name <span class="o">(</span>eg, section<span class="o">)</span> <span class="o">[]</span>:
</span><span class='line'>Common Name <span class="o">(</span>eg, your name or your server<span class="err">&#39;</span>s hostname<span class="o">)</span> <span class="o">[]</span>:rootca.company.example
</span><span class='line'>Email Address <span class="o">[]</span>:
</span></code></pre></td></tr></table></div></figure>


<p>For the sake of conciseness, I shortened the console output a bit. Note that we are adding the command-line option <code>-extensions v3_ca</code> to denote this certificate as a CA certificate. Otherwise, an end-entity certificate would have been generated by default.</p>

<h3>Company SubCA certificate</h3>

<p>Next, issue the command to generate the private key of the Company&rsquo;s SubCA:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl genrsa -out subca.company.example.key.pem 4096
</span></code></pre></td></tr></table></div></figure>


<p>Using the private key, let&rsquo;s create a Certificate Signing Request:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl req -new -key subca.company.example.key.pem -out subca.company.example.csr.pem
</span><span class='line'>-----
</span><span class='line'>Country Name <span class="o">(</span><span class="m">2</span> letter code<span class="o">)</span> <span class="o">[</span>XX<span class="o">]</span>:US
</span><span class='line'>State or Province Name <span class="o">(</span>full name<span class="o">)</span> <span class="o">[]</span>:CA
</span><span class='line'>Locality Name <span class="o">(</span>eg, city<span class="o">)</span> <span class="o">[</span>Default City<span class="o">]</span>:San Diego
</span><span class='line'>Organization Name <span class="o">(</span>eg, company<span class="o">)</span> <span class="o">[</span>Default Company Ltd<span class="o">]</span>:Company SubCA
</span><span class='line'>Organizational Unit Name <span class="o">(</span>eg, section<span class="o">)</span> <span class="o">[]</span>:
</span><span class='line'>Common Name <span class="o">(</span>eg, your name or your server<span class="s1">&#39;s hostname) []:subca.company.example</span>
</span><span class='line'><span class="s1">Email Address []:</span>
</span><span class='line'>
</span><span class='line'><span class="s1">Please enter the following &#39;</span>extra<span class="err">&#39;</span> attributes
</span><span class='line'>to be sent with your certificate request
</span><span class='line'>A challenge password <span class="o">[]</span>:
</span><span class='line'>An optional company name <span class="o">[]</span>:
</span></code></pre></td></tr></table></div></figure>


<p>And finally, generate the certificate of the Company&rsquo;s SubCA:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl x509 -req -in subca.company.example.csr.pem -CA rootca.company.example.crt.pem -CAkey rootca.company.example.key.pem -CAcreateserial -extfile /etc/pki/tls/openssl.cnf -extensions v3_ca -out subca.company.example.crt.pem
</span></code></pre></td></tr></table></div></figure>


<h3>Customer1 SubCA certificate</h3>

<p>Steps to generate the Customer1 SubCA certificate should be quite clear now:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl genrsa -out subca.customer1.example.key.pem 4096
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl req -new -key subca.customer1.example.key.pem -out subca.customer1.example.csr.pem
</span><span class='line'>-----
</span><span class='line'>Country Name <span class="o">(</span><span class="m">2</span> letter code<span class="o">)</span> <span class="o">[</span>XX<span class="o">]</span>:US
</span><span class='line'>State or Province Name <span class="o">(</span>full name<span class="o">)</span> <span class="o">[]</span>:CA
</span><span class='line'>Locality Name <span class="o">(</span>eg, city<span class="o">)</span> <span class="o">[</span>Default City<span class="o">]</span>:San Diego
</span><span class='line'>Organization Name <span class="o">(</span>eg, company<span class="o">)</span> <span class="o">[</span>Default Company Ltd<span class="o">]</span>:Customer1 SubCA
</span><span class='line'>Organizational Unit Name <span class="o">(</span>eg, section<span class="o">)</span> <span class="o">[]</span>:
</span><span class='line'>Common Name <span class="o">(</span>eg, your name or your server<span class="s1">&#39;s hostname) []:subca.customer1.example</span>
</span><span class='line'><span class="s1">Email Address []:</span>
</span><span class='line'>
</span><span class='line'><span class="s1">Please enter the following &#39;</span>extra<span class="err">&#39;</span> attributes
</span><span class='line'>to be sent with your certificate request
</span><span class='line'>A challenge password <span class="o">[]</span>:
</span><span class='line'>An optional company name <span class="o">[]</span>:
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl x509 -req -in subca.customer1.example.csr.pem -CA rootca.company.example.crt.pem -CAkey rootca.company.example.key.pem -CAserial rootca.srl -extfile /etc/pki/tls/openssl.cnf -extensions v3_ca -out subca.customer1.example.crt.pem
</span></code></pre></td></tr></table></div></figure>


<h3>Customer2 SubCA certificate</h3>

<p>Analogicaly,  we are going to generate the Customer2 SubCA certificate:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl genrsa -out subca.customer2.example.key.pem 4096
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl req -new -key subca.customer2.example.key.pem -out subca.customer2.example.csr.pem
</span><span class='line'>-----
</span><span class='line'>Country Name <span class="o">(</span><span class="m">2</span> letter code<span class="o">)</span> <span class="o">[</span>XX<span class="o">]</span>:US
</span><span class='line'>State or Province Name <span class="o">(</span>full name<span class="o">)</span> <span class="o">[]</span>:CA
</span><span class='line'>Locality Name <span class="o">(</span>eg, city<span class="o">)</span> <span class="o">[</span>Default City<span class="o">]</span>:San Diego
</span><span class='line'>Organization Name <span class="o">(</span>eg, company<span class="o">)</span> <span class="o">[</span>Default Company Ltd<span class="o">]</span>:Customer2 SubCA
</span><span class='line'>Organizational Unit Name <span class="o">(</span>eg, section<span class="o">)</span> <span class="o">[]</span>:
</span><span class='line'>Common Name <span class="o">(</span>eg, your name or your server<span class="s1">&#39;s hostname) []:subca.customer2.example</span>
</span><span class='line'><span class="s1">Email Address []:</span>
</span><span class='line'>
</span><span class='line'><span class="s1">Please enter the following &#39;</span>extra<span class="err">&#39;</span> attributes
</span><span class='line'>to be sent with your certificate request
</span><span class='line'>A challenge password <span class="o">[]</span>:
</span><span class='line'>An optional company name <span class="o">[]</span>:
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl x509 -req -in subca.customer2.example.csr.pem -CA rootca.company.example.crt.pem -CAkey rootca.company.example.key.pem -CAserial rootca.srl -extfile /etc/pki/tls/openssl.cnf -extensions v3_ca -out subca.customer2.example.crt.pem
</span></code></pre></td></tr></table></div></figure>


<h3>Edge service certificate</h3>

<p>This is the first out of the three end-entity certificates. First, we will generate the private key:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl genrsa -out app.company.example.key.pem 2048
</span></code></pre></td></tr></table></div></figure>


<p>Next, we are going to create the CSR. Make sure you populate the <code>Common Name</code> field exactly as you can see below:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl req -new -key app.company.example.key.pem -out app.company.example.csr.pem
</span><span class='line'>-----
</span><span class='line'>Country Name <span class="o">(</span><span class="m">2</span> letter code<span class="o">)</span> <span class="o">[</span>XX<span class="o">]</span>:US
</span><span class='line'>State or Province Name <span class="o">(</span>full name<span class="o">)</span> <span class="o">[]</span>:CA
</span><span class='line'>Locality Name <span class="o">(</span>eg, city<span class="o">)</span> <span class="o">[</span>Default City<span class="o">]</span>:San Diego
</span><span class='line'>Organization Name <span class="o">(</span>eg, company<span class="o">)</span> <span class="o">[</span>Default Company Ltd<span class="o">]</span>:Company
</span><span class='line'>Organizational Unit Name <span class="o">(</span>eg, section<span class="o">)</span> <span class="o">[]</span>:
</span><span class='line'>Common Name <span class="o">(</span>eg, your name or your server<span class="s1">&#39;s hostname) []:app.company.example</span>
</span><span class='line'><span class="s1">Email Address []:</span>
</span><span class='line'>
</span><span class='line'><span class="s1">Please enter the following &#39;</span>extra<span class="err">&#39;</span> attributes
</span><span class='line'>to be sent with your certificate request
</span><span class='line'>A challenge password <span class="o">[]</span>:
</span><span class='line'>An optional company name <span class="o">[]</span>:
</span></code></pre></td></tr></table></div></figure>


<p>And finally, type this command to generate the certificate:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl x509 -req -in app.company.example.csr.pem -CA subca.company.example.crt.pem -CAkey subca.company.example.key.pem -CAcreateserial -out app.company.example.crt.pem
</span></code></pre></td></tr></table></div></figure>


<p>Note that we didn&rsquo;t include the <code>-extensions v3_ca</code> option as we wanted to create an end-entity certificate.</p>

<h3>Customer1 client certificate</h3>

<p>Now we are going to create a certificate for our first customer.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl genrsa -out customer1.example.key.pem 2048
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl req -new -key customer1.example.key.pem -out customer1.example.csr.pem
</span><span class='line'>-----
</span><span class='line'>Country Name <span class="o">(</span><span class="m">2</span> letter code<span class="o">)</span> <span class="o">[</span>XX<span class="o">]</span>:US
</span><span class='line'>State or Province Name <span class="o">(</span>full name<span class="o">)</span> <span class="o">[]</span>:MA
</span><span class='line'>Locality Name <span class="o">(</span>eg, city<span class="o">)</span> <span class="o">[</span>Default City<span class="o">]</span>:Boston
</span><span class='line'>Organization Name <span class="o">(</span>eg, company<span class="o">)</span> <span class="o">[</span>Default Company Ltd<span class="o">]</span>:Customer1
</span><span class='line'>Organizational Unit Name <span class="o">(</span>eg, section<span class="o">)</span> <span class="o">[]</span>:
</span><span class='line'>Common Name <span class="o">(</span>eg, your name or your server<span class="s1">&#39;s hostname) []:customer1.example</span>
</span><span class='line'><span class="s1">Email Address []:</span>
</span><span class='line'>
</span><span class='line'><span class="s1">Please enter the following &#39;</span>extra<span class="err">&#39;</span> attributes
</span><span class='line'>to be sent with your certificate request
</span><span class='line'>A challenge password <span class="o">[]</span>:
</span><span class='line'>An optional company name <span class="o">[]</span>:
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl x509 -req -in customer1.example.csr.pem -CA subca.customer1.example.crt.pem -CAkey subca.customer1.example.key.pem -CAcreateserial -out customer1.example.crt.pem
</span></code></pre></td></tr></table></div></figure>


<h3>Customer2 client certificate</h3>

<p>And finally, we are going to create a certificate for our second customer.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl genrsa -out customer2.example.key.pem 2048
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl req -new -key customer2.example.key.pem -out customer2.example.csr.pem
</span><span class='line'>-----
</span><span class='line'>Country Name <span class="o">(</span><span class="m">2</span> letter code<span class="o">)</span> <span class="o">[</span>XX<span class="o">]</span>:US
</span><span class='line'>State or Province Name <span class="o">(</span>full name<span class="o">)</span> <span class="o">[]</span>:TX
</span><span class='line'>Locality Name <span class="o">(</span>eg, city<span class="o">)</span> <span class="o">[</span>Default City<span class="o">]</span>:Austin
</span><span class='line'>Organization Name <span class="o">(</span>eg, company<span class="o">)</span> <span class="o">[</span>Default Company Ltd<span class="o">]</span>:Customer2
</span><span class='line'>Organizational Unit Name <span class="o">(</span>eg, section<span class="o">)</span> <span class="o">[]</span>:
</span><span class='line'>Common Name <span class="o">(</span>eg, your name or your server<span class="s1">&#39;s hostname) []:customer2.example</span>
</span><span class='line'><span class="s1">Email Address []:</span>
</span><span class='line'>
</span><span class='line'><span class="s1">Please enter the following &#39;</span>extra<span class="err">&#39;</span> attributes
</span><span class='line'>to be sent with your certificate request
</span><span class='line'>A challenge password <span class="o">[]</span>:
</span><span class='line'>An optional company name <span class="o">[]</span>:
</span></code></pre></td></tr></table></div></figure>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl x509 -req -in customer2.example.csr.pem -CA subca.customer2.example.crt.pem -CAkey subca.customer2.example.key.pem -CAcreateserial -out customer2.example.crt.pem
</span></code></pre></td></tr></table></div></figure>


<p>If everything went well, your working directory should contain a collection of PKI files similar to this list:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>ls -1
</span><span class='line'>app.company.example.crt.pem
</span><span class='line'>app.company.example.csr.pem
</span><span class='line'>app.company.example.key.pem
</span><span class='line'>customer1.example.crt.pem
</span><span class='line'>customer1.example.csr.pem
</span><span class='line'>customer1.example.key.pem
</span><span class='line'>customer2.example.crt.pem
</span><span class='line'>customer2.example.csr.pem
</span><span class='line'>customer2.example.key.pem
</span><span class='line'>rootca.company.example.crt.pem
</span><span class='line'>rootca.company.example.key.pem
</span><span class='line'>rootca.srl
</span><span class='line'>subca.company.example.crt.pem
</span><span class='line'>subca.company.example.csr.pem
</span><span class='line'>subca.company.example.key.pem
</span><span class='line'>subca.customer1.example.crt.pem
</span><span class='line'>subca.customer1.example.csr.pem
</span><span class='line'>subca.customer1.example.key.pem
</span><span class='line'>subca.customer2.example.crt.pem
</span><span class='line'>subca.customer2.example.csr.pem
</span><span class='line'>subca.customer2.example.key.pem
</span><span class='line'>subca.srl
</span></code></pre></td></tr></table></div></figure>


<h2>Installing the edge service</h2>

<p>In our POC project, the role of the edge service will be played by the Apache server. To install the Apache server, issue the following command on the edge instance:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>yum install -y httpd
</span></code></pre></td></tr></table></div></figure>


<p>You can start the Apache server by typing:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>systemctl start httpd
</span></code></pre></td></tr></table></div></figure>


<p>The default static web page served by Apache is for our purposes a bit too long. Let&rsquo;s replace it with a simple, one line message:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span><span class="nb">echo</span> <span class="s1">&#39;It works!&#39;</span> &gt; /var/www/html/index.html
</span></code></pre></td></tr></table></div></figure>


<p>To verify that Apache was installed properly and is running, issue the command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl localhost
</span><span class='line'>It works!
</span></code></pre></td></tr></table></div></figure>


<h2>Configuring the reverse proxy on the edge instance</h2>

<p>In this section, we are going to set up the reverse proxy on the edge instance. First, let&rsquo;s prepare two files which will be referred to from the HAProxy configuration. The file <code>app.crt</code> will include the edge service certificate along with the CA chain and the respective private key. It is used by HAProxy to authenticate itself to the clients. In the following sections, we will configure the client-side HAProxy to trust these certificates and hence verify that it is connecting to the correct service and not for example to a service of an attacker. To create the <code>app.crt</code> file, you can type:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>cat <span class="se">\</span>
</span><span class='line'>app.company.example.crt.pem <span class="se">\</span>
</span><span class='line'>subca.company.example.crt.pem <span class="se">\</span>
</span><span class='line'>rootca.company.example.crt.pem <span class="se">\</span>
</span><span class='line'>app.company.example.key.pem <span class="se">\</span>
</span><span class='line'>&gt; /etc/haproxy/ssl/app.crt
</span></code></pre></td></tr></table></div></figure>


<p>The important task of the server-side HAProxy is to authenticate the incoming client connections. In our project, we are looking at two customers that should be allowed to access our application. For that, we&rsquo;re going to include the CA certificate chains of the two customers into the <code>customer-ca.crt</code> file:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>cat <span class="se">\</span>
</span><span class='line'>subca.customer1.example.crt.pem <span class="se">\</span>
</span><span class='line'>subca.customer2.example.crt.pem <span class="se">\</span>
</span><span class='line'>rootca.company.example.crt.pem <span class="se">\</span>
</span><span class='line'>&gt; /etc/haproxy/ssl/customer-ca.crt
</span></code></pre></td></tr></table></div></figure>


<p>As the last step in this section, we are going to configure HAProxy. You can open the HAProxy configuration file <code>/etc/haproxy/haproxy.cfg</code> in your favorite editor and replace its content with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>global
</span><span class='line'>  tune.ssl.default-dh-param 1024
</span><span class='line'>
</span><span class='line'>defaults
</span><span class='line'>  timeout client 30s
</span><span class='line'>  timeout server 30s
</span><span class='line'>  timeout connect 5s
</span><span class='line'>
</span><span class='line'>frontend proxy
</span><span class='line'>  <span class="nb">bind </span>0.0.0.0:443 ssl crt /etc/haproxy/ssl/app.crt ca-file /etc/haproxy/ssl/customer-ca.crt verify required
</span><span class='line'>  mode http
</span><span class='line'>
</span><span class='line'>  use_backend edge_customer1 <span class="k">if</span> <span class="o">{</span> ssl_c_s_dn<span class="o">(</span>cn<span class="o">)</span> -i customer1.example <span class="o">}</span>
</span><span class='line'>  use_backend edge_customer2 <span class="k">if</span> <span class="o">{</span> ssl_c_s_dn<span class="o">(</span>cn<span class="o">)</span> -i customer2.example <span class="o">}</span>
</span><span class='line'>
</span><span class='line'>backend edge_customer1
</span><span class='line'>  mode http
</span><span class='line'>  server customer1 127.0.0.1:80 check
</span><span class='line'>
</span><span class='line'>backend edge_customer2
</span><span class='line'>  mode http
</span><span class='line'>  server customer2 127.0.0.1:80 check
</span></code></pre></td></tr></table></div></figure>


<p>This is a minimalist configuration file, good enough for our proof of concept. HAProxy is going to listen on port 443 for incoming TLS connections. It will present the edge service certificate to the clients. At the same time, it will only accept connections from the clients sending the Customer1 or Customer2 certificate. Based on the <code>Common Name</code> field of the client&rsquo;s certificate, HAProxy will forward the request to the respective backend. In our case, both customers will be served by the same service listening on 127.0.0.1:80, however, you can imagine that in the real scenario there would be two separate edge services perhaps running on two different machines.</p>

<p>You can start the HAProxy service using the following command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>systemctl restart haproxy
</span></code></pre></td></tr></table></div></figure>


<p>Before moving on, check the HAProxy logs. If everything worked well, there should be no errors or warnings:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>journalctl -u haproxy -e
</span></code></pre></td></tr></table></div></figure>


<h2>Configuring the proxy on the client instance</h2>

<p>In this section, we&rsquo;re going to turn our attention to the client instance. First, we&rsquo;ll need to copy some of the PKI keys and certificates from the edge instance to the client instance. I configured SSH between the two instances, so that I can issue the following command on the edge instance to copy the files to the client instance:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>scp <span class="se">\</span>
</span><span class='line'>customer1.example.crt.pem <span class="se">\</span>
</span><span class='line'>customer1.example.key.pem <span class="se">\</span>
</span><span class='line'>customer2.example.crt.pem <span class="se">\</span>
</span><span class='line'>customer2.example.key.pem <span class="se">\</span>
</span><span class='line'>subca.company.example.crt.pem <span class="se">\</span>
</span><span class='line'>rootca.company.example.crt.pem <span class="se">\</span>
</span><span class='line'>ip-172-31-33-109.us-west-2.compute.internal:
</span></code></pre></td></tr></table></div></figure>


<p>In the above command, make sure that you replace the target host name <code>ip-172-31-33-109.us-west-2.compute.internal</code> with the DNS name of your client instance.</p>

<p>On the client instance, we are going to add a line to the <code>/etc/hosts</code> file which will make the DNS name <code>app.company.example</code> resolve to the IP address of the edge instance. In the following command, replace the IP address with the IP address of your edge instance:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span><span class="nb">echo </span>172.31.44.105 app.company.example &gt;&gt; /etc/hosts
</span></code></pre></td></tr></table></div></figure>


<p>Let&rsquo;s check that the TLS client is able to connect to the edge service. On the client instance, type:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl <span class="se">\</span>
</span><span class='line'>--cacert rootca.company.example.crt.pem <span class="se">\</span>
</span><span class='line'>--cert ./customer1.example.crt.pem <span class="se">\</span>
</span><span class='line'>--key ./customer1.example.key.pem <span class="se">\</span>
</span><span class='line'>https://app.company.example
</span><span class='line'>It works!
</span></code></pre></td></tr></table></div></figure>


<p>Excellent! We&rsquo;ve just verified that the client with the built-in TLS support is able to successfully connect to our edge service and authenticate itself as Customer1.</p>

<p>The ultimate goal of this tutorial was to allow a client without TLS support to access the edge service, too. In order to achieve this goal, we&rsquo;re going to set up a client-side HAProxy. First, let&rsquo;s prepare two files that will be needed by HAProxy. The <code>customer1.crt</code> file enables HAProxy to authenticate itself to the edge service as Customer1. You can create this file by issuing the command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>cat <span class="se">\</span>
</span><span class='line'>customer1.example.crt.pem <span class="se">\</span>
</span><span class='line'>customer1.example.key.pem <span class="se">\</span>
</span><span class='line'>&gt; /etc/haproxy/ssl/customer1.crt
</span></code></pre></td></tr></table></div></figure>


<p>Second file allows HAProxy to verify the authenticity of the edge service. It includes a chain of CA certificates, against which the certificate presented by the edge service will be verified. You can create the <code>company-ca.crt</code> file using the following command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>cat <span class="se">\</span>
</span><span class='line'>subca.company.example.crt.pem <span class="se">\</span>
</span><span class='line'>rootca.company.example.crt.pem <span class="se">\</span>
</span><span class='line'>&gt; /etc/haproxy/ssl/company-ca.crt
</span></code></pre></td></tr></table></div></figure>


<p>And finally, we are going to configure the client-side HAProxy. On the client instance, open the file <code>/etc/haproxy/haproxy.cfg</code>  in your favorite editor and replace its content with the following configuration:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>global
</span><span class='line'>  tune.ssl.default-dh-param 1024
</span><span class='line'>
</span><span class='line'>defaults
</span><span class='line'>  timeout client 30s
</span><span class='line'>  timeout server 30s
</span><span class='line'>  timeout connect 5s
</span><span class='line'>
</span><span class='line'>frontend proxy
</span><span class='line'>  <span class="nb">bind </span>0.0.0.0:80
</span><span class='line'>  mode http
</span><span class='line'>
</span><span class='line'>  use_backend app
</span><span class='line'>
</span><span class='line'>backend app
</span><span class='line'>  mode http
</span><span class='line'>  server app.company.example app.company.example:443 check ssl verify required ca-file /etc/haproxy/ssl/company-ca.crt crt /etc/haproxy/ssl/customer1.crt
</span></code></pre></td></tr></table></div></figure>


<p>HAProxy will listen on port 80 for the incoming HTTP connections. For each HTTP connection it will open a secure HTTPS connection to the edge service and it will pass the data back and forth between the two connections. You can start the HAProxy on the client instance by typing:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>systemctl restart haproxy
</span></code></pre></td></tr></table></div></figure>


<p>Double-check that there are no warnings or errors in the HAProxy logs:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>journalctl -u haproxy -e
</span></code></pre></td></tr></table></div></figure>


<p>If everything went well, you should be able to connect to the edge service using a non-TLS client. Here we go:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl localhost
</span><span class='line'>It works!
</span></code></pre></td></tr></table></div></figure>


<p>To verify that our client is recognized by the cloud application as Customer1, you can comment out the line <code>use_backend edge_customer1 if ...</code> in the <code>/etc/haproxy/haproxy.cfg</code> file on the edge instance. Remember to restart HAProxy after you modified the configuration. The repeated test from the client instance proves that indeed there&rsquo;s no backend for the Customer1 available anymore:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl localhost
</span><span class='line'>&lt;html&gt;&lt;body&gt;&lt;h1&gt;503 Service Unavailable&lt;/h1&gt;
</span><span class='line'>No server is available to handle this request.
</span><span class='line'>&lt;/body&gt;&lt;/html&gt;
</span></code></pre></td></tr></table></div></figure>


<h2>Conclusion and final remarks</h2>

<p>In this blog post, we established a secure communication channel between the client and our application running in the cloud. As demonstrated, our approach is also suitable for client applications that don&rsquo;t support TLS.</p>

<p>In our example, the client certificate authentication was carried out by HAProxy and the edge service sitting behind the proxy didn&rsquo;t get any information about the client. To improve the design, HAProxy could be configured to forward the attributes of the certificate presented by the client to the backend, by setting them as HTTP request headers. HAProxy even allows to insert the entire client certificate into a request header for the backend.</p>

<p>Our server presented a certificate that was signed by our own CA. In practice, we would deploy a certificate signed by a trusted third-party CA. AWS provides <a href="https://aws.amazon.com/certificate-manager">AWS Certificate Manager</a> (ACM) service to generate certificates for ELBs, API Gateway and other AWS services. Unfortunately, we cannot utilize this service for our use case as it doesn&rsquo;t provide access to the private key. Instead, we can purchase a certificate from one of the trusted certificate authorities or leverage the free of charge <a href="https://letsencrypt.org/">Let&rsquo;s Encrypt</a> certificate authority.</p>

<p>This concludes our miniseries about edge security for cloud applications. We are still evaluating the proposed approach. What do you think about it? If you have any feedback, please, feel free to add your comments in the comment section below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Edge Security for Your Cloud Application, Part I]]></title>
    <link href="http://alesnosek.com/blog/2018/01/10/edge-security-for-your-cloud-application-part-i/"/>
    <updated>2018-01-10T23:09:17-08:00</updated>
    <id>http://alesnosek.com/blog/2018/01/10/edge-security-for-your-cloud-application-part-i</id>
    <content type="html"><![CDATA[<p>When designing a cloud application, one of the challenges you need to tackle is the secure communication of your clients with your servers over the public Internet. In this post, I&rsquo;m going to sketch a system architecture that allows you to authenticate the clients and exchange data between your clients and servers securely. The proposed architecture is also suitable when migrating an existing application, which didn&rsquo;t implement the secure communication, to the cloud.</p>

<!-- more -->


<p>In this article, we are going to assume that we&rsquo;re dealing with a multi-tenant cloud application whose architecture follows the <a href="https://en.wikipedia.org/wiki/Service-oriented_architecture">SOA</a> architecture principles. The application consists of several edge services that accept the incoming client requests. After validating the incoming request, the request is passed to the backend services for processing. Here is a diagram depicting such an application:</p>

<p><img src="http://alesnosek.com/images/posts/edge_security_for_your_cloud_application_soa.svg" width="800" height="1000" title="SOA architecture" ></p>

<h2>Requirements</h2>

<p>Before we start walking through the design, let&rsquo;s take a look at the set of requirements that drove the design decisions:</p>

<ol>
<li><p>One of the first steps of request validation should be the client authentication. Requests originating from the clients that fail to authenticate should be refused immediately.</p></li>
<li><p>We don&rsquo;t impose any restrictions on what protocol is used by the clients to communicate with our application. Typically, clients will leverage the HTTP protocol but we want to support any TCP based protocol. In other words, our application doesn&rsquo;t have to be a web service.</p></li>
<li><p>From the past, we inherited some clients that are not able to communicate in a secure way. For instance, they are only able to talk plain HTTP and don&rsquo;t support HTTPS. We would like to make such clients work with our application without the need to modify the clients themselves.</p></li>
<li><p>And last but not least, we would like to avoid the need to establish a VPN connection between the client machine and the cloud. In practice, setting up a VPN connection requires quite invasive configuration of the client operating system. For our application tenants (our customers) this would impose a barrier that would hurt the adoption of our cloud application. Something we definitely don&rsquo;t want.</p></li>
</ol>


<h2>The edge layer</h2>

<p>SSL/TLS is a family of security protocols that many VPNs rely upon. Currently, the TLS protocol is considered to be one of the strongest and most mature security protocols available. It was a clear choice for us to leverage the TLS protocol for communication between the clients and servers including the mutual authentication using PKI certificates. Finally, here is a diagram showing the edge layer of our cloud application in detail:</p>

<p><img src="http://alesnosek.com/images/posts/edge_security_for_your_cloud_application_server.svg" width="800" height="1000" title="The edge layer" ></p>

<h2>Internet-facing ELB</h2>

<p>Interestingly, AWS Elastic load balancers don&rsquo;t support client certificate authentication. Both, Classic load balancer and Application load balancer, support TLS offloading where they can terminate the TLS connection for you. However, they are not able to authenticate the client certificate and they also don&rsquo;t allow forwarding the certificate details to the backend application which could carry out the authentication by itself. In order to implement the client certificate authentication, you are pretty much left with two options: you can either use the Classic load balancer in the TCP mode, or you can employ the Network load balancer which operates on the TCP level. In both cases, the Elastic load balancer just passes the TLS connection through to your EC2 instances where you have to terminate and authenticate the TLS connection yourself. As the connection between ELB and the EC2 instances remains encrypted, you are gaining a plus from the security standpoint, too.</p>

<p>While ELB doesn&rsquo;t authenticate clients, it plays an important role in our architecture. First, ELB is highly-available and distributes the traffic to several edge instances that are not highly-available. Second, ELB implements a layer 3 (e.g. UDP reflection) and layer 4 (e.g. SYN flood) DDOS attack protection.</p>

<p>Btw., Amazon API Gateway doesn&rsquo;t support client certificate authentication either and so is less helpful for our scenario.</p>

<h2>Reverse proxy on edge instances</h2>

<p>Depending on your architecture, your edge services may or may not have a support for accepting TLS connections built in. I personally see the connection security to be a job for the infrastructure and would vote against implementing the TLS support in your application services. Here are some concrete arguments why to terminate the TLS connection outside of your application services:</p>

<ol>
<li><p>Your application services may be written in different languages. For each language and its runtime, the TLS configuration is different. You don&rsquo;t want to spend your time figuring out, how to configure TLS on Apache Tomcat, Eclipse Jetty, Apache server, Node.js and others, do you? Different technologies support different TLS features. For example, TLS SNI is supported only in Tomcat >= 8.5. Researching the supported feature set of every runtime is time consuming.</p></li>
<li><p>The proxy provides a good place to monitor and log what&rsquo;s going on on the wire. Remember that ELB won&rsquo;t have any insight into your traffic as the traffic is TLS encrypted.</p></li>
<li><p>You may want to automate the TLS certificate rotation and revocation. You&rsquo;ll have to implement this for all your different web servers. That&rsquo;s quite a bit of work.</p></li>
</ol>


<p>Instead of handling TLS in your application services, I would recommend deploying a reverse proxy in front of your services. This proxy will terminate the incoming TLS connection. This will allow you to manage all the TLS related settings in one place. The battle-tested proxies like <a href="http://www.haproxy.org/">HAProxy</a> or <a href="https://www.nginx.com/">NGINX</a> can authenticate the client certificate against a trusted certificate authority. They can also forward the certificate details to your edge service which can implement a more complex authentication logic if you need it.</p>

<p>You should deploy the reverse proxy on the same instance with your edge service. This way, the decrypted communication between the proxy and your service will never leave the instance. In order to capture the decrypted traffic, one would need to have a root access on the instance. The possibility to capture the communication in the clear will come handy when troubleshooting, though.</p>

<h2>The client side</h2>

<p>After discussing the server-side design, let&rsquo;s talk about the client-side part of the picture. As we mentioned in the requirements section above, our architecture has to also support clients that don&rsquo;t have the TLS functionality built in. In turns out that there are actually three different client types:</p>

<ol>
<li>Clients with native TLS mutual authentication support</li>
<li>Clients with no TLS support at all</li>
<li>Clients supporting TLS with one-way authentication only, i.e. the client is able to authenticate the server certificate but it doesn&rsquo;t present its own certificate to the server.</li>
</ol>


<p>There is nothing special to do for the clients with native TLS mutual authentication support. They can directly connect to the application servers. To ensure the secure communication for the remaining two client types, we&rsquo;re going to deploy a proxy on the client machines. This proxy will enforce the TLS mutually authenticated connection over the Internet. A diagram depicting the three client types looks as follows:</p>

<p><img src="http://alesnosek.com/images/posts/edge_security_for_your_cloud_application_client.svg" width="600" height="800" title="The client side" ></p>

<p>In the case number two, the proxy is configured in the SSL/TLS encryption mode. It accepts an unencrypted connection from the client and forwards the communication over a TLS encrypted channel to the server.  For the case number three, the proxy must be configured using the SSL/TLS bridging aka re-encryption mode. The proxy accepts an encrypted connection from the client, creates a separate encrypted connection to the server and passes the data between the two connections. Both HAProxy and Nginx support these configuration modes. You can also refer to the TLS layouts described in the HAProxy <a href="https://www.haproxy.com/documentation/aloha/7-0/deployment-guides/tls-layouts/">documentation</a>.</p>

<p>Alternatively, as a proxy one could also employ <a href="https://www.stunnel.org">stunnel</a>. Before the SSL/TLS support was built into HAProxy, stunnel used to be deployed along with HAProxy to provide the SSL/TLS functionality. To accomplish the case number three using stunnel, one would actually need to combine two stunnels in series.</p>

<p>Recently, a modern <a href="https://www.envoyproxy.io/">Envoy</a> proxy emerged and I would like to encourage you to check it out. It provides a really impressive set of features that goes far beyond the load balancing functionality: automatic retries, circuit breaking, zone local load balancing, very detailed metrics etc. Envoy helps to solve several networking problems common to the cloud-native applications. In our proposed architecture, Envoy would be deployed in the role of an edge proxy on the server side as well as in the role of the service proxy on the client side.</p>

<p>Whichever proxy you choose on the client-side, remember to deploy it in a highly available fashion. You don&rsquo;t want to introduce a single point of failure into your system, do you?</p>

<h2>Conclusion</h2>

<p>In this post, we walked through the secure edge design explaining the reasoning behind the individual design decisions. In the <a href="http://alesnosek.com/blog/2018/01/12/edge-security-for-your-cloud-application-part-ii">second blog post</a> of this miniseries, we&rsquo;re going to demonstrate a practical implementation of our approach using HAProxy.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[What I Learned at KubeCon + CloudNativeCon 2017]]></title>
    <link href="http://alesnosek.com/blog/2017/12/10/what-i-learned-at-kubecon-plus-cloudnativecon-2017/"/>
    <updated>2017-12-10T16:16:25-08:00</updated>
    <id>http://alesnosek.com/blog/2017/12/10/what-i-learned-at-kubecon-plus-cloudnativecon-2017</id>
    <content type="html"><![CDATA[<p>KubeCon + CloudNativeCon North America 2017 took place in Austin, Texas on December 6-8, 2017. I greatly enjoyed this conference. This blog post presents some of the notes I made during the conference talks. Would you like to learn about Amazon EKS, Kata Containers or the CRI-O project? Do you want to better understand Docker&rsquo;s Moby project and its relation to LinuxKit and InfraKit? Then read on!</p>

<!-- more -->


<p><img class="center" src="http://alesnosek.com/images/posts/kubecon_2017.png" width="500" height="200"></p>

<h2>Keynotes</h2>

<ul>
<li><strong>Keynote: A Community of Builders</strong>

<ul>
<li>Linux Foundation created a <a href="https://www.edx.org/course/introduction-kubernetes-linuxfoundationx-lfs158x">Introduction to Kubernetes</a> course available to everybody who wants to learn Kubernetes. The course is provided free of charge on the edX platform.</li>
<li>CNCF in collaboration with The Linux Foundation created a certification program <a href="https://www.cncf.io/certification/expert/">Certified Kubernetes Administrator</a> (CKA). Check it out, if you want to demonstrate your Kubernetes skills.</li>
</ul>
</li>
<li><strong>Keynote: CNCF Project Updates</strong>

<ul>
<li>The recently released <a href="https://coredns.io/">CoreDNS</a> v1.0 is going to replace <a href="https://github.com/kubernetes/dns">Kube DNS</a> in Kubernetes v1.9.</li>
<li>Container runtime <a href="https://containerd.io/">containerd</a>, originally created by Docker and now hosted by CNCF, reached its v1.0 milestone.</li>
<li>Today, the CNCF logging project <a href="https://www.fluentd.org/">Fluentd</a> reached its v1.0 release.</li>
<li>A rather young project <a href="http://fluentbit.io/">Fluent Bit</a> is a log forwarder which allows you to collect your data/logs from various sources and forward them to multiple destinations. Supported destinations are among others: Elasticsearch, InfluxDB or Kafka. Fluent Bit is implemented in C and it only leverages asynchronous operations to collect and deliver data. It integrates well with the <a href="https://prometheus.io/">Prometheus</a> monitoring system.</li>
</ul>
</li>
<li><strong>Keynote: Cloud Native at AWS</strong>

<ul>
<li>AWS chose containerd as the runtime for their managed Kubernetes service <a href="https://aws.amazon.com/eks/">EKS</a>.</li>
<li>With the recent announcements of <a href="https://aws.amazon.com/about-aws/whats-new/2017/11/announcing-amazon-ec2-bare-metal-instances-preview/">Amazon EC2 Bare Metal instances</a> and <a href="https://aws.amazon.com/blogs/aws/amazon-elastic-container-service-for-kubernetes/">Amazon Elastic Container Service for Kubernetes</a>, there are now four first-class deployment entities available on AWS: bare metal, VMs, containers and functions.</li>
<li>Amazon ECS for Kubernetes will leverage IAM to authenticate identities and the Kubernetes&#8217; RBAC for authorization. Pods will be assigned IAM roles in order for them to be able to talk to other AWS services, e.g. S3.</li>
</ul>
</li>
</ul>


<h2>Sessions attended</h2>

<ul>
<li><strong>Embedding the Containerd Runtime for Fun and Profit</strong>

<ul>
<li>ShiftFS is trying to solve the problem with the access (uid/gid) to a single file system from two containers running in different user namespaces.</li>
</ul>
</li>
<li><strong>Kata Containers: Hypervisor-Based Container Runtime</strong>

<ul>
<li>The <a href="https://katacontainers.io/">Kata Containers</a> project is a merger of two projects: <a href="https://github.com/clearcontainers/runtime">Intel Clear Containers</a> and <a href="https://github.com/hyperhq/runv">Hyper runV</a>. The goal of the project is to create a container runtime that would run containers as virtual machines, however, with the speed comparable to the namespace-based containers. This would allow to bring the secure hypervisor-based isolation to containers.</li>
<li>Kata Containers would provide an alternative to the <a href="https://github.com/opencontainers/runc">runc</a> runtime which is used by the namespace-based containers. In the Kubernetes cluster, both namespace-based as well as hypervisor-based containers could run at the same time.</li>
<li>On Kubernetes, a Kubernetes pod would be implemented as a single VM.</li>
</ul>
</li>
<li><strong>Building Specialized Container-Based Systems with Moby: A Few Use Cases</strong>

<ul>
<li><a href="https://blog.docker.com/2017/04/introducing-the-moby-project/">Moby project</a> is pretty much an umbrella for all Docker open-source projects. Projects like <a href="https://github.com/containerd/containerd">containerd</a>, <a href="https://github.com/linuxkit/linuxkit">LinuxKit</a>, <a href="https://github.com/docker/infrakit">InfraKit</a>, and many more are part of the Moby project. From the components of the Moby project, Docker company builds their Docker SE product, from which the Docker EE product is derived. Moby project is governed by the Technical Steering Committee (<a href="https://github.com/moby/tsc">TSC</a>). In times before the TSC was established, technical disputes were resolved by the Benevolent Dictator for Life (BDFL) which was Solomon Hykes.</li>
<li><a href="https://github.com/linuxkit/linuxkit">LinuxKit</a> is a toolkit for building custom Linux images. The single purpose of these images is to run containerized applications. When creating an image, you can choose a Linux kernel, choose a container runtime and pick additional services that should be started on boot. If the basic containerd runtime doesn&rsquo;t suit you, LinuxKit can even install Kubernetes for you. Resulting images include only packages that you&rsquo;ve chosen and nothing else, hence reducing the attack surface and improving security. You can spin AWS instances off of your customized images and start scheduling containerized applications on them. Instances are considered immutable. In order to apply software updates, you must build a new version of the image and replace the instances running the old image.</li>
<li>In order to simplify the managing of the infrastructure running on top of immutable images (built with LinuxKit), Docker came up with another toolkit called <a href="https://github.com/docker/infrakit.git">InfraKit</a>. InfraKit allows you to create and manage clustered infrastructure like Swarm or Kubernetes clusters. Docker&rsquo;s motivation behind InfraKit was the deployment and management of their Docker SE and Docker EE products. Behind the scenes, InfraKit leverages Terraform for resource provisioning.</li>
</ul>
</li>
<li><strong>CRI-O: All the Runtime Kubernetes Needs and Nothing More</strong>

<ul>
<li>Container runtime containerd originally developed by Docker Inc. is used for various purposes. Besides Kubernetes and OpenShift, containerd powers Swarm clusters and the recently announced Amazon EKS is also going to leverage containerd. Being a general purpose runtime, containerd suffered from compatibility issues with Kubernetes, where an update of containerd would break the Kubernetes cluster. For the sake of stability and <a href="http://www.projectatomic.io/blog/2017/06/6-reasons-why-cri-o-is-the-best-runtime-for-kubernetes/">several other reasons</a>, Red Hat initiated a project <a href="http://cri-o.io/">CRI-O</a> which aims to create an alternative runtime that would be dedicated to Kubernetes only. CRI-O is available for testing in OpenShift 3.7 and it is going to ship as the default container runtime in the future OpenShift releases, replacing containerd.</li>
</ul>
</li>
<li><strong>Extending Kubernetes 101</strong>

<ul>
<li>Kubernetes can be extended by implementing a custom controller also called <em>operator</em>.</li>
<li>This controller runs as a pod on Kubernetes cluster. On startup, custom controller is supposed to register one or more custom resources with Kubernetes. After that, the controller watches the changes in the desired state of custom resources and acts upon them.</li>
<li>The concept of <a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/">Custom Resources</a> is supported by Kubernetes since version 1.7.</li>
<li>Custom resources are designed to feel like native Kubernetes resources. They are exposed as another API along with the native APIs. The kubectl client can discover and work with custom resources right away.</li>
<li>Using the kubectl, user can change the desired state of custom resources (add, update, delete). The custom controller that watches the desired state of the custom resources will carry on operations required to reach the desired state.</li>
<li>Btw., there&rsquo;s a generator available that speeds up the implementation of operators.</li>
<li>See also <a href="https://github.com/rook/operator-kit">Kubernetes Operator Kit</a>.</li>
</ul>
</li>
<li><strong>Certifik8ts: All You Need to Know about Certificates in Kubernetes</strong>

<ul>
<li>As of Kubernetes 1.8, the Kubelet can request a new client certificate when the current one is nearing its expiration.</li>
<li>Currently, Kubernetes cannot revoke certificates. This is good to know if you use client certificates to athenticate users. Once you issue a client certificate to the user, you won&rsquo;t be able to revoke it.</li>
</ul>
</li>
<li><strong>Istio: Saling to a Secure Services Mesh</strong>

<ul>
<li>Service mesh acts as a proxy on the communication path between your microservices. As a proxy, service mesh adds a host of useful functionality that you would otherwise have to implement yourself in your microservices: secure communication, traffic routing, load balancing, rate limiting, circuit breakers, timeouts and retries, metrics, reporting, etc.</li>
<li>Somebody on the conference has noted, that the year 2018 will be the year of the service mesh. Based on how many problems the service mesh can solve for distributed microservices, I can only agree with that.</li>
<li>In this talk, the security features of <a href="https://istio.io/">Istio</a> service mesh were presented.</li>
<li>Istio can establish a mutually authenticated TLS connections between your microservices, without the need to change your application code. It solves the problem of bootstrapping the TLS trust for you by establishing a custom CA.</li>
<li>Istio Ingress can expose the microservice outside of the service mesh.</li>
<li>Istio Egress controls which external hosts (hosts outside of the service mesh) your microservices can talk to.</li>
</ul>
</li>
<li><strong>Cost-effective Compute Clusters with Spot and Pre-emptible Instances</strong>

<ul>
<li>Quote: <em>Kubernetes is going to make spot instances mainstream</em>.</li>
<li>What applications work well with spot instances? Elastic/bursting applications, HPC workloads, HA cluster apps and horizontally scalable apps.</li>
</ul>
</li>
</ul>

]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[AWS Lambda Adapter for Vert.x Web Applications]]></title>
    <link href="http://alesnosek.com/blog/2017/10/18/aws-lambda-adapter-for-vert-dot-x-web-applications/"/>
    <updated>2017-10-18T23:09:33-07:00</updated>
    <id>http://alesnosek.com/blog/2017/10/18/aws-lambda-adapter-for-vert-dot-x-web-applications</id>
    <content type="html"><![CDATA[<p><a href="http://vertx.io/">Vert.x</a> is an awesome tool-kit for developing reactive microservices. In this article, I&rsquo;m going to present an adapter that allows you to run your Vert.x web service as a Lambda function on AWS.</p>

<!-- more -->


<p><img class="right" src="http://alesnosek.com/images/posts/vertx_logo.png" width="130" height="130"></p>

<p>If you are creating web services using Vert.x, you are leveraging the HTTP server built into the core of the Vert.x tool-kit. This HTTP server is based on the <a href="https://netty.io/">Netty</a> framework. When your web service comes up, the HTTP server opens a network port and starts listening for the incoming client connections. After a client connects, the HTTP server reads the HTTP request and processes this request by invoking callbacks that you registered. HTTP response generated by your implementation is returned back to the client. The web service remains running and processing requests until you shut it down.</p>

<p><img class="right" src="http://alesnosek.com/images/posts/aws_lambda_logo.png" width="100" height="100"></p>

<p>In contrast to the constantly running web service, an AWS Lambda function is instantiated to serve a single HTTP request. After the HTTP request has been processed, the Lambda function is torn down.</p>

<p>In our company, we&rsquo;re looking at deploying our web services on-premise and in the cloud. We realized that when deploying into AWS it would be more cost effective if we could run some of our web services as Lambda functions. The question was, how to allow a Vert.x web service to alternatively run as a Lambda function? And how to accomplish this with a minimum development and maintenance effort?</p>

<p>After browsing through the Vert.x source code, it occurred to me that it would be possible to write a simple adapter that would convert HTTP requests and responses between the Lambda API and the Vert.x API. I then get on with the job and implemented such an adapter. And because software practitioners love open source, you can find this adapter along with the sample application on GitHub: <a href="https://github.com/noseka1/vertx-aws-lambda">vertx-aws-lambda</a>.</p>

<p>As always, I&rsquo;d love to hear your feedback. What do you think about this project? Feel free to leave your comments below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Evaluating Application Metrics Solutions - Prometheus?]]></title>
    <link href="http://alesnosek.com/blog/2017/09/10/evaluating-application-metrics-solutions-prometheus/"/>
    <updated>2017-09-10T16:17:49-07:00</updated>
    <id>http://alesnosek.com/blog/2017/09/10/evaluating-application-metrics-solutions-prometheus</id>
    <content type="html"><![CDATA[<p>In our <a href="https://en.wikipedia.org/wiki/Service-oriented_architecture">SOA-based</a> application, the problem of application metrics hasn&rsquo;t been solved yet. We would like to have our application services expose metrics that could be used for monitoring, auto-scaling and analytics. In this blog post, I would like to present to you one of the proposals to solve the application metrics which suggests leveraging <a href="https://prometheus.io/">Prometheus</a>.</p>

<!-- more -->


<p>Our application consists of multiple services that are deployed on multiple machines. Currently, our applicaton is deployed on-premise or as a managed offering on the virtual machines in AWS. We&rsquo;re also working on containerizing the application services to achieve higher density and better manageability when deploying into the AWS cloud. Some of our services are so light-weight that we&rsquo;re going to turn them into Lambda functions in the future to further reduce the operational costs. Thus, a solution for application metrics should be able to work in the serverless environment, too.</p>

<p>As of now, we deploy <a href="https://www.icinga.com/">Icinga</a> along with our application to provide system-level monitoring. Icinga collects the information about the nodes and checks that our services are still running. However, we don&rsquo;t collect any application-level metrics that would allow us to better assess the performance of our system. For example, we would like to know how many requests are processed per second, average request latency, request error rate, what are the depths of the internal queues and so forth. Application metrics would be a welcome input to the auto-scaling decisions and we would like to feed them into our analytics engine as well.</p>

<h2>Getting to know Prometheus</h2>

<p><img class="right" src="http://alesnosek.com/images/posts/prometheus_logo.png" width="130" height="130"></p>

<p>Before jumping in and implementing our own solution for metrics collection and perhaps reinventing the wheel we started shopping around. It seemed to us, that Prometheus monitoring solution was gaining a lot of momentum in recent times. So, we took a closer look at Prometheus and this is what we found:</p>

<ol>
<li>Prometheus is an open-source monitoring solution hosted by <a href="https://www.cncf.io/">CNCF</a> - a foundation that hosts Kubernetes as well. Many companies use and contribute to Prometheus.</li>
<li>The <a href="https://prometheus.io/docs/introduction/overview/">architecture</a> of Prometheus is easy to understand and is modular. While Prometheus provides modules for metrics collection, alerts and Web UI, we would not have to use all of them.</li>
<li>Prometheus is a pull-based monitoring system. Each monitored target has to expose Prometheus formatted metrics. By default, targets make the metrics endpoint available at <a href="http://target/metrics.">http://target/metrics.</a> Prometheus periodically scrapes the metrics exposed by the targets.</li>
<li>Our services would need to expose the application metrics in the <a href="https://prometheus.io/docs/instrumenting/exposition_formats/">Prometheus format</a>. There are actually two formats available: a simple text format and protobufs. There are instrumentation <a href="https://prometheus.io/docs/instrumenting/clientlibs/">libraries</a> for Java, C++ and other languages, to gather the metrics and expose them in the Prometheus format.</li>
<li>The text-based Prometheus metrics format is so simple that it could be collected by other monitoring systems like Nagios or Icinga. Exposing metrics in the Prometheus format doesn&rsquo;t really mandate using Prometheus server for monitoring.</li>
<li>There&rsquo;s a Prometheus <a href="https://github.com/prometheus/jmx_exporter">jmx_exporter</a> library to convert the JMX MBeans data into Prometheus format. This would come in handy for gathering Tomcat metrics, for example.</li>
<li><a href="http://metrics.dropwizard.io/">Dropwizard metrics</a> is a popular Java instrumentation library. For instance, Vert.x toolkit can report its <a href="http://vertx.io/docs/vertx-dropwizard-metrics/java/">internal metrics</a> using the Dropwizard metrics library and there are other frameworks that supports it. Prometheus comes with a <a href="https://github.com/prometheus/client_java/tree/master/simpleclient_dropwizard">simpleclient_dropwizard</a> library that can make Dropwizard metrics available to Prometheus monitoring.</li>
<li>To prevent unauthorized access, the metric targets would need to be protected using TLS in combination with client certs, bearer token or HTTP basic authentication.</li>
<li>Prometheus pulls the metrics from the monitored targets. In addition, Prometheus comes with a <a href="https://prometheus.io/docs/practices/pushing/">Pushgateway</a> where clients can push their metrics to. However, as noted in the Prometheus documentation: <em>Usually, the only valid use case for the Pushgateway is for capturing the outcome of a service-level batch job</em>. Hence, Pushgateway would not work for aggregating metrics pushed by the Lambda functions.</li>
<li>In addition to application-level metrics, system-level metrics can be collected by Prometheus as well thanks to the <a href="https://github.com/prometheus/node_exporter">node_exporter</a>.</li>
<li>Prometheus is a great fit for dynamic environments like clouds and container clusters due to its discovery capabilities. In AWS, operator attaches tags to VMs and based on that Prometheus can discover them and start monitoring them automatically. The same principle works for container clusters like Kubernetes, too. One has to add annotations to pods and Prometheus will discover them automatically.</li>
<li>Prometheus makes the collected metrics available for querying via an <a href="https://prometheus.io/docs/querying/api/">HTTP API</a>. We could retrieve the metrics using this API in order to feed them into our analytics engine.</li>
<li>There is a great <a href="https://prometheus.io/docs/practices/naming/">guide</a> that would help us when designing our custom metrics.</li>
<li>Prometheus is written in Go and comes in a form of statically-linked binaries. This makes the installation of Prometheus a breeze.</li>
</ol>


<h2>Instrumenting Java applications</h2>

<p>In order to gather application metrics and to make them available to the Prometheus monitoring system, we would need to instrument our application services using Prometheus libraries. To get a clear idea, we created a proof-of-concept Java application instrumented using Prometheus. You can find it on <a href="https://github.com/noseka1/prometheus-poc">GitHub</a>.</p>

<p>Alternatively, we are thinking about leveraging Dropwizard metrics library for instrumentation. The Dropwizard metrics library is rather popular and is not connected with any particular monitoring solution. We would still be able to expose the Dropwizard metrics to Prometheus using a wrapper <a href="https://github.com/prometheus/client_java/tree/master/simpleclient_dropwizard">simpleclient_dropwizard</a>.</p>

<h2>Monitoring AWS Lambda functions</h2>

<p>AWS Lambda functions are extremely short-lived processes. Prometheus won&rsquo;t be able to pull the application metrics from them. Instead, Lambdas will have to push their metrics to Prometheus. At the first glance, we thought that the Prometheus Pushgateway could help here, however, reading the Pushgateway&rsquo;s documentation more carefully we found that <em>the Pushgateway is explicitly not an aggregator or distributed counter but rather a metrics cache</em>. And that&rsquo;s a problem, as we would like to count how many Lambda instances are being invoked per second and so on.</p>

<p>At the moment, we can see two approaches how to make the monitoring of Lambda functions work with Prometheus. Either, push the application metrics from the Lambda functions using a StatsD client. Prometheus&#8217; <a href="https://github.com/prometheus/statsd_exporter">statsd_exporter</a> would play a role of a StastD server and make the metrics available to Prometheus. Or, the second approach would be to create our own metrics aggregator that would receive the metrics from Lambda functions in the Prometheus format, aggregate them and expose them to the Prometheus server.</p>

<h2>Alternatives</h2>

<p>Besides using Prometheus, we were also thinking about other solutions for application metrics. As we already deploy Icinga for the system-level monitoring, it would make sense to use it for application metrics, too. We really like Icinga, it&rsquo;s a great monitoring software. Unfortunately, Icinga is based on the node and services model where a statically configured set of nodes are running services on them. This doesn&rsquo;t really fit with the modern containerized deployments where containers are dynamically scheduled on the cluster nodes and are also scaled up and down. Also, Prometheus server supports all sorts of metric queries and aggregations. Icinga is lacking this feature altogether. That&rsquo;s why we&rsquo;re leaning towards replacing Icinga with Prometheus for system-level as well as application-level monitoring.</p>

<p><a href="http://www.hawkular.org/">Hawkular</a> seems to be another modern monitoring project we would like to take a closer look at. In contrast to Prometheus project which is developed by many parties, it seems that Hawkular project is mostly driven by Red Hat.</p>

<h2>Conclusion</h2>

<p>Prometheus is a modern monitoring system. It was the first system we evaluated as we were trying to find a good solution for application metrics. In addition to application-level metrics, we could use Prometheus to collect system-level metrics as well. This would make Prometheus a single monitoring solution for our application. The only bigger issue for us is the absence of the AWS Lambda monitoring story.</p>

<p>If you have an application that you deliver on-premise as well as in the cloud, how did you solve the application metrics collection and monitoring? Is Prometheus a good way to go? Please, leave your comments below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Ansible Certification EX407]]></title>
    <link href="http://alesnosek.com/blog/2017/09/03/ansible-certification-ex407/"/>
    <updated>2017-09-03T22:47:06-07:00</updated>
    <id>http://alesnosek.com/blog/2017/09/03/ansible-certification-ex407</id>
    <content type="html"><![CDATA[<p>Last week I passed the Red Hat certification exam <a href="https://www.redhat.com/en/services/training/ex407-red-hat-certificate-expertise-ansible-automation">EX407 Red Hat Certificate of Expertise in Ansible Automation</a>. In this blog post, I&rsquo;d like to share some of my experience with you.</p>

<!-- more -->


<p><img class="right" src="http://alesnosek.com/images/posts/ansible_logo.png" width="80" height="80"></p>

<p><img class="right" src="http://alesnosek.com/images/posts/redhat_logo.png" width="130" height="130"></p>

<p>In addition to the Ansible certification, Red Hat offers a certification for Puppet as well, <a href="https://www.redhat.com/en/services/training/ex405-red-hat-certificate-expertise-configuration-management-puppet">EX405 Red Hat Certificate of Expertise in Configuration Management with Puppet</a>. I was using Puppet in the years 2008/2009. Back then, Puppet was a state of the art configuration management tool that did a great job for us. However, later on Ansible showed up and I quickly realized that the problems that we were solving with Puppet could have been more easily solved with Ansible. In our company, we switched from Puppet to Ansible in 2014 and have never looked back. As I gained much more experience using Ansible, I decided to go for the Ansible certification. For you, perhaps the Puppet certification would be more interesting.</p>

<p>It is the way of testing, that makes the Red Hat certification exams so enjoyable for me. Red Hat certification exams are purely practical. In the exam, you&rsquo;ll be given a list of requirements and an access to one or more virtual machines. Your goal is to configure the virtual machines based on the requirements.</p>

<p>The Ansible exam focused on writing Ansible playbooks, working with Ansible inventories including dynamic inventories, running ad-hoc Ansible commands, leveraging Ansible facts, creating Jinja2 templates, error handling, playbook tags, downloading a role from Ansible Galaxy and using Ansible vault to secure passwords.</p>

<p>To prepare for the exam, I used the online course Automation with Ansible I (DO407R) that is included in my <a href="https://www.redhat.com/en/services/training/learning-subscription">Red Hat Learning Subscription</a>.</p>

<p>Overall, I didn&rsquo;t find the exam too difficult. There were 4 hours of time provided for the exam. I was able to complete all my tasks 1 hour and 20 minutes before the exam end while reaching the maximum score of 300 points.</p>

<p>If you&rsquo;ve kept reading my blog post until this point, here are three basic exam tips for you:</p>

<ol>
<li>Use the <code>ansible-playbook --limit</code> parameter, to try out your playbook against a single host first before applying it to all your hosts.</li>
<li>Make sure you can run Ansible modules as ad-hoc commands. They can come handy when you want to undo the effects of running an erroneous playbook.</li>
<li>Use <code>ansible-doc --list</code> to look up a module that can achieve your goal and <code>ansible-doc &lt;module&gt;</code> to learn about the module parameters and example usage.</li>
</ol>


<p>Passing the Ansible exam brought me to the rank of Red Hat Certified Architect Level II. The current list of my Red Hat certifications can be found on the <a href="https://www.redhat.com/rhtapps/certification/verify/?certId=160-216-727">Verify a Red Hat Certified Professional</a> website.</p>

<p>Are you working on your Red Hat certification and would like to share your experience? I would always like to hear from you! Feel free to leave your comments below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Test-Driving OpenShift Online]]></title>
    <link href="http://alesnosek.com/blog/2017/08/27/test-driving-openshift-online/"/>
    <updated>2017-08-27T17:29:49-07:00</updated>
    <id>http://alesnosek.com/blog/2017/08/27/test-driving-openshift-online</id>
    <content type="html"><![CDATA[<p>Did you decide to dockerize your application? Awesome! Are you looking for a place to build and host your Docker containers? OpenShift Online is a service that allows you to build and run your Docker containers. Read on, if you want to learn more about it.</p>

<!-- more -->


<p><a href="https://manage.openshift.com/">OpenShift Online</a> is a Platform-as-a-Service cloud managed by Red Hat. It is hosted on Amazon Web Services (AWS). OpenShift Online is a multi-tenant offering. That means that containers owned by different tenants are running on the same cluster. If you&rsquo;re more interested in renting your own private OpenShift cluster, you shall consider the <a href="https://www.openshift.com/dedicated/index.html">OpenShift Dedicated</a> offering, instead.</p>

<p>The information about the plans and pricing of the OpenShift Online service can be found <a href="https://www.openshift.com/pricing/index.html">here</a>. In this blog post, we&rsquo;re going to utilize the free of charge <em>Starter</em> plan. With this plan, you get 1GiB of memory for your containers, another 1GiB of memory for running builds, deployments and jobs, and finally 1GiB of persistent storage. The real limitation of the free <em>Starter</em> plan is the resource hibernation. Your containers will be forced to sleep 18 hours in a 72 hour period.</p>

<h2>Welcome to OpenShift Online</h2>

<p>You can login into OpenShift Online <a href="https://manage.openshift.com/">here</a>. If you don&rsquo;t have an OpenShift account, the sign up and creation of the OpenShift account doesn&rsquo;t take you long. After you login into your OpenShift account, find a small question mark icon in the top right corner of the welcome page. From the drop down menu choose the &ldquo;Command Line Tools&rdquo; option.</p>

<p><img class="left" src="http://alesnosek.com/images/posts/openshift_online/openshift_online_welcome.png"></p>

<p>Download the <code>oc</code> client tool using one of the provided download links. We&rsquo;re going to use the <code>oc</code> tool throughout this tutorial. After downloading the distribution archive, extract the <code>oc</code> tool from it and place it somewhere on your PATH. The <code>oc</code> tool is a single statically linked binary which makes the installation straight forward. On the same page, notice the instructions on how to login into the CLI. We&rsquo;ll be logging into it in a moment.</p>

<p><img class="left" src="http://alesnosek.com/images/posts/openshift_online/openshift_online_command_line_tools.png"></p>

<p>Go back to the welcome page and this time choose the &ldquo;About&rdquo; option from the drop down menu in the top right corner. On the &ldquo;About&rdquo; page, you can learn that OpenShift Online is built upon a fairly recent version of OpenShift. That&rsquo;s great to know, as each new version of OpenShift comes with a ton of new features. As the OpenShift Online user you would not like to be left behind with an older version of OpenShift.</p>

<p>On the same &ldquo;About&rdquo; page, you can find the address of the integrated Docker registry. You can push ready images to this registry in order to make them available for deployment on OpenShift. If you build images on OpenShift, the finished images will be pushed into this registry at the end of the build process. In addition to deploying images from the integrated Docker registry, you can, of course, deploy images directly from the Docker Hub as well.</p>

<p><img class="left" src="http://alesnosek.com/images/posts/openshift_online/openshift_online_about.png"></p>

<h2>Getting our hands dirty</h2>

<p>In this section, we&rsquo;re going to use the CLI tool to exercise the OpenShift Online functionality. From the &ldquo;Command Line Tools&rdquo; page that we visited previously, copy the command to login into the CLI tool:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>oc login https://api.starter-us-west-1.openshift.com --token<span class="o">=</span>6BoQ7DQ8nUeEE3FOWzvDCgcUD0T6LdBBZEuUiPlD7Tc
</span><span class='line'>Logged into <span class="s2">&quot;https://api.starter-us-west-1.openshift.com:443&quot;</span> as <span class="s2">&quot;anosek@example.com&quot;</span> using the token provided.
</span><span class='line'>
</span><span class='line'>You don<span class="err">&#39;</span>t have any projects. You can try to create a new project, by running
</span><span class='line'>
</span><span class='line'>    oc new-project &lt;projectname&gt;
</span></code></pre></td></tr></table></div></figure>


<p>Note that the value of your login token will differ. Next, let&rsquo;s create a new OpenShift project called <code>php-hello</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>oc new-project php-hello
</span><span class='line'>Now using project <span class="s2">&quot;php-hello&quot;</span> on server <span class="s2">&quot;https://api.starter-us-west-1.openshift.com:443&quot;</span>.
</span><span class='line'>
</span><span class='line'>You can add applications to this project with the <span class="s1">&#39;new-app&#39;</span> command. For example, try:
</span><span class='line'>
</span><span class='line'>    oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git
</span><span class='line'>
</span><span class='line'>to build a new example application in Ruby.
</span></code></pre></td></tr></table></div></figure>


<p>The Starter plan allows you to create only a single project. Next, we&rsquo;re going to build our PHP application. The source code of the application can be found at <a href="https://github.com/noseka1/openshift-php-hello">GitHub</a>. The whole purpose of the application is to return a Hello message containing the name of the host that the application is running on. You can build the application using the <code>oc new-build</code> command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>oc new-build https://github.com/noseka1/openshift-php-hello.git
</span><span class='line'>--&gt; Found image 0d2f8fa <span class="o">(</span><span class="m">4</span> weeks old<span class="o">)</span> in image stream <span class="s2">&quot;openshift/php&quot;</span> under tag <span class="s2">&quot;7.0&quot;</span> <span class="k">for</span> <span class="s2">&quot;php&quot;</span>
</span><span class='line'>
</span><span class='line'>    Apache 2.4 with PHP 7.0
</span><span class='line'>    -----------------------
</span><span class='line'>    PHP 7.0 available as docker container is a base platform <span class="k">for</span> building and running various PHP 7.0 applications and frameworks. PHP is an HTML-embedded scripting language. PHP attempts to make it easy <span class="k">for</span> developers to write dynamically generated web pages. PHP also offers built-in database integration <span class="k">for</span> several commercial and non-commercial database management systems, so writing a database-enabled webpage with PHP is fairly simple. The most common use of PHP coding is probably as a replacement <span class="k">for</span> CGI scripts.
</span><span class='line'>
</span><span class='line'>    Tags: builder, php, php70, rh-php70
</span><span class='line'>
</span><span class='line'>    * The <span class="nb">source </span>repository appears to match: php
</span><span class='line'>    * A <span class="nb">source </span>build using <span class="nb">source </span>code from https://github.com/noseka1/openshift-php-hello.git will be created
</span><span class='line'>      * The resulting image will be pushed to image stream <span class="s2">&quot;openshift-php-hello:latest&quot;</span>
</span><span class='line'>      * Use <span class="s1">&#39;start-build&#39;</span> to trigger a new build
</span><span class='line'>
</span><span class='line'>--&gt; Creating resources with label <span class="nv">build</span><span class="o">=</span>openshift-php-hello ...
</span><span class='line'>    imagestream <span class="s2">&quot;openshift-php-hello&quot;</span> created
</span><span class='line'>    buildconfig <span class="s2">&quot;openshift-php-hello&quot;</span> created
</span><span class='line'>--&gt; Success
</span><span class='line'>    Build configuration <span class="s2">&quot;openshift-php-hello&quot;</span> created and build triggered.
</span><span class='line'>    Run <span class="s1">&#39;oc logs -f bc/openshift-php-hello&#39;</span> to stream the build progress.
</span></code></pre></td></tr></table></div></figure>


<p>OpenShift automatically detects, that we&rsquo;re building a PHP application and will use an appropriate build image. The build image contains a pre-installed Apache server with mod_php. During the build, the <code>index.php</code> file from the source code repository is copied into the document root of the Apache server. The build process takes a minute or two to complete. You can follow the progress of your build using the <code>oc logs</code> command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>oc logs -f bc/openshift-php-hello
</span><span class='line'>Cloning <span class="s2">&quot;https://github.com/noseka1/openshift-php-hello.git&quot;</span> ...
</span><span class='line'>        Commit: 35d7f33ca8180b9a331d9bd5ce4735c941da9d03 <span class="o">(</span>Add index.php<span class="o">)</span>
</span><span class='line'>        Author: Ales Nosek &lt;ales.nosek@gmail.com&gt;
</span><span class='line'>        Date:   Mon Aug <span class="m">28</span> 13:50:56 <span class="m">2017</span> -0700
</span><span class='line'>Pulling image <span class="s2">&quot;registry.access.redhat.com/rhscl/php-70-rhel7@sha256:1f12d421bfc18874c5a7fdc41634ca5dd1cbd955c437738d571088e65dd0ba51&quot;</span> ...
</span><span class='line'>---&gt; Installing application source...
</span><span class='line'>
</span><span class='line'>Pushing image 172.30.148.65:5000/php-hello/openshift-php-hello:latest ...
</span><span class='line'>Pushed 0/6 layers, 7% <span class="nb">complete</span>
</span><span class='line'>Pushed 1/6 layers, 18% <span class="nb">complete</span>
</span><span class='line'>Pushed 2/6 layers, 37% <span class="nb">complete</span>
</span><span class='line'>Pushed 3/6 layers, 56% <span class="nb">complete</span>
</span><span class='line'>Pushed 4/6 layers, 79% <span class="nb">complete</span>
</span><span class='line'>Pushed 5/6 layers, 99% <span class="nb">complete</span>
</span><span class='line'>Pushed 6/6 layers, 100% <span class="nb">complete</span>
</span><span class='line'>Push successful
</span></code></pre></td></tr></table></div></figure>


<p>The output of the build is a new <code>openshift-php-hello</code> Docker image. This image is automatically pushed into the integrated Docker registry by OpenShift. In the next step, we&rsquo;re going to create a DeploymentConfig file which describes how to deploy our brand new <code>openshift-php-hello</code> image on OpenShift. Create a file named <code>php-hello.yml</code> with the following content:</p>

<figure class='code'><figcaption><span>php-hello.yml </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">kind</span><span class="p-Indicator">:</span> <span class="s">&quot;DeploymentConfig&quot;</span>
</span><span class='line'><span class="l-Scalar-Plain">apiVersion</span><span class="p-Indicator">:</span> <span class="s">&quot;v1&quot;</span>
</span><span class='line'><span class="l-Scalar-Plain">metadata</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="s">&quot;php-hello&quot;</span>
</span><span class='line'><span class="l-Scalar-Plain">spec</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">template</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="l-Scalar-Plain">metadata</span><span class="p-Indicator">:</span>
</span><span class='line'>      <span class="l-Scalar-Plain">labels</span><span class="p-Indicator">:</span>
</span><span class='line'>        <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="s">&quot;php-hello&quot;</span>
</span><span class='line'>    <span class="l-Scalar-Plain">spec</span><span class="p-Indicator">:</span>
</span><span class='line'>      <span class="l-Scalar-Plain">containers</span><span class="p-Indicator">:</span>
</span><span class='line'>        <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="s">&quot;php-hello&quot;</span>
</span><span class='line'>          <span class="l-Scalar-Plain">image</span><span class="p-Indicator">:</span> <span class="s">&quot;&quot;</span>
</span><span class='line'>          <span class="l-Scalar-Plain">imagePullPolicy</span><span class="p-Indicator">:</span> <span class="s">&quot;Always&quot;</span>
</span><span class='line'>          <span class="l-Scalar-Plain">ports</span><span class="p-Indicator">:</span>
</span><span class='line'>            <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">containerPort</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">8080</span>
</span><span class='line'>  <span class="l-Scalar-Plain">replicas</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">1</span>
</span><span class='line'>  <span class="l-Scalar-Plain">selector</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="s">&quot;php-hello&quot;</span>
</span><span class='line'>  <span class="l-Scalar-Plain">triggers</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">type</span><span class="p-Indicator">:</span> <span class="s">&quot;ConfigChange&quot;</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">type</span><span class="p-Indicator">:</span> <span class="s">&quot;ImageChange&quot;</span>
</span><span class='line'>      <span class="l-Scalar-Plain">imageChangeParams</span><span class="p-Indicator">:</span>
</span><span class='line'>        <span class="l-Scalar-Plain">automatic</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">true</span>
</span><span class='line'>        <span class="l-Scalar-Plain">containerNames</span><span class="p-Indicator">:</span>
</span><span class='line'>          <span class="p-Indicator">-</span> <span class="s">&quot;php-hello&quot;</span>
</span><span class='line'>        <span class="l-Scalar-Plain">from</span><span class="p-Indicator">:</span>
</span><span class='line'>          <span class="l-Scalar-Plain">kind</span><span class="p-Indicator">:</span> <span class="s">&quot;ImageStreamTag&quot;</span>
</span><span class='line'>          <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="s">&quot;openshift-php-hello:latest&quot;</span>
</span><span class='line'>  <span class="l-Scalar-Plain">strategy</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="l-Scalar-Plain">type</span><span class="p-Indicator">:</span> <span class="s">&quot;Rolling&quot;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Submit this file to OpenShift in order to launch the deployment:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>oc create -f php-hello.yml
</span><span class='line'>deploymentconfig <span class="s2">&quot;php-hello&quot;</span> created
</span></code></pre></td></tr></table></div></figure>


<p>Based on the DeploymentConfig descriptor, OpenShift will start one container <code>php-hello-1-XXXXX</code>. You can check whether the container is running with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>oc get pod
</span><span class='line'>NAME                          READY     STATUS      RESTARTS   AGE
</span><span class='line'>openshift-php-hello-1-build   0/1       Completed   <span class="m">0</span>          2h
</span><span class='line'>php-hello-1-zw48t             1/1       Running     <span class="m">0</span>          2m
</span></code></pre></td></tr></table></div></figure>


<p>Verify that the <code>READY</code> column for your <code>php-hello</code> container eventually reads <code>1/1</code>. This indicates, that the deployment was successful and your container is running. In the following steps, we&rsquo;re going to configure OpenShift routing to allow the external network traffic to reach our container. For further information about the traffic routing on OpenShift, you can refer to my older blog post <a href="http://alesnosek.com/blog/2017/02/14/accessing-kubernetes-pods-from-outside-of-the-cluster/">Accessing Kubernetes Pods from Outside of the Cluster</a>. First, let&rsquo;s create a service for our <code>php-hello</code> container:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>oc expose dc php-hello --port 8080
</span><span class='line'>service <span class="s2">&quot;php-hello&quot;</span> exposed
</span></code></pre></td></tr></table></div></figure>


<p>The created service functions as a internal load balancer. It forwards the traffic to the <code>php-hello</code> container on port 8080. The internal IP address of this load balancer is <code>172.30.80.50</code>, as we can learn when listing the existing services:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>oc get svc
</span><span class='line'>NAME        CLUSTER-IP     EXTERNAL-IP   PORT<span class="o">(</span>S<span class="o">)</span>    AGE
</span><span class='line'>php-hello   172.30.80.50   &lt;none&gt;        8080/TCP   4s
</span></code></pre></td></tr></table></div></figure>


<p>Finally, we&rsquo;re going to create a route which will forward the external traffic to our service and hence to our <code>php-hello</code> container:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>oc expose svc php-hello
</span><span class='line'>route <span class="s2">&quot;php-hello&quot;</span> exposed
</span></code></pre></td></tr></table></div></figure>


<p>The route is assigned a public FQDN <code>php-hello-php-hello.a3c1.starter-us-west-1.openshiftapps.com</code>. As we haven&rsquo;t configured the route to use the TLS protocol, our application will be reachable on the standard HTTP port 80. The assigned FQDN can be found in the output of the <code>oc get route</code> command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>oc get route
</span><span class='line'>NAME        HOST/PORT                                                      PATH      SERVICES    PORT      TERMINATION   WILDCARD
</span><span class='line'>php-hello   php-hello-php-hello.a3c1.starter-us-west-1.openshiftapps.com             php-hello   <span class="m">8080</span>                    None
</span></code></pre></td></tr></table></div></figure>


<p>At this moment, our application should be reachable over the public Internet. Let&rsquo;s send an HTTP request to our application:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl php-hello-php-hello.a3c1.starter-us-west-1.openshiftapps.com
</span><span class='line'>Hello from php-hello-1-zw48t!
</span></code></pre></td></tr></table></div></figure>


<p>Excellent, the application responded with the Hello message as expected. In the last exercise of our tutorial, we&rsquo;re going to scale our application. Let&rsquo;s ask OpenShift to create one more <code>php-hello</code> container:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>oc scale dc php-hello --replicas 2
</span><span class='line'>deploymentconfig <span class="s2">&quot;php-hello&quot;</span> scaled
</span></code></pre></td></tr></table></div></figure>


<p>The deployment of the second container will complete shortly. You can check the status of running containers using the <code>oc get pod</code> commmand:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>oc get pod
</span><span class='line'>NAME                          READY     STATUS      RESTARTS   AGE
</span><span class='line'>openshift-php-hello-1-build   0/1       Completed   <span class="m">0</span>          2h
</span><span class='line'>php-hello-1-cl5nw             1/1       Running     <span class="m">0</span>          30s
</span><span class='line'>php-hello-1-zw48t             1/1       Running     <span class="m">0</span>          21m
</span></code></pre></td></tr></table></div></figure>


<p>At this point, we&rsquo;re having two Docker containers ready to serve our HTTP requests. Let&rsquo;s generate some traffic and observe how OpenShift load balances the incoming requests between the two containers:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl php-hello-php-hello.a3c1.starter-us-west-1.openshiftapps.com
</span><span class='line'>Hello from php-hello-1-zw48t!
</span><span class='line'><span class="nv">$ </span>curl php-hello-php-hello.a3c1.starter-us-west-1.openshiftapps.com
</span><span class='line'>Hello from php-hello-1-cl5nw!
</span><span class='line'><span class="nv">$ </span>curl php-hello-php-hello.a3c1.starter-us-west-1.openshiftapps.com
</span><span class='line'>Hello from php-hello-1-zw48t!
</span><span class='line'><span class="nv">$ </span>curl php-hello-php-hello.a3c1.starter-us-west-1.openshiftapps.com
</span><span class='line'>Hello from php-hello-1-cl5nw!
</span></code></pre></td></tr></table></div></figure>


<h2>Conclusion</h2>

<p>OpenShift is a feature-rich container platform. In this blog post, we were only able to scratch the surface.</p>

<p>Are you considering to host your containerized application on OpenShift? Or are you already running your production apps on OpenShift? I would like to hear your experiences, please, leave your comments below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Centralized Configuration Management, Our Approach]]></title>
    <link href="http://alesnosek.com/blog/2017/08/20/centralized-configuration-management/"/>
    <updated>2017-08-20T13:09:28-07:00</updated>
    <id>http://alesnosek.com/blog/2017/08/20/centralized-configuration-management</id>
    <content type="html"><![CDATA[<p>Are you migrating your existing application to the cloud? Are you missing a solution for centralized configuration management? Read on to learn, how we implemented centralized configuration management on top of our existing application and got it ready for the cloud.</p>

<!-- more -->


<p>Our existing application is a <a href="https://en.wikipedia.org/wiki/Service-oriented_architecture">SOA-based</a> application, i.e an application consisting of multiple services that communicate with each other and that are deployed across multiple nodes. Services are configured using configuration files located on the local filesystem. Currently, the operator has to edit multiple configuration files across multiple nodes by hand. In addition to that, for the sake of performance and redundancy, there are multiple instances of each service deployed behind a load balancer. This increases the configuration burden even further as the operator has to keep the configuration files consistent across several instances of the same service.</p>

<p>With the growing number of services and the need to deploy our application into dynamic cloud environments, a centralized configuration management became a necessity.</p>

<h2>Looking for a solution</h2>

<p>It would be possible to leverage the standard DevOps tools like Puppet, Chef or Ansible to manage the configuration files on each of the deployed nodes. For bare metal deployments or when deploying on virtual machines in the cloud, these tools could do a decent job. However, on our way to the cloud, were looking at containerizing all of our services. Furthermore, down the road we would also like to leverage serverless architecture as well. For updating a handful of configuration files inside of a Docker container, Puppet, Chef or Ansible just seem too heavy. Needless to say that these tools would not be usable when considering serverless architecture.</p>

<p>When searching for a solution, we came across the <a href="https://github.com/kelseyhightower/confd">confd</a> and <a href="https://github.com/hashicorp/consul-template">consul-template</a> projects. Both tools are based on the same principle. First, the values of the configuration options are persisted in a backend store. While consul-template can store values in Consul only, confd supports a host of different backends like Consul, etcd, Redis or DynamoDB. Second, a set of templates on the filesystem is populated with the values from the backend store and hence forming valid configuration files. These configuration files are then consumed by the application. We drew a great deal of inspiration from this approach.</p>

<h2>Our approach</h2>

<p>Our centralized configuration management consists of two components: <em>CCS</em> (Centralized Configuration Store) which is a Consul cluster holding the configuration data, and <em>CCT</em> (Centralized Configuration Tool) which is a command-line client. CCT implements two functions. First, it allows the operator to query and modify the configuration values persisted in CCS. Second, it syncs up the configuration files on the local filesystem with their state in CCS. The following diagram depicts the components involved in the centralized configuration management:</p>

<p><img class="left" src="http://alesnosek.com/images/posts/centralized_configuration_management.png"></p>

<p>In addition to the configuration values, the CCS component also stores all the additional data that is needed to completely recreate a given configuration file on the local filesystem. For instance, in the case of an ini file, CCS stores the absolute file path, file owner, file group, file mode, sections of the ini file, ini options with their values and all comment lines. Each ini option is also assigned a type or a set of allowed values and any configuration changes made by the operator are checked against the type information before they are accepted.</p>

<p>Apart from the ini file format, Java properties and XML files are also supported. The configuration management verifies that the XML file either conforms to a specific XML schema or is well-formed, before it is accepted. Lastly, all other configuration files that are not parsed by the configuration management are marked as &ldquo;unmanaged&rdquo; and the entire content of such a file is stored under a single key in the key-value store in Consul.</p>

<p>Next, let&rsquo;s review an example scenario where an operator wants to modify a configuration of a specific service. The individual steps are depicted in the diagram above:</p>

<ol>
<li><p>Using the CCT command-line client, the operator obtains a list of configuration files for a specific service managed by CCS. The operator uses CCT to edit the selected configuration file. CCT fetches all the data from CCS that are required to recreate the configuration file and presents it to the operator for editing (for example by opening the file in the operator&rsquo;s favorite editor).</p></li>
<li><p>After the operator made changes to the configuration file, the CCT parses the file to find out which values have been modified. The modified values are checked for corectness by CCT before they are saved in CCS.</p></li>
<li><p>Upon request, CCT fetches the configuration data from CCS in order to use it in the next step.</p></li>
<li><p>CCT syncs up the configuration files on the local filesystem with the data fetched from CCS.</p></li>
<li><p>The new configuration takes effect after the respective service has been restarted by the operator.</p></li>
</ol>


<p>Overall, CCS is a single source of truth for the application configuration. This is in contrast with the confd or consul-template approach where the configuration values are stored in the backend while the templates are stored on the filesystem. When a new release of the application is deployed, having all configuration data in one place makes the upgrade of the configuration data easier.</p>

<h2>Future directions</h2>

<p>It was important to us to introduce the centralized configuration management into our existing application without breaking the existing operational workflows. For example, operators should be able to edit the configuration files as they did in the previous versions of our application. Also, as the configuration files are written to the filesystem, the existing services continue to work without any modification from the previous versions. Hence the centralized configuration management can be deployed as a truly optional component on top of the existing application.</p>

<p>In the future, when some of our services will be deployed as (Lambda) functions in the serverless environment, those services will need to fetch their configuration by directly contacting CCS. However, nothing will change from the operator&rsquo;s standpoint. The operator will continue editing configuration files even when those won&rsquo;t exist on any filesystem anymore.</p>

<p>Do you use confd or consul-template to configure your application? Or did you build your own centralized configuration management? I would like to hear your comments. Feel free to use the comment section below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[How I Became a Red Hat Certified Architect in One Year]]></title>
    <link href="http://alesnosek.com/blog/2017/08/01/how-i-became-a-red-hat-certified-architect-in-one-year/"/>
    <updated>2017-08-01T22:47:14-07:00</updated>
    <id>http://alesnosek.com/blog/2017/08/01/how-i-became-a-red-hat-certified-architect-in-one-year</id>
    <content type="html"><![CDATA[<p>Roughly a year ago, my boss offered to me a Red Hat Learning Subscription. Because continuous education belongs to the habits of a good software practitioner, I appreciated this opportunity to deepen my knowledge of Red Hat technologies. At that time I didn&rsquo;t have an idea about how much fun I was going to have on my journey to become a Red Hat Certified Architect. Read more, if you want to find out!</p>

<!-- more -->


<p>In Wikipedia, you can find a great <a href="https://en.wikipedia.org/wiki/Red_Hat_Certification_Program">overview</a> of the Red Hat certification program. To achieve the Red Hat Certified Architect level, you have to pass seven exams in total. The certification path starts with the RHCSA and RHCE exams. After becoming RHCE, you will have to pass five more exams based on your choice to obtain the RHCA certificate.</p>

<h2>Starting off with RHCSA and RHCE</h2>

<p><img class="right" src="http://alesnosek.com/images/posts/rhcsa_logo.png" width="130" height="130"></p>

<p>The certification path begins with an entry-level certification - Red Hat Certified System Administrator (<a href="https://www.redhat.com/en/services/certification/rhcsa">RHCSA</a>). In the exam, I had to demonstrate basic Linux administration skills like creating users and groups, managing file system permissions including POSIX access control lists (ACLs), creating cron jobs, basics of SELinux, managing software packages with yum, creating and mounting local file systems, working with LVM, network configuration, mounting NFS and SMB file systems and firewall configuration. I had plenty of time to complete all the exam tasks. Overall, RHCSA didn&rsquo;t seem too difficult to me.</p>

<p><img class="right" src="http://alesnosek.com/images/posts/rhce_logo.png" width="130" height="130"></p>

<p>About two weeks after passing the RHCSA, I took the next exam - the Red Hat Certified Engineer (<a href="https://www.redhat.com/en/services/certification/rhce">RHCE</a>) exam. The main focus of this exam was configuration of a caching DNS server (unbound), SMTP nullclient (Postfix), configuration of an iSCSI target and initiator, configuration of Apache web server including HTTPS, running MariaDB, configuration of NFS and SMB servers and basic shell scripting. Based on the experience from the RHCSA exam, I thought that I would have more than enough of time again to complete my tasks and perhaps make myself a coffee, too. How wrong I was! The RHCE exam is loaded with so many tasks that you will be very busy for the entire 3.5 hours. Due to my rather slow and relaxed approach at the beginning of the exam, I was not able to complete all the tasks in time. In the end, I was very happy that I still passed.</p>

<p>You can find further details about my RHCSA/RHCE experience in <a href="http://alesnosek.com/blog/2016/11/07/rhcsa-slash-rhce-exam-experience/">this</a> blog post.</p>

<h2>Climbing to the top</h2>

<p><img class="right" src="http://alesnosek.com/images/posts/rhca_logo.png" width="130" height="130"></p>

<p>After becoming a Red Hat Certified Engineer, I turned my attention to the Red Hat Certified Architect (<a href="https://www.redhat.com/en/services/certification/rhca">RHCA</a>) certification. To achieve this highest level of certification, I had to pass five additional exams based on my own selection. Red Hat provides about twenty exams to choose from, divided into concentrations like Datacenter, DevOps, Application platform or Cloud. The concentrations are only advisory. You can pick exams across concentrations which I also did.</p>

<p>At the time I was choosing my next exam, I was already working with OpenShift for several months. In order to increase my knowledge of OpenShift, I decided to take the OpenShift certification exam next. You can find my experience from the Red Hat Certificate of Expertise in Platform-as-a-Service (<a href="https://www.redhat.com/en/services/training/ex280-red-hat-certificate-expertise-platform-service-exam">EX280</a>) exam in <a href="http://alesnosek.com/blog/2017/04/04/passed-the-openshift-ex280-certification/">this</a> blog post. I passed this exam with my lowest score ever but yeah, I did pass.</p>

<p>During the preparation for the OpenShift exam, I had to work with Docker containers rather intensively. It would make sense to continue down the container route and take the container exams <a href="https://www.redhat.com/en/services/training/ex270-red-hat-certificate-expertise-atomic-host-container-administration">EX270</a> and <a href="https://www.redhat.com/en/services/training/ex276-red-hat-certificate-expertise-containerized-application-development">EX276</a> next. However, I realized that the OpenStack certification exams which I planned to take, too, were not available at my location. As I already had travel plans to Prague where the OpenStack exams were available, I decided to shift the gears and started preparing for the two OpenStack exams Red Hat Certified System Administrator in Red Hat OpenStack (<a href="https://www.redhat.com/en/services/training/ex210-red-hat-certified-system-administrator-red-hat-openstack-exam">EX210</a>) and Red Hat Certified Engineer in Red Hat OpenStack (<a href="https://www.redhat.com/en/services/training/ex310-red-hat-certified-engineer-red-hat-openstack-exam">EX310</a>). I was able to pass these two exams with the maximum score of 300 points as you can read in <a href="http://alesnosek.com/blog/2017/06/26/acing-the-red-hat-openstack-certification-exams/">this</a> article.</p>

<p>The last two exams on my journey were the Red Hat Certificate of Expertise in Atomic Host Container Administration (<a href="https://www.redhat.com/en/services/training/ex270-red-hat-certificate-expertise-atomic-host-container-administration">EX270</a>) and the Red Hat Certificate of Expertise in Containerized Application Development (<a href="https://www.redhat.com/en/services/training/ex276-red-hat-certificate-expertise-containerized-application-development">EX276</a>). I wrote about my experiences from these two exams <a href="http://alesnosek.com/blog/2017/07/29/passed-red-hat-container-certifications-ex270-and-ex276/">here</a>. These two exams concluded my journey and I became a Red Hat Certified Architect.</p>

<p>Red Hat maintains a web page showing a track record of each Red Hat Certified Professional. The list of my Red Hat certifications can be found on this <a href="https://www.redhat.com/rhtapps/certification/verify/?certId=160-216-727">Verify a Red Hat Certified Professional</a> page.</p>

<h2>What&rsquo;s next?</h2>

<p>The RHCA status doesn&rsquo;t last forever as I learned on the Red Hat web page describing how to <a href="http://servicesblog.redhat.com/2016/09/23/stay-current/">stay current</a>. In order for my RHCA to stay current, I must maintain five eligible credentials beyond the RHCE level. Certificates that I obtained beyond the RHCE are valid for three years. That means that I must take five exams every three years in order to maintain my RHCA status. Those five exams can be the same five exams I already passed before or I can pick any other exams from the selection of exams eligible for RHCA. Besides that, as a RHCA I&rsquo;m entitled to a 50 percent discount on the <a href="https://www.redhat.com/en/about/videos/red-hat-learning-subscription-standard">Red Hat Learning Subscription - Standard</a>. This subscription includes all the prep materials and five first exam attempts.</p>

<p>It seems to me that the best way to maintain my RHCA status would be to buy the Learning Subscription every three years and use it to pass five exams. Currently, the Learning Subscription costs $7000 which would be $3500 after the 50 percent discount. In three years from now, I&rsquo;m looking forward to the respective conversation with my boss ;-)</p>

<p>Are you a fellow RHCA? Would you like to share some details about your certification journey? And do you plan do maintain your RHCA? I would like to hear from you, feel free to leave your comments below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Passed Red Hat Container Certifications EX270 and EX276]]></title>
    <link href="http://alesnosek.com/blog/2017/07/29/passed-red-hat-container-certifications-ex270-and-ex276/"/>
    <updated>2017-07-29T22:44:56-07:00</updated>
    <id>http://alesnosek.com/blog/2017/07/29/passed-red-hat-container-certifications-ex270-and-ex276</id>
    <content type="html"><![CDATA[<p>Recently I passed two Red Hat certification exams: <a href="https://www.redhat.com/en/services/training/ex270-red-hat-certificate-expertise-atomic-host-container-administration">EX270 Red Hat Certificate of Expertise in Atomic Host Container Administration</a> and the <a href="https://www.redhat.com/en/services/training/ex276-red-hat-certificate-expertise-containerized-application-development">EX276 Red Hat Certificate of Expertise in Containerized Application Development</a>. I&rsquo;d like to share some of my experience with you in this blog post.</p>

<!-- more -->


<p><img class="right" src="http://alesnosek.com/images/posts/redhat_logo.png" width="110" height="110"></p>

<p>As you might already know, all the Red hat certification exams are purely practical. You&rsquo;ll have to configure the provided virtual machines based on the list of instructions given to you in the exam.</p>

<p><img class="right" src="http://alesnosek.com/images/posts/docker_logo.png" width="100" height="100"></p>

<p>From my experience, I can say that the scope of exams EX270 and EX276 is highly overlapping. Both exams are focused on working with Docker, concretely: creating Docker images using Dockerfiles, pulling and pushing images into Docker registries, running Docker containers, linking Docker containers, creating and sharing volumes between containers, exposing network ports. This pretty much covers the content of the EX276 exam and the major part of the EX270 exam. In addition, the exam EX270 includes several tasks related to Red Hat Atomic Host.</p>

<p>To prepare for the exams, I used the online courses Managing Containers with Red Hat Enterprise Linux Atomic Host (RH270R) and Containerizing Software Applications (DO276R) that are included in my <a href="https://www.redhat.com/en/services/training/learning-subscription">Red Hat Learning Subscription</a>. In comparison to prep materials for other exams, these materials are not that large but they covered the exam requirements well.</p>

<p>And how did I score? I achieved 260 points on the EX270 exam and 291 points on the EX276 exam. For both exams, the minimum passing score was 210 points and the maximum score was 300. Subjectively, I found the difficulty of the EX270 to be on par with the other Red Hat exams I did in the past. On the other hand, the EX276 appeared to me as a rather easy exam. I was able to complete all my tasks, including double-checking, 45 minutes before the exam end.</p>

<p>Passing these two exams concluded my <a href="https://www.redhat.com/en/services/certification/rhca">RHCA</a> journey and earned me the Red Hat Certified Architect certificate. Here is the obligatory link to my current list of certifications on the <a href="https://www.redhat.com/rhtapps/certification/verify/?certId=160-216-727">Verify a Red Hat Certified Professional</a> website.</p>

<p>I believe that many folks are on their way to become RHCA. How are you doing on your journey? Would you like to share your experiences with the Red Hat certification program? Please, leave your comments below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Check-and-Set Operation and Transactions in Consul]]></title>
    <link href="http://alesnosek.com/blog/2017/07/25/check-and-set-operation-and-transactions-in-consul/"/>
    <updated>2017-07-25T23:07:07-07:00</updated>
    <id>http://alesnosek.com/blog/2017/07/25/check-and-set-operation-and-transactions-in-consul</id>
    <content type="html"><![CDATA[<p>In the <a href="http://alesnosek.com/blog/2017/07/15/first-look-at-the-key-value-store-in-consul/">previous</a> blog post, we were checking out the basic functionality of the key-value store in Consul. In this article, we will explore two of the more advanced features of Consul&rsquo;s key-value store, namely: Check-and-Set operation and transactions.</p>

<!-- more -->


<p><img class="right" src="http://alesnosek.com/images/posts/consul_logo.png" width="200" height="300"></p>

<p>For our experimenting, let&rsquo;s start a one-node Consul cluster. The meaning of the individual command-line parameters was described in the <a href="http://alesnosek.com/blog/2017/07/15/first-look-at-the-key-value-store-in-consul/">previous</a> article:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul agent -ui -server -data-dir mydata -advertise 127.0.0.1 -bootstrap-expect 1
</span></code></pre></td></tr></table></div></figure>


<p>In a short moment, the one-node Consul cluster should be up and ready. In the following, we&rsquo;re going to leverage Consul&rsquo;s <a href="https://www.consul.io/api/index.html">HTTP API</a> as not all the desired functionality is exposed via the command-line client. First, let&rsquo;s verify that the Consul cluster is working properly. For that, we&rsquo;ll ask it to provide us with a list of cluster nodes:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl http://localhost:8500/v1/catalog/nodes?pretty
</span><span class='line'><span class="o">[</span>
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>        <span class="s2">&quot;ID&quot;</span>: <span class="s2">&quot;be79786e-749d-758c-2b65-824c1e956788&quot;</span>,
</span><span class='line'>        <span class="s2">&quot;Node&quot;</span>: <span class="s2">&quot;zihadlo&quot;</span>,
</span><span class='line'>        <span class="s2">&quot;Address&quot;</span>: <span class="s2">&quot;127.0.0.1&quot;</span>,
</span><span class='line'>        <span class="s2">&quot;Datacenter&quot;</span>: <span class="s2">&quot;dc1&quot;</span>,
</span><span class='line'>        <span class="s2">&quot;TaggedAddresses&quot;</span>: <span class="o">{</span>
</span><span class='line'>            <span class="s2">&quot;lan&quot;</span>: <span class="s2">&quot;127.0.0.1&quot;</span>,
</span><span class='line'>            <span class="s2">&quot;wan&quot;</span>: <span class="s2">&quot;127.0.0.1&quot;</span>
</span><span class='line'>        <span class="o">}</span>,
</span><span class='line'>        <span class="s2">&quot;Meta&quot;</span>: <span class="o">{}</span>,
</span><span class='line'>        <span class="s2">&quot;CreateIndex&quot;</span>: 5,
</span><span class='line'>        <span class="s2">&quot;ModifyIndex&quot;</span>: 6
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>The response from Consul contains information about the single node which is what we expected.</p>

<h2>Check-and-Set operation</h2>

<p>The purpose of the Check-and-Set operation is to avoid lost updates when multiple clients are simultaneously trying to update a value of the same key. Check-and-Set operation allows the update to happen only if the value has not been changed since the client last read it. If the current value does not match what the client previously read, the client will receive a conflicting update error message and will have to retry the read-update cycle.</p>

<p>The Check-and-Set operation can be used to implement a shared counter, semaphore or a distributed lock. Let&rsquo;s demonstrate how to create a basic distributed lock using the Check-and-Set operation. We&rsquo;ll start with creating a key that will represent our lock:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl --request PUT http://localhost:8500/v1/kv/mylock --data <span class="s2">&quot;&quot;</span>
</span><span class='line'><span class="nb">true</span>
</span></code></pre></td></tr></table></div></figure>


<p>We created the <code>mylock</code> key holding an empty value. The empty value signalizes that the lock is not taken. Before trying to acquire the lock, each client has to check whether the lock is unlocked:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl http://localhost:8500/v1/kv/mylock?pretty
</span><span class='line'><span class="o">[</span>
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>        <span class="s2">&quot;LockIndex&quot;</span>: 0,
</span><span class='line'>        <span class="s2">&quot;Key&quot;</span>: <span class="s2">&quot;mylock&quot;</span>,
</span><span class='line'>        <span class="s2">&quot;Flags&quot;</span>: 0,
</span><span class='line'>        <span class="s2">&quot;Value&quot;</span>: null,
</span><span class='line'>        <span class="s2">&quot;CreateIndex&quot;</span>: 5638,
</span><span class='line'>        <span class="s2">&quot;ModifyIndex&quot;</span>: 5638
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>The value of the key in the Consul&rsquo;s response is still empty (null) which indicates that nobody is holding the lock. The second important item in the Consul&rsquo;s response is the <code>ModifyIndex</code>. Each key in the key-value store has its own <code>ModifyIndex</code>. The <code>ModifyIndex</code> is incremented by Consul each time the respective key is modified.</p>

<p>After verifying that the lock is not taken, the client can try to acquire it:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl --request PUT http://localhost:8500/v1/kv/mylock?cas<span class="o">=</span><span class="m">5638</span> --data <span class="s2">&quot;client1&quot;</span>
</span><span class='line'><span class="nb">true</span>
</span></code></pre></td></tr></table></div></figure>


<p>The client is trying to update the value of the key <code>mylock</code>. The value of the <code>ModifyIndex</code> is passed along as the query parameter <code>cas=5638</code> (cas meaning Check-and-Set). Because the query parameter <code>cas=5638</code> is specified in the request, Consul will update the value of the <code>mylock</code> key only if the current <code>ModifyIndex</code> of the <code>mylock</code> key matches 5638. In other words, the key has not been updated since the client last read it. In our example, the update was successful and the client is now holding the lock. Note that an arbitrary non-empty value can be stored under the <code>mylock</code> key. We chose to use the identification of the client that has acquired the lock.</p>

<p>Let&rsquo;s pretend that at the same time a second client was competing for the lock. The <code>client2</code> was trying to acquire the lock by sending this request to Consul including the same query parameter <code>cas=5638</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl --request PUT http://localhost:8500/v1/kv/mylock?cas<span class="o">=</span><span class="m">5638</span> --data <span class="s2">&quot;client2&quot;</span>
</span><span class='line'><span class="nb">false</span>
</span></code></pre></td></tr></table></div></figure>


<p>Consul&rsquo;s response sent to <code>client2</code> shows that Consul refused to update the <code>mylock</code> value as, in the meantime, this value has been modified. In order to check the current status of the lock, <code>client2</code> can follow up with a get request:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl http://localhost:8500/v1/kv/mylock?pretty
</span><span class='line'><span class="o">[</span>
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>        <span class="s2">&quot;LockIndex&quot;</span>: 0,
</span><span class='line'>        <span class="s2">&quot;Key&quot;</span>: <span class="s2">&quot;mylock&quot;</span>,
</span><span class='line'>        <span class="s2">&quot;Flags&quot;</span>: 0,
</span><span class='line'>        <span class="s2">&quot;Value&quot;</span>: <span class="s2">&quot;Y2xpZW50MQ==&quot;</span>,
</span><span class='line'>        <span class="s2">&quot;CreateIndex&quot;</span>: 5638,
</span><span class='line'>        <span class="s2">&quot;ModifyIndex&quot;</span>: 5801
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>In Consul&rsquo;s response we can see that the lock is currently being held by <code>client1</code>. Until <code>client1</code> hasn&rsquo;t released the lock, <code>client2</code> must not try to acquire it. It can only periodically check the status of the lock and wait until it is released. To release the lock, <code>clent1</code> will simply set its value to an empty-value:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>curl --request PUT http://localhost:8500/v1/kv/mylock --data <span class="s2">&quot;&quot;</span>
</span><span class='line'><span class="nb">true</span>
</span></code></pre></td></tr></table></div></figure>


<p>There are two more comments to add. First, the lock we implemented is purely advisory. All the clients working with the lock have to follow the same rules for the lock to function properly. Each client has to check that the lock was not acquired by somebody else before trying to acquire it. A misbehaved client can easily break the lock. Second, if the client holding the lock fails to release it (e.g. client crashes before releasing the lock), the lock will remain locked and no other client will be able to acquire it. More robust locks that are automatically released in the case of client failure can be implemented using the Consul&rsquo;s <a href="https://www.consul.io/docs/internals/sessions.html">sessions</a> along with the acquire and release operations.</p>

<h2>Leveraging the parameter cas=0</h2>

<p>In our lock implementation, we created an opened lock first and the lock acquisition comprised of two steps. In the first step, the client read the current <code>ModifyIndex</code> of the lock. In the second step, the client tried to update the lock while passing the <code>ModifyIndex</code> as a <code>cas</code> query parameter. When implementing the lock, we could have alternatively leveraged the fact that if the <code>cas</code> parameter is set to <code>0</code>, Consul will only create the key in the key-value store if it does not already exist. The state of our lock would then correspond to the existence or non-existence of the respective key in the key-value store. In order to acquire the lock, the client would send a request to create the key:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl --request PUT localhost:8500/v1/kv/mykey2?cas<span class="o">=</span><span class="m">0</span> --data <span class="s1">&#39;client1&#39;</span>
</span><span class='line'><span class="nb">true</span>
</span></code></pre></td></tr></table></div></figure>


<p>And to release the lock, the client would simply remove the respective key from the key-value store:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl --request DELETE localhost:8500/v1/kv/mykey2
</span><span class='line'><span class="nb">true</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Transactions</h2>

<p><a href="https://www.consul.io/api/txn.html">Transactions</a> in Consul manage updates or selects of multiple keys within a single, atomic transaction. A list of operations that will be executed in the transaction is specified in the body of the HTTP request. First, let&rsquo;s create a list of operations and save it as a file <code>transaction1.txt</code>:</p>

<figure class='code'><figcaption><span>transaction1.txt </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>
</span><span class='line'>  <span class="o">{</span>
</span><span class='line'>    <span class="s2">&quot;KV&quot;</span>: <span class="o">{</span>
</span><span class='line'>      <span class="s2">&quot;Verb&quot;</span>: <span class="s2">&quot;set&quot;</span>,
</span><span class='line'>      <span class="s2">&quot;Key&quot;</span>: <span class="s2">&quot;foo&quot;</span>,
</span><span class='line'>      <span class="s2">&quot;Value&quot;</span>: <span class="s2">&quot;MQ==&quot;</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>,
</span><span class='line'>  <span class="o">{</span>
</span><span class='line'>    <span class="s2">&quot;KV&quot;</span>: <span class="o">{</span>
</span><span class='line'>      <span class="s2">&quot;Verb&quot;</span>: <span class="s2">&quot;set&quot;</span>,
</span><span class='line'>      <span class="s2">&quot;Key&quot;</span>: <span class="s2">&quot;bar&quot;</span>,
</span><span class='line'>      <span class="s2">&quot;Value&quot;</span>: <span class="s2">&quot;Mg==&quot;</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Our transaction doesn&rsquo;t do anything spectacular. It just creates two key-value pairs <code>foo=1</code> and <code>bar=2</code>. Let&rsquo;s submit the transaction to Consul:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>curl --request PUT <span class="s1">&#39;localhost:8500/v1/txn?pretty&#39;</span> --data @transaction1.txt
</span><span class='line'><span class="o">{</span>
</span><span class='line'>    <span class="s2">&quot;Results&quot;</span>: <span class="o">[</span>
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>            <span class="s2">&quot;KV&quot;</span>: <span class="o">{</span>
</span><span class='line'>                <span class="s2">&quot;LockIndex&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;Key&quot;</span>: <span class="s2">&quot;foo&quot;</span>,
</span><span class='line'>                <span class="s2">&quot;Flags&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;Value&quot;</span>: null,
</span><span class='line'>                <span class="s2">&quot;CreateIndex&quot;</span>: 7267,
</span><span class='line'>                <span class="s2">&quot;ModifyIndex&quot;</span>: 7267
</span><span class='line'>            <span class="o">}</span>
</span><span class='line'>        <span class="o">}</span>,
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>            <span class="s2">&quot;KV&quot;</span>: <span class="o">{</span>
</span><span class='line'>                <span class="s2">&quot;LockIndex&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;Key&quot;</span>: <span class="s2">&quot;bar&quot;</span>,
</span><span class='line'>                <span class="s2">&quot;Flags&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;Value&quot;</span>: null,
</span><span class='line'>                <span class="s2">&quot;CreateIndex&quot;</span>: 7267,
</span><span class='line'>                <span class="s2">&quot;ModifyIndex&quot;</span>: 7267
</span><span class='line'>            <span class="o">}</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>    <span class="o">]</span>,
</span><span class='line'>    <span class="s2">&quot;Errors&quot;</span>: null
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The transaction completed successfully. In the response from Consul, we can find the list of results. The order of results corresponds to the order of operations that we submitted in our request. The value of the <code>ModifyIndex</code> <code>7267</code> is the same for both keys <code>foo</code> and <code>bar</code> as they were updated in the same transaction.</p>

<p>Next, let&rsquo;s see what happens if one of the operations in the transaction fails. To demonstrate this, we&rsquo;ll create a transaction that consists of two operations. The first operation updates the key <code>foo</code> to value <code>10</code>. The second operation updates the key <code>bar</code> to value <code>20</code> but only if the <code>ModifyIndex</code> of <code>bar</code> matches 100. We know that this condition is not fulfilled and the update should fail.</p>

<figure class='code'><figcaption><span>transaction2.txt </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>
</span><span class='line'>  <span class="o">{</span>
</span><span class='line'>    <span class="s2">&quot;KV&quot;</span>: <span class="o">{</span>
</span><span class='line'>      <span class="s2">&quot;Verb&quot;</span>: <span class="s2">&quot;set&quot;</span>,
</span><span class='line'>      <span class="s2">&quot;Key&quot;</span>: <span class="s2">&quot;foo&quot;</span>,
</span><span class='line'>      <span class="s2">&quot;Value&quot;</span>: <span class="s2">&quot;MTA=&quot;</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>,
</span><span class='line'>  <span class="o">{</span>
</span><span class='line'>    <span class="s2">&quot;KV&quot;</span>: <span class="o">{</span>
</span><span class='line'>      <span class="s2">&quot;Verb&quot;</span>: <span class="s2">&quot;cas&quot;</span>,
</span><span class='line'>      <span class="s2">&quot;Index&quot;</span>: 100,
</span><span class='line'>      <span class="s2">&quot;Key&quot;</span>: <span class="s2">&quot;bar&quot;</span>,
</span><span class='line'>      <span class="s2">&quot;Value&quot;</span>: <span class="s2">&quot;MjA=&quot;</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Let&rsquo;s submit the transaction to Consul:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>curl --request PUT <span class="s1">&#39;localhost:8500/v1/txn?pretty&#39;</span> --data @transaction2.txt
</span><span class='line'><span class="o">{</span>
</span><span class='line'>    <span class="s2">&quot;Results&quot;</span>: null,
</span><span class='line'>    <span class="s2">&quot;Errors&quot;</span>: <span class="o">[</span>
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>            <span class="s2">&quot;OpIndex&quot;</span>: 1,
</span><span class='line'>            <span class="s2">&quot;What&quot;</span>: <span class="s2">&quot;failed to set key \&quot;bar\&quot;, index is stale&quot;</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>    <span class="o">]</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The transaction failed, indeed. The returned error list contains all errors that occurred during the transaction processing. The operations that failed are denoted by the <code>OpIndex</code> which starts from value 0. In the example output we can see that the second operation in our transaction failed because of the stale index. Let&rsquo;s check the values of the keys <code>foo</code> and <code>bar</code> after the failed transaction:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv get foo
</span><span class='line'>1
</span><span class='line'><span class="nv">$ </span>./consul kv get bar
</span><span class='line'>2
</span></code></pre></td></tr></table></div></figure>


<p>As expected, due to the failed udpate the entire transaction has been rolled back. Keys <code>foo</code> and <code>bar</code> retained their original values <code>1</code> and <code>2</code>.</p>

<h2>Conclusion</h2>

<p>In this blog post, we explored the Check-and-Set operation supported by Consul and used it to implement a simple distributed lock. In the second part of the article, we poked into the transaction capabilities of Consul.</p>

<p>And what about you? How is your experience with using Consul for distributed locking or leader election? Did you get a chance to use transactions? I would like to hear your experiences, feel free to use the comment section below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[First Look at the Key-Value Store in Consul]]></title>
    <link href="http://alesnosek.com/blog/2017/07/15/first-look-at-the-key-value-store-in-consul/"/>
    <updated>2017-07-15T20:38:38-07:00</updated>
    <id>http://alesnosek.com/blog/2017/07/15/first-look-at-the-key-value-store-in-consul</id>
    <content type="html"><![CDATA[<p>If you are developing a distributed application that consists of multiple services, you might be thinking about how to manage the ever growing application configuration data. Instead of maintaining individual configuration files for each service, you can store all your configuration data in a key-value store. In this blog post we&rsquo;ll check out the key-value store in Consul.</p>

<!-- more -->


<p><img class="right" src="http://alesnosek.com/images/posts/consul_logo.png" width="200" height="300"></p>

<p>Consul is an open-source product developed by <a href="https://www.hashicorp.com/">HashiCorp</a> and licensed under the <a href="https://github.com/hashicorp/consul/blob/master/LICENSE">MPL 2.0</a>. While Consul uses an open core business model, it comes with a great deal of functionality in its free edition. The top two features of Consul would be the service discovery combined with health checking and the key-value store functionality that we are going to review in this article. They both come handy when building distributed applications.</p>

<h2>Getting started</h2>

<p>HashiCorp products are known for its thorough documentation and the Consul&rsquo;s <a href="https://www.consul.io/docs/index.html">documenation</a> is not an exception.</p>

<p>What I like about Consul is its installation. Written in the Go language, Consul is distributed as a single statically linked binary. Download <a href="https://www.consul.io/downloads.html">links</a> for various platforms are provided. After unzipping the distribution archive you can directly run the <code>consul</code> executable.</p>

<p>Let&rsquo;s put togher a command-line to start the Consul cluster. Our test cluster consists of a single node (<code>-bootstrap-expect 1</code>). For a production deployment, you should be looking at a cluster of three or five Consul nodes that is able to survive node failures. We will make the Consul Web UI available at <code>http://localhost:8500</code> by appending the <code>-ui</code> parameter. Consul needs a location where it will persist its data. In our example, we instruct Consul to create a directory <code>mydata</code> and store all its data in this directory. After a bit of typing, the complete commmand-line to start the Consul cluster looks as follows:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul agent -ui -server -data-dir mydata -advertise 127.0.0.1 -bootstrap-expect 1
</span></code></pre></td></tr></table></div></figure>


<p>In several seconds the one-node Consul cluster is up and running:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">==</span>&gt; WARNING: BootstrapExpect Mode is specified as 1<span class="p">;</span> this is the same as Bootstrap mode.
</span><span class='line'><span class="o">==</span>&gt; WARNING: Bootstrap mode enabled! Do not <span class="nb">enable </span>unless <span class="nv">necessary</span>
</span><span class='line'><span class="o">==</span>&gt; Starting Consul agent...
</span><span class='line'><span class="o">==</span>&gt; Consul agent running!
</span><span class='line'>           Version: <span class="s1">&#39;v0.8.5&#39;</span>
</span><span class='line'>           Node ID: <span class="s1">&#39;be79786e-749d-758c-2b65-824c1e956788&#39;</span>
</span><span class='line'>         Node name: <span class="s1">&#39;zihadlo&#39;</span>
</span><span class='line'>        Datacenter: <span class="s1">&#39;dc1&#39;</span>
</span><span class='line'>            Server: <span class="nb">true</span> <span class="o">(</span>bootstrap: <span class="nb">true</span><span class="o">)</span>
</span><span class='line'>       Client Addr: 127.0.0.1 <span class="o">(</span>HTTP: 8500, HTTPS: -1, DNS: 8600<span class="o">)</span>
</span><span class='line'>      Cluster Addr: 127.0.0.1 <span class="o">(</span>LAN: 8301, WAN: 8302<span class="o">)</span>
</span><span class='line'>    Gossip encrypt: <span class="nb">false</span>, RPC-TLS: <span class="nb">false</span>, TLS-Incoming: <span class="nb">false</span>
</span><span class='line'>
</span><span class='line'><span class="o">==</span>&gt; Log data will now stream in as it occurs:
</span><span class='line'>
</span><span class='line'>    2017/07/15 20:37:57 <span class="o">[</span>INFO<span class="o">]</span> raft: Initial configuration <span class="o">(</span><span class="nv">index</span><span class="o">=</span>1<span class="o">)</span>: <span class="o">[{</span>Suffrage:Voter ID:127.0.0.1:8300 Address:127.0.0.1:8300<span class="o">}]</span>
</span><span class='line'>    2017/07/15 20:37:57 <span class="o">[</span>INFO<span class="o">]</span> raft: Node at 127.0.0.1:8300 <span class="o">[</span>Follower<span class="o">]</span> entering Follower state <span class="o">(</span>Leader: <span class="s2">&quot;&quot;</span><span class="o">)</span>
</span><span class='line'>    2017/07/15 20:37:57 <span class="o">[</span>INFO<span class="o">]</span> serf: EventMemberJoin: zihadlo 127.0.0.1
</span><span class='line'>    2017/07/15 20:37:57 <span class="o">[</span>INFO<span class="o">]</span> consul: Adding LAN server zihadlo <span class="o">(</span>Addr: tcp/127.0.0.1:8300<span class="o">)</span> <span class="o">(</span>DC: dc1<span class="o">)</span>
</span><span class='line'>    2017/07/15 20:37:57 <span class="o">[</span>INFO<span class="o">]</span> serf: EventMemberJoin: zihadlo.dc1 127.0.0.1
</span><span class='line'>    2017/07/15 20:37:57 <span class="o">[</span>INFO<span class="o">]</span> consul: Handled member-join event <span class="k">for</span> server <span class="s2">&quot;zihadlo.dc1&quot;</span> in area <span class="s2">&quot;wan&quot;</span>
</span><span class='line'>    2017/07/15 20:37:57 <span class="o">[</span>INFO<span class="o">]</span> agent: Started DNS server 127.0.0.1:8600 <span class="o">(</span>udp<span class="o">)</span>
</span><span class='line'>    2017/07/15 20:37:57 <span class="o">[</span>INFO<span class="o">]</span> agent: Started DNS server 127.0.0.1:8600 <span class="o">(</span>tcp<span class="o">)</span>
</span><span class='line'>    2017/07/15 20:37:57 <span class="o">[</span>INFO<span class="o">]</span> agent: Started HTTP server on 127.0.0.1:8500
</span><span class='line'>    2017/07/15 20:38:02 <span class="o">[</span>WARN<span class="o">]</span> raft: Heartbeat timeout from <span class="s2">&quot;&quot;</span> reached, starting election
</span><span class='line'>    2017/07/15 20:38:02 <span class="o">[</span>INFO<span class="o">]</span> raft: Node at 127.0.0.1:8300 <span class="o">[</span>Candidate<span class="o">]</span> entering Candidate state in term 2
</span><span class='line'>    2017/07/15 20:38:02 <span class="o">[</span>INFO<span class="o">]</span> raft: Election won. Tally: 1
</span><span class='line'>    2017/07/15 20:38:02 <span class="o">[</span>INFO<span class="o">]</span> raft: Node at 127.0.0.1:8300 <span class="o">[</span>Leader<span class="o">]</span> entering Leader state
</span><span class='line'>    2017/07/15 20:38:02 <span class="o">[</span>INFO<span class="o">]</span> consul: cluster leadership acquired
</span><span class='line'>    2017/07/15 20:38:02 <span class="o">[</span>INFO<span class="o">]</span> consul: New leader elected: zihadlo
</span><span class='line'>    2017/07/15 20:38:02 <span class="o">[</span>INFO<span class="o">]</span> consul: member <span class="s1">&#39;zihadlo&#39;</span> joined, marking health alive
</span><span class='line'>    2017/07/15 20:38:02 <span class="o">[</span>INFO<span class="o">]</span> agent: Synced service <span class="s1">&#39;consul&#39;</span>
</span></code></pre></td></tr></table></div></figure>


<p>In the log output, Consul informs us that the Consul API is available at 127.0.0.1:8500. That&rsquo;s where the Consul client will connect to by default. In the following, you want to make sure that you&rsquo;re running the Consul commands on the same box as you started your Consul cluster.</p>

<p>The single <code>consul</code> binary provides the server as well as the client functionality. Let&rsquo;s list our current cluster members to verify that the client can connect to the cluster:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul members
</span><span class='line'>Node     Address         Status  Type    Build  Protocol  DC
</span><span class='line'>zihadlo  127.0.0.1:8301  alive   server  0.8.5  <span class="m">2</span>         dc1
</span></code></pre></td></tr></table></div></figure>


<h2>Basic CRUD with Consul</h2>

<p>In this section, we&rsquo;re going to exercise the basic Create, Read, Update and Delete functionality of the Consul key-store. First, let&rsquo;s store the value <code>12345</code> under the key <code>foo</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv put foo 12345
</span><span class='line'>Success! Data written to: foo
</span></code></pre></td></tr></table></div></figure>


<p>Great, the value is saved in the store. To retrieve the value under the key <code>foo</code> from Consul we can type:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv get foo
</span><span class='line'>12345
</span></code></pre></td></tr></table></div></figure>


<p>By the way, Consul doesn&rsquo;t impose any restrictions on what kind of data you may store. Only the size of the data is limited to 512KB of data per key. It&rsquo;s up to your application, what data format you choose to use. For example, you can decide to store numbers, strings, JSON-formatted data or arbitrary binary data. For instance, when designing a centralized configuration management solution for your application, you have the flexibility of storing individual configuration options as key-value pairs or decide to save entire configuration files as values in Consul.</p>

<p>To replace the value, simply put a new value in Consul under the existing key:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv put foo bar
</span><span class='line'>Success! Data written to: foo
</span></code></pre></td></tr></table></div></figure>


<p>The value has been successfully updated as we can see:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv get foo
</span><span class='line'>bar
</span></code></pre></td></tr></table></div></figure>


<p>To remove the value from the key-value store you can use the <code>delete</code> command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv delete foo
</span><span class='line'>Success! Deleted key: foo
</span></code></pre></td></tr></table></div></figure>


<p>To verify that the value is really gone, try to retrieve it:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv get foo
</span><span class='line'>Error! No key exists at: foo
</span></code></pre></td></tr></table></div></figure>


<h2>Hierarchical keys and prefix matching</h2>

<p>Keys in Consul can be organized in a hierarchy where different levels of the hierarchy are separated by the slash character (<code>/</code>). For example, you can create a database that holds the population numbers in different continents and countries (in millions of inhabitants) like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv put europe 743.1
</span><span class='line'><span class="nv">$ </span>./consul kv put europe/germany 82.67
</span><span class='line'><span class="nv">$ </span>./consul kv put europe/france 66.9
</span><span class='line'><span class="nv">$ </span>./consul kv put asia 4436
</span><span class='line'><span class="nv">$ </span>./consul kv put asia/india 1324
</span></code></pre></td></tr></table></div></figure>


<p>Now that you organized your keys hierarchically, you can use the Consul&rsquo;s prefix matching to discover the keys on the single level of hierarchy. For example, to retrive the keys with the prefix <code>e</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv get -recurse -keys e
</span><span class='line'>europe
</span><span class='line'>europe/
</span></code></pre></td></tr></table></div></figure>


<p>Prefix matching can be used to retrieve the values, too. For example, to retrieve the population numbers in Europe, you can type:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv get -recurse e
</span><span class='line'>europe:743.1
</span><span class='line'>europe/france:66.9
</span><span class='line'>europe/germany:82.67
</span></code></pre></td></tr></table></div></figure>


<p>Note that when retrieving the keys recursively, only the keys on the single level of hierarchy were returned whereas when retrieving the values recursively, values on all the nested levels of hierarchy were returned.</p>

<p>To obtain the population numbers for the European countries, you can append a slash to the keys name (<code>europe/</code>):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv get -recurse europe/
</span><span class='line'>europe/france:66.9
</span><span class='line'>europe/germany:82.67
</span></code></pre></td></tr></table></div></figure>


<p>And if you are interested only in the European countries that start with letter <code>g</code>, you can use the prefix <code>europe/g</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>./consul kv get -recurse europe/g
</span><span class='line'>europe/germany:82.67
</span></code></pre></td></tr></table></div></figure>


<h2>Export/import of key-value pairs</h2>

<p>Another useful feaure of the Consul&rsquo;s key-value store is the bulk export and import of key-value pairs. To export the entire key-value store database, you can type:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv <span class="nb">export</span>
</span><span class='line'><span class="o">[</span>
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>                <span class="s2">&quot;key&quot;</span>: <span class="s2">&quot;asia&quot;</span>,
</span><span class='line'>                <span class="s2">&quot;flags&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;value&quot;</span>: <span class="s2">&quot;NDQzNg==&quot;</span>
</span><span class='line'>        <span class="o">}</span>,
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>                <span class="s2">&quot;key&quot;</span>: <span class="s2">&quot;asia/india&quot;</span>,
</span><span class='line'>                <span class="s2">&quot;flags&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;value&quot;</span>: <span class="s2">&quot;MTMyNA==&quot;</span>
</span><span class='line'>        <span class="o">}</span>,
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>                <span class="s2">&quot;key&quot;</span>: <span class="s2">&quot;europe&quot;</span>,
</span><span class='line'>                <span class="s2">&quot;flags&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;value&quot;</span>: <span class="s2">&quot;NzQzLjE=&quot;</span>
</span><span class='line'>        <span class="o">}</span>,
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>                <span class="s2">&quot;key&quot;</span>: <span class="s2">&quot;europe/france&quot;</span>,
</span><span class='line'>                <span class="s2">&quot;flags&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;value&quot;</span>: <span class="s2">&quot;NjYuOQ==&quot;</span>
</span><span class='line'>        <span class="o">}</span>,
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>                <span class="s2">&quot;key&quot;</span>: <span class="s2">&quot;europe/germany&quot;</span>,
</span><span class='line'>                <span class="s2">&quot;flags&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;value&quot;</span>: <span class="s2">&quot;ODIuNjc=&quot;</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'><span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Consul exports the key-value pairs into the JSON format which is currently the only supported format. In the sample output, you can see that all the values are <a href="https://en.wikipedia.org/wiki/Base64">base64</a> encoded. The base64 encoding is commonly used in the text-based formats like JSON and XML to allow embedding of binary data.</p>

<p>You can export a subset of the key-value pairs by specifying the prefix. For instance, to export the data pertaining Europe, you can speficy the <code>europe</code> prefix:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv <span class="nb">export </span>europe
</span><span class='line'><span class="o">[</span>
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>                <span class="s2">&quot;key&quot;</span>: <span class="s2">&quot;europe&quot;</span>,
</span><span class='line'>                <span class="s2">&quot;flags&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;value&quot;</span>: <span class="s2">&quot;NzQzLjE=&quot;</span>
</span><span class='line'>        <span class="o">}</span>,
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>                <span class="s2">&quot;key&quot;</span>: <span class="s2">&quot;europe/france&quot;</span>,
</span><span class='line'>                <span class="s2">&quot;flags&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;value&quot;</span>: <span class="s2">&quot;NjYuOQ==&quot;</span>
</span><span class='line'>        <span class="o">}</span>,
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>                <span class="s2">&quot;key&quot;</span>: <span class="s2">&quot;europe/germany&quot;</span>,
</span><span class='line'>                <span class="s2">&quot;flags&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;value&quot;</span>: <span class="s2">&quot;ODIuNjc=&quot;</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'><span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>To import the JSON-formatted data back to the Consul key-value store, you can use the command <code>./consul kv import</code>.</p>

<h2>Web UI</h2>

<p>Besides the commmand-line client, you can access Consul through its beautiful Web interface. Point your web browser to <a href="http://localhost:8500">http://localhost:8500</a>.</p>

<p><img class="right" src="http://alesnosek.com/images/posts/consul_ui.png"></p>

<h2>Conclusion</h2>

<p>In this blog post, we reviewed the basics of the key-value store in Consul. There are many other cool features of the key-value store that we didn&rsquo;t cover like atomic key updates using Check-and-Set operations, <a href="https://www.consul.io/api/txn.html">transactions</a>, <a href="https://www.consul.io/docs/commands/lock.html">locks</a> or <a href="https://www.consul.io/docs/commands/watch.html">watches</a>. Also, I recommend to you to take a look at the Consul&rsquo;s great <a href="https://www.consul.io/api/index.html">RESTful API</a> that allows you to interact with Consul programatically.</p>

<p>If you&rsquo;re looking for a key-value store that would enhance your distributed application, Consul is definitely a candidate to consider. Besides that, Consul will be ready when you later on realize that service discovery is what you need to address next.</p>

<p>Are you considering or already using Consul at your company? I would like to hear your experiences, please leave your comments below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Acing the Red Hat OpenStack Certification Exams]]></title>
    <link href="http://alesnosek.com/blog/2017/06/26/acing-the-red-hat-openstack-certification-exams/"/>
    <updated>2017-06-26T19:54:19-07:00</updated>
    <id>http://alesnosek.com/blog/2017/06/26/acing-the-red-hat-openstack-certification-exams</id>
    <content type="html"><![CDATA[<p>Recently I passed two OpenStack certification exams from Red Hat: <a href="https://www.redhat.com/en/services/training/ex210-red-hat-certified-system-administrator-red-hat-openstack-exam">EX210 Red Hat Certified System Administrator in Red Hat OpenStack exam</a> and the consecutive <a href="https://www.redhat.com/en/services/training/ex310-red-hat-certified-engineer-red-hat-openstack-exam">EX310 Red Hat Certified Engineer in Red Hat OpenStack exam</a>. In this blog post, I&rsquo;m going to share how I - as a software practitioner - got the job done.</p>

<!-- more -->


<p><img class="right" src="http://alesnosek.com/images/posts/redhat_openstack.jpg" width="250" height="300"></p>

<p>All exams in the Red Hat certification program are purely practical. The first exam EX210 focuses on deployment and administration of OpenStack which includes installation of OpenStack using the Red Hat OpenStack Platform Director, creating OpenStack users, projects, managing user roles, uploading images into Glance, creating Cinder volumes, adding Neutron networks and launching stacks using Heat. The second EX310 exam includes deploying the Ceph storage on multiple nodes, integrating Ceph with OpenStack Nova, Glance and Cinder and configuring various Neutron resources like networks, load balancers and routers.</p>

<p>To prepare for the exams, I used the online courses Red Hat OpenStack Administration I, II, III (<a href="https://www.redhat.com/en/services/training/cl110-red-hat-openstack-administration-i">CL110</a>, <a href="https://www.redhat.com/en/services/training/cl210-red-hat-openstack-administration-ii">CL210</a>, <a href="https://www.redhat.com/en/services/training/cl310-red-hat-openstack-administration-iii">CL310</a>) that are included in my <a href="https://www.redhat.com/en/services/training/learning-subscription">Red Hat Learning Subscription</a>. They covered the exam requirements very well.</p>

<p>I encountered a glitch right when scheduling the exam. As I realized, the OpenStack exams EX210 and EX310 are offered in the selected exam locations only and my San Diego location, where I so far completed all of my Red Hat exams, was not included. Surprise, surprise! However, as I was already planning to visit Prague during my upcoming vacation, I decided to take my exams in Prague, for Prague - as a city of the kings - had the EX210 and EX310 exams available.</p>

<p>As the two OpenStack exams partially overlap, it was a good idea to be preparing for both of them at the same time. I even chose to take the exams on the two consecutive days Thursday and Friday. And how did I score? Well, I made 300 out of 300 points in each of the exams. It could not be any better and I was glad that the sometimes painfully gained experience with OpenStack made itself apparent.</p>

<p>The updated list of my certifications can be found on the <a href="https://www.redhat.com/rhtapps/certification/verify/?certId=160-216-727">Verify a Red Hat Certified Professional</a> website.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[What I Learned at Red Hat Summit 2017]]></title>
    <link href="http://alesnosek.com/blog/2017/05/05/what-i-learned-at-red-hat-summit-2017/"/>
    <updated>2017-05-05T18:46:14-07:00</updated>
    <id>http://alesnosek.com/blog/2017/05/05/what-i-learned-at-red-hat-summit-2017</id>
    <content type="html"><![CDATA[<p>I had the great opportunity to visit the Red Hat Summit 2017. It was hosted in the Boston Convention and Exhibition Center in Boston in May 2-4, 2017. This blog post summarizes the interesting things I learned at the summit.</p>

<!-- more -->


<h2>Major announcements</h2>

<p><img class="right" src="http://alesnosek.com/images/posts/rh_summit.png" width="200" height="200"></p>

<p><strong><a href="https://www.redhat.com/en/about/press-releases/red-hat-and-aws-extend-strategic-alliance-package-access-aws-services-within-red-hat-openshift">Red Hat OpenShift &amp; Amazon Web Services</a>.</strong> Red Hat will make AWS services accessible directly from the OpenShift web console. From within the OpenShift web console developers will be able to provision and configure AWS services such as CloudFront, ElastiCache, ELB, RDS, EMR, RedShift, S3 and Lambda.</p>

<p><strong><a href="https://www.redhat.com/en/about/press-releases/red-hat-unveils-end-end-cloud-native-development-environment-red-hat-openshiftio">Red Hat OpenShift.io</a>.</strong> <a href="https://openshift.io/">OpenShift.io</a> is an online development environment for building container-based applications. OpenShift.io can create a project in GitHub for you to store your source code. The source code editing can be done in the integrated Eclipse Che. OpenShift.io can create a project in OpenShift Online in order to build your application and deploy it. Jenkins pipelines are used to orchestrate the CI/CD process. Currently, OpenShift.io is available in a limited developer preview. You can sign up at <a href="https://openshift.io">https://openshift.io</a></p>

<h2>Sessions attended</h2>

<ul>
<li><strong>The future Red Hat Middleware portfolio: Stacks and services and solutions</strong>

<ul>
<li>Netflix OSS is interesting and Red Hat will support it because customers like it. However, some of the Netflix OSS features might be more efficiently implemented directly in Kubernetes instead of on top of Kubernetes. Netflix had to make architectural choices based on how AWS looked several years ago. AWS evolved since that time.</li>
<li>Architectural evolution: monolith -> n-tier architecture -> microservices.</li>
</ul>
</li>
<li><strong>Red Hat container technology strategy</strong>

<ul>
<li>Kubernetes has won. You may have just not realized it yet.</li>
<li>There are two reasons why the Kubernetes open source project is winning. First, it brings value to people. Second, people are excited to work on it.</li>
<li>Kubernetes = kernel of the cloud operating systems</li>
</ul>
</li>
<li><strong>Reproducible development to live applications with Java and Red Hat CDK</strong>

<ul>
<li>When developing containerized applications, developers can run them locally (outside of a container), locally on <a href="https://github.com/minishift/minishift">MiniShift</a>, or hosted on <a href="https://www.openshift.com/">OpenShift Online</a>.</li>
<li><a href="https://developers.redhat.com/products/cdk">Red Hat Container Development Kit</a> can help you develop container-based applications quickly. It uses MiniShift under the hood.</li>
</ul>
</li>
<li><strong>Modern Java and DevOps lightning talks</strong>

<ul>
<li>You can issue <code>oc cluster up</code> to create a local OpenShift all-in-one cluster (requires Origin >= 1.3).</li>
<li><a href="https://projects.eclipse.org/proposals/eclipse-microprofile">Eclipse MicroProfile</a> project is aimed at optimizing Enterprise Java for the microservices architecture. It focuses, among others, on application configuration, health-checking, fault tolerance and security.</li>
<li>You can use API Gateway (e.g <a href="https://apigee.com/about/cp/api-gateway">Apigee</a>, <a href="https://www.3scale.net/">3scale</a>) to dynamically route traffic into different OpenShift namespaces (test, staging, production).</li>
</ul>
</li>
<li><strong>Atomic BOF</strong>

<ul>
<li>In the future, the classic RHEL will be derived from the RHEL Atomic Host. It means that the new features will appear in the RHEL Atomic Host before being included into RHEL.</li>
</ul>
</li>
<li><strong>Stepping off a cliff: Common sense approaches to cloud security</strong>

<ul>
<li>VMs in the cloud are created and destroyed dynamically. This is one of the challenges for the security team.</li>
</ul>
</li>
<li><strong>Wicked fast PaaS: Performance tuning of OpenShift and Docker</strong>

<ul>
<li>RHEL 7.4 should support OverlayFS. For Docker storage, the OverlayFS is more memory efficient than the LVM thin pool provisioning, as with OverlayFS, the pages in the page cache can be shared between multiple containers.</li>
<li>Beginning with OpenShift 3.5, the container image metadata will be stored only in the Docker registry. In previous versions of OpenShift, a duplicate of the image metadata was stored in etcd, too.</li>
</ul>
</li>
<li><strong>The Truth about Microservices</strong>

<ul>
<li>&ldquo;Building a single microservice is easy. Building a microservices architecture is hard.&rdquo;</li>
</ul>
</li>
<li><strong>The hardest part of microservices is your data</strong>

<ul>
<li><a href="http://debezium.io/">Debezium</a> monitors the changes committed to the database (MySQL, MongoDB, PostgreSQL). For each database change, Debezium publishes an event to the Kafka broker. To consume the change events, an application can create a Kafka consumer that will consume all events for the topics associated with the database. In summary, Debezium turns a database transaction log into a Kafka stream that other applications can consume.</li>
</ul>
</li>
<li><strong>Reactive systems with Eclipse Vert.x and Red Hat OpenShift</strong>

<ul>
<li><a href="http://vertx.io/">Vert.x</a> is a toolkit for building reactive applications on the JVM. It can discover services on OpenShift and Kubernetes.</li>
</ul>
</li>
<li><strong>Container infrastructure trends: Optimizing for production workloads</strong>

<ul>
<li><a href="https://github.com/projectatomic/skopeo">skopeo</a> is a command line utility that allows you to inspect Docker images and image registries. It retrieves the required information from the repository metadata without the need to download the actual image. For example, with skopeo you can find out which tags are available for the given repository.</li>
<li><a href="https://github.com/projectatomic/buildah">buildah</a> a tool for building Docker images. It can build container images without using the Docker daemon. Ansible-container and OpenShift&rsquo;s S2I will be modified to use buildah under the hood.</li>
</ul>
</li>
<li><strong>Function as a Service (Faas) - why you should care and what you need to know</strong>

<ul>
<li>Architectural evolution: service -> microservice -> function.</li>
<li>Good serverless use-cases: processing web-hooks, scheduled tasks (a la cron), data transformation (converting small images).</li>
<li>Serverless architecture challenges: cannot use a larger programming framework to implement the function, as this would be too slow to initialize, increased latency in comparison to a long-running server process, large variance in latency, very complex at scale, debugging of functions is hard.</li>
<li>Red Hat participates on development of <a href="https://funktion.fabric8.io/">Funktion</a> that is part of the project <a href="https://fabric8.io/">fabric8</a>.</li>
</ul>
</li>
</ul>

]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Two Years of Technical Blogging]]></title>
    <link href="http://alesnosek.com/blog/2017/04/22/two-years-of-technical-blogging/"/>
    <updated>2017-04-22T15:06:57-07:00</updated>
    <id>http://alesnosek.com/blog/2017/04/22/two-years-of-technical-blogging</id>
    <content type="html"><![CDATA[<p>Wow, the time goes by so fast. It&rsquo;s been two years since I began writing this blog. Why did I start and am I having fun? Let&rsquo;s take a closer look.</p>

<!-- more -->


<p>I think that there are two kinds of authors. The first kind are the extroverted authors, that can fill pages and pages of paper without difficulty. They love to communicate and writing is yet another way of communication for them. The second kind of authors would be the introverted authors. Communication is not necessarily second nature for them. Before creating an article, they are concerned whether they really have something interesting to share.</p>

<p>It seems to me that in the developer community the introverted authors prevail and I personally count myself in this category, too. Only after a decade of software development, and now heading towards software architecture, I finally decided to start my own technical blog.</p>

<p><img class="right" src="http://alesnosek.com/images/posts/octopress_github.png" width="200" height="200"></p>

<h2>Getting my blog started</h2>

<p>Coming late to the game, there were plenty of technical blogs out there to draw inspiration from. I quickly settled on <a href="http://octopress.org/">Octopress</a> as the engine for my blog. Octopress is a blog generator. I can write articles using the Markdown format and Octopress generates the entire website for me. As the generated website is completely static I&rsquo;m hosting it on GitHub. No PHP or MySQL databases are needed to run my blog. Currently, the development of the Octopress engine is stalled for more than a year, however, I still like to ride this horse even when it might be dead.</p>

<p>Before I get going I typically need to read at least one book on the subject. Regarding technical blogging, I would recommend the <a href="http://technicalblogging.com/book/">Technical Blogging</a> book from The Pragmatic Programmers. I read the first several chapters before creating this blog and it was a great eye opener for me.</p>

<h2>Why is technical blogging fun?</h2>

<p>Technical blogging takes some effort, however, I&rsquo;m really enjoying it. Here are my top five reasons why I like technical blogging:</p>

<ol>
<li><em>Blog gives others insight into what you&rsquo;re working on</em>. People with similar interests will find you if you show what you&rsquo;re passionate about.</li>
<li><em>Blog as a contribution to open source.</em> Creating articles and tutorials about open source software is a form of contribution to the software we all love.</li>
<li><em>Blogging helps you to more deeply understand the subject you are writing about.</em> The best way to learn something is to teach it. In addition to teaching, writing about something is also a good way to expand your learning.</li>
<li><em>Blog is a documentation you can refer to in the future.</em> It happened to me several times that I had to come back to my own article to refresh my memory on the subject.</li>
<li><em>Blogging advances your writing skills.</em> Concise and easy to understand written communication is certainly appreciated by other developers on your team.</li>
</ol>


<h2>Let me know how I&rsquo;m doing</h2>

<p>Feedback is always greatly appreciated. If you find some time, I would be happy to hear how I&rsquo;m doing as a blogger. You can leave your comments in the comment section below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Passed the OpenShift EX280 certification!]]></title>
    <link href="http://alesnosek.com/blog/2017/04/04/passed-the-openshift-ex280-certification/"/>
    <updated>2017-04-04T23:00:22-07:00</updated>
    <id>http://alesnosek.com/blog/2017/04/04/passed-the-openshift-ex280-certification</id>
    <content type="html"><![CDATA[<p>It&rsquo;s been more than half a year since I started working with OpenShift. Today I successfully passed the <a href="https://www.redhat.com/en/services/training/ex280-red-hat-certificate-expertise-platform-service-exam">EX280 Red Hat Certificate of Expertise in Platform-as-a-Service exam</a> and earned a certificate. I&rsquo;m going to share a few details about the exam in this blog post.</p>

<!-- more -->


<p>Similar to the RHCSA/RHCE exams that I <a href="http://alesnosek.com/blog/2016/11/07/rhcsa-slash-rhce-exam-experience/">completed</a> some time ago, the EX280 OpenShift exam is also purely practical. You will have to install OpenShift 3.0, configure it and deploy multiple containerized applications on it.</p>

<p>For preparation I used the <a href="https://www.redhat.com/en/services/training/do280-openshift-enterprise-administration">DO280 OpenShift Enterprise Administration</a> materials that were included in my <a href="https://www.redhat.com/en/services/training/learning-subscription">Red Hat Learning Subscription</a>. I practiced the provided lab exercices over and over again until I gained a good confidence.</p>

<p><img class="right" src="http://alesnosek.com/images/posts/openshift_container_platform.png" width="250" height="300"></p>

<p>The exam took three hours and I have to say that I was very busy typing the whole time. Despite of my best effort I ran out of time with three tasks left untouched. How happy I was when I received my exam results. Passing score for the exam was 210 points. I made it through with 225 points.</p>

<p>I truly enjoy the Red Hat certification program and want to keep growing my collection of certificates. The list of my current certifications can be found on the <a href="https://www.redhat.com/rhtapps/certification/verify/?certId=160-216-727">Verify a Red Hat Certified Professional</a> website.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[An Introduction to Building on OpenShift]]></title>
    <link href="http://alesnosek.com/blog/2017/03/19/an-introduction-to-building-on-openshift/"/>
    <updated>2017-03-19T10:48:32-07:00</updated>
    <id>http://alesnosek.com/blog/2017/03/19/an-introduction-to-building-on-openshift</id>
    <content type="html"><![CDATA[<p>For years a Jenkins server has been driving the software builds in our company. Some time ago, we deployed an OpenShift cluster. The primary purpose of our OpenShift cluster was to support the efforts of dockerizing our software products. However, as OpeShift is a complete PaaS solution we started thinking about leveraging OpenShift for software builds, too. In this blog post I&rsquo;d like to share what we learned about building on OpenShift so far.</p>

<!-- more -->


<p>Before we begin talking about OpenShift, let&rsquo;s briefly discuss our current build environment. In the center of our build environment there is a Jenkins server. In Jenkins, we maintain numerous jobs to build our software, run automated tests, drive various devops tasks and much more. Jenkins is our central place from where the automated processes are started and monitored. As Jenkins is greatly extensible via plugins, we were able to easily integrate Jenkins with other tools, too.</p>

<h2>Building the OpenShift way</h2>

<p><img class="right" src="http://alesnosek.com/images/posts/openshift_logo.gif" width="200" height="200"></p>

<p>In OpenShift, one has to create a <em>BuildConfig</em> resource to describe the <a href="https://docs.openshift.org/latest/dev_guide/builds/index.html">build process</a>. The BuildConfig resource in OpenShift is roughly equivalent to a job definition in Jenkins. When creating a BuildConfig resource, a build strategy has to be chosen. The build strategy resembles a job type in Jenkins. Currently, there are four build strategies available in OpenShift:</p>

<p><strong>Source-to-Image strategy</strong>. Allows you to create a container image starting from the application source code. During the build process, the source code is downloaded into a container and compiled there. The finished binary artifacts are installed into the container. The complete container image is then pushed into the Docker registry from where it can be deployed as an application on OpenShift.</p>

<p><strong>Docker strategy</strong>. The input of the build process is a Dockerfile. OpenShift will execute a Docker build using the provided Dockerfile and upload the resulting image into the Docker registry from where it can be deployed.</p>

<p><strong>Custom strategy</strong>. Custom strategy could be compared to a free style job in Jenkins. The outcome of the build doesn&rsquo;t have to be a Docker image. Instead, the custom strategy allows you to create JARs, tarballs, RPMs or other artifacts which you have to upload to the repository of your choice by the end of the build.</p>

<p><strong>Pipeline strategy</strong>. In OpenShift 3.3, a new build strategy was introduced called <em>Pipeline</em>. This strategy doesn&rsquo;t really build anything but enables you to implement workflows on OpenShift. The great article <a href="https://blog.openshift.com/openshift-3-3-pipelines-deep-dive/">OpenShift 3.3 Pipelines - Deep Dive</a> describes how the Pipeline strategy works. In summary, you can create a BuildConfig in OpenShift that contains a definition of a Jenkins pipeline (using the Groovy DSL language). Based on this definition, OpenShift will create a pipeline job in Jenkins and execute it. Among other things, the Jenkins job can trigger a build on OpenShift, verify that the build succeeded and trigger a deployment. This approach allows OpenShift to leverage Jenkins pipelines to orchestrate a more involved CI/CD workflow possibly encompassing a conditional execution of multiple OpenShift builds and deployments.</p>

<p>An alternative to using the Pipeline strategy in OpenShift would be defining the pipeline job directly in Jenkins. With the <a href="https://plugins.jenkins.io/openshift-pipeline">OpenShift pipeline plugin</a> installed, one can trigger OpenShift operations from within the pipeline job.</p>

<p>As I didn&rsquo;t really work with the OpenShift strategies much I&rsquo;m not going to elaborate any further. Instead, in the next section, I&rsquo;m going to mention two Jenkins plugins that we are successfully using to run builds on OpenShift.</p>

<h2>Builds on Openshift driven by Jenkins</h2>

<p><img class="right" src="http://alesnosek.com/images/posts/jenkins_logo.png" width="200" height="200"></p>

<p>There are two Jenkins plugins that can leverage OpenShift containers as build slaves:</p>

<p><strong><a href="https://wiki.jenkins-ci.org/display/JENKINS/Swarm+Plugin">Swarm plugin</a></strong>. The Swarm plugin consists of two parts: a Jenkins plugin and a CLI client. Jenkins plugin exposes an endpoint where the CLI clients can register themselves. A CLI client acts as a Jenkins slave. It runs indefinitely within a Docker container and provides Jenkins with a configurable number of build executors. While the plugin is called a Swarm plugin it doesn&rsquo;t really need any Swarm orchestration. It can happily run in a Docker container on OpenShift.</p>

<p><strong><a href="https://wiki.jenkins-ci.org/display/JENKINS/Kubernetes+Plugin">Kubernetes plugin</a></strong>. Works perfectly with OpenShift. In contrast to the Swarm plugin, Kubernetes plugin spins up a new Docker slave for each job on the fly and destroys it as soon as the job has finished running.</p>

<p>Because the Jenkins workspace is created inside of the container, it will be deleted as soon as the Docker container is terminated. If you&rsquo;d like to reuse the same workspace for subsequent builds, I&rsquo;d like to offer you two options how to create persistent workspaces:</p>

<ol>
<li><p>You can attach a volume of type <em>hostPath</em> to your slave pods and place your workspace on that volume. At the same time you have to speficy a <em>nodeSelector</em> on your slave pods that would instruct OpenShift to schedule all your slave pods onto the same OpenShift node. With this approach the Jenkins slave can access its workspace on the local storage. Unfortunately, all the slaves that need to share a workspace have to run on the same OpenShift node which can get overloaded.</p></li>
<li><p>You can attach a volume with the <em>ReadWriteMany</em> capability to your slave pods and place your workspace on this volume. Eligible volume types are NFS, GlusterFS or CephFS. Using this method a Jenkins slave running on any node in the cluster can access the shared workspace. The downside is that the access is over the network and hence slower than an access to the local storage.</p></li>
</ol>


<h2>Conclusion</h2>

<p>In the this blog post we reviewed different approaches how to leverage an OpenShift cluster for software builds. On one hand, builds can be defined within OpenShift by creating the BuildConfig resources. This approach might be less flexible than using a full-fledged build server like Jenkins, however, one can be sure that the builds will work on any OpenShift cluster including the public cloud. On the other hand, we have seen that in an environment where Jenkins is already the king, we can leverage the Swarm or Kubernetes plugin to allow Jenkins to schedule build jobs on OpenShift.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Accessing Kubernetes Pods from Outside of the Cluster]]></title>
    <link href="http://alesnosek.com/blog/2017/02/14/accessing-kubernetes-pods-from-outside-of-the-cluster/"/>
    <updated>2017-02-14T23:36:37-08:00</updated>
    <id>http://alesnosek.com/blog/2017/02/14/accessing-kubernetes-pods-from-outside-of-the-cluster</id>
    <content type="html"><![CDATA[<p>There are several ways how to expose your application running on the Kubernetes cluster to the outside world. When reading the <a href="https://kubernetes.io/docs/">Kubernetes documentation</a> I had a hard time ordering the different approaches in my head. I created this blog post for my future reference but will be happy if it can be of any use to you. Without further ado let&rsquo;s discuss the <em>hostNetwork</em>, <em>hostPort</em>, <em>NodePort</em>, <em>LoadBalancer</em> and <em>Ingress</em> features of Kubernetes.</p>

<!-- more -->


<h2>hostNetwork: true</h2>

<p>The <code>hostNetwork</code> setting applies to the Kubernetes pods. When a pod is configured with <code>hostNetwork: true</code>, the applications running in such a pod can directly see the network interfaces of the host machine where the pod was started. An application that is configured to listen on all network interfaces will in turn be accessible on all network interfaces of the host machine. Here is an example definition of a pod that uses host networking:</p>

<figure class='code'><figcaption><span>influxdb-hostnetwork.yml </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">apiVersion</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">v1</span>
</span><span class='line'><span class="l-Scalar-Plain">kind</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Pod</span>
</span><span class='line'><span class="l-Scalar-Plain">metadata</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'><span class="l-Scalar-Plain">spec</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">hostNetwork</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">true</span>
</span><span class='line'>  <span class="l-Scalar-Plain">containers</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'>      <span class="l-Scalar-Plain">image</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span></code></pre></td></tr></table></div></figure>


<p>You can start the pod with the following command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>kubectl create -f influxdb-hostnetwork.yml
</span></code></pre></td></tr></table></div></figure>


<p>You can check that the InfluxDB application is running with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl -v http://kubenode01.example.com:8086/ping
</span></code></pre></td></tr></table></div></figure>


<p>Remember to replace the host name in the above URL with the host name or IP address of the Kubernetes node where your pod has been scheduled to run. InfluxDB will respond with HTTP 204 No Content when working properly.</p>

<p>Note that every time the pod is restarted Kubernetes can reschedule the pod onto a different node and so the application will change its IP address. Besides that two applications requiring the same port cannot run on the same node. This can lead to port conflicts when the number of applications running on the cluster grows. On top of that, creating a pod with <code>hostNetwork: true</code> on OpenShift is a privileged operation. For these reasons, the host networking is not a good way to make your applications accessible from outside of the cluster.</p>

<p>What is the host networking good for? For cases where a direct access to the host networking is required. For example, the Kubernetes networking plugin Flannel can be deployed as a daemon set on all nodes of the Kubernetes cluster. Due to <code>hostNetwork: true</code> the Flannel has full control of the networking on every node in the cluster allowing it to manage the overlay network to which the pods with <code>hostNetwork: false</code> are connected to.</p>

<h2>hostPort</h2>

<p>The <code>hostPort</code> setting applies to the Kubernetes containers. The container port will be exposed to the external network at <em>&lt;hostIP>:&lt;hostPort></em>, where the <em>hostIP</em> is the IP address of the Kubernetes node where the container is running and the <em>hostPort</em> is the port requested by the user. Here comes a sample pod definition:</p>

<figure class='code'><figcaption><span>influxdb-hostport.yml </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">apiVersion</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">v1</span>
</span><span class='line'><span class="l-Scalar-Plain">kind</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Pod</span>
</span><span class='line'><span class="l-Scalar-Plain">metadata</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'><span class="l-Scalar-Plain">spec</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">containers</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'>      <span class="l-Scalar-Plain">image</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'>      <span class="l-Scalar-Plain">ports</span><span class="p-Indicator">:</span>
</span><span class='line'>        <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">containerPort</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">8086</span>
</span><span class='line'>          <span class="l-Scalar-Plain">hostPort</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">8086</span>
</span></code></pre></td></tr></table></div></figure>


<p>The hostPort feature allows to expose a single container port on the host IP. Using the hostPort to expose an application to the outside of the Kubernetes cluster has the same drawbacks as the hostNetwork approach discussed in the previous section. The host IP can change when the container is restarted, two containers using the same hostPort cannot be scheduled on the same node and the usage of the hostPort is considered a privileged operation on OpenShift.</p>

<p>What is the hostPort used for? For example, the nginx based <a href="https://github.com/kubernetes/ingress/tree/master/controllers/nginx">Ingress controller</a> is deployed as a set of containers running on top of Kubernetes. These containers are configured to use hostPorts 80 and 443 to allow the inbound traffic on these ports from the outside of the Kubernetes cluster.</p>

<h2>NodePort</h2>

<p>The <code>NodePort</code> setting applies to the Kubernetes services. By default Kubernetes services are accessible at the ClusterIP which is an internal IP address reachable from inside of the Kubernetes cluster only. The ClusterIP enables the applications running within the pods to access the service. To make the service accessible from outside of the cluster a user can create a service of type NodePort. At first, let&rsquo;s review the definition of the pod that we&rsquo;ll expose using a NodePort service:</p>

<figure class='code'><figcaption><span>influxdb-pod.yml </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">apiVersion</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">v1</span>
</span><span class='line'><span class="l-Scalar-Plain">kind</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Pod</span>
</span><span class='line'><span class="l-Scalar-Plain">metadata</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'>  <span class="l-Scalar-Plain">labels</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'><span class="l-Scalar-Plain">spec</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">containers</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'>      <span class="l-Scalar-Plain">image</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'>      <span class="l-Scalar-Plain">ports</span><span class="p-Indicator">:</span>
</span><span class='line'>        <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">containerPort</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">8086</span>
</span></code></pre></td></tr></table></div></figure>


<p>When creating a NodePort service, the user can specify a port from the range 30000-32767, and each Kubernetes node will proxy that port to the pods selected by the service. A sample definition of a NodePort service looks as follows:</p>

<figure class='code'><figcaption><span>influxdb-nodeport.yml </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">kind</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Service</span>
</span><span class='line'><span class="l-Scalar-Plain">apiVersion</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">v1</span>
</span><span class='line'><span class="l-Scalar-Plain">metadata</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'><span class="l-Scalar-Plain">spec</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">type</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">NodePort</span>
</span><span class='line'>  <span class="l-Scalar-Plain">ports</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">port</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">8086</span>
</span><span class='line'>      <span class="l-Scalar-Plain">nodePort</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">30000</span>
</span><span class='line'>  <span class="l-Scalar-Plain">selector</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span></code></pre></td></tr></table></div></figure>


<p>Note that on OpenShift more privileges are required to create a NodePort service. After the service has been created, the kube-proxy component that runs on each node of the Kubernetes cluster and listens on all network interfaces is instructed to accept connections on port 30000. The incoming traffic is forwarded by the kube-proxy to the selected pods in a round-robin fashion. You should be able to access the InfluxDB application from outside of the cluster using the command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl -v http://kubenode01.example.com:30000/ping
</span></code></pre></td></tr></table></div></figure>


<p>The NodePort service represents a static endpoint through which the selected pods can be reached. If you prefer serving your application on a different port than the 30000-32767 range, you can deploy an external load balancer in front of the Kubernetes nodes and forward the traffic to the NodePort on each of the Kubernetes nodes. This gives you an extra resiliency for the case that some of the Kubernetes nodes becomes unavailable, too. If you&rsquo;re hosting your Kubernetes cluster on one of the supported cloud providers like AWS, Azure or GCE, Kubernetes can provision an external load balancer for you. We&rsquo;ll take a look at how to do it in the next section.</p>

<h2>LoadBalancer</h2>

<p>The <code>LoadBalancer</code> setting applies to the Kubernetes service. In order to be able to create a service of type LoadBalancer, a cloud provider has to be enabled in the configuration of the Kubernetes cluster. As of version 1.6, Kubernetes can provision load balancers on AWS, Azure, CloudStack, GCE and OpenStack. Here is an example definition of the LoadBalancer service:</p>

<figure class='code'><figcaption><span>influxdb-loadbalancer.yml </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">kind</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Service</span>
</span><span class='line'><span class="l-Scalar-Plain">apiVersion</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">v1</span>
</span><span class='line'><span class="l-Scalar-Plain">metadata</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'><span class="l-Scalar-Plain">spec</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">type</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">LoadBalancer</span>
</span><span class='line'>  <span class="l-Scalar-Plain">ports</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">port</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">8086</span>
</span><span class='line'>  <span class="l-Scalar-Plain">selector</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span></code></pre></td></tr></table></div></figure>


<p>Let&rsquo;s take a look at what Kubernetes created for us:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>kubectl get svc influxdb
</span><span class='line'>NAME       CLUSTER-IP     EXTERNAL-IP     PORT<span class="o">(</span>S<span class="o">)</span>          AGE
</span><span class='line'>influxdb   10.97.121.42   10.13.242.236   8086:30051/TCP   39s
</span></code></pre></td></tr></table></div></figure>


<p>In the command output we can read that the influxdb service is internally reachable at the ClusterIP 10.97.121.42. Next, Kubernetes allocated a NodePort 30051. Because we didn&rsquo;t specify a desired NodePort number, Kubernetes picked one for us. We can check the reachability of the InfluxDB application through the NodePort with the command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl -v http://kubenode01.example.com:30051/ping
</span></code></pre></td></tr></table></div></figure>


<p>Finally, Kubernetes reached out to the cloud provider to provision a load balancer. The VIP of the load balancer is 10.13.242.236 as it is shown in the command output. Now we can access the InfluxDB application through the load balancer like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl -v http://10.13.242.236:8086/ping
</span></code></pre></td></tr></table></div></figure>


<p>My cloud provider is OpenStack. Let&rsquo;s examine how the provisioned load balancer on OpenStack looks like:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>neutron lb-vip-show 9bf2a580-2ba4-4494-93fd-9b6969c55ac3
</span><span class='line'>+---------------------+--------------------------------------------------------------+
</span><span class='line'><span class="p">|</span> Field               <span class="p">|</span> Value                                                        <span class="p">|</span>
</span><span class='line'>+---------------------+--------------------------------------------------------------+
</span><span class='line'><span class="p">|</span> address             <span class="p">|</span> 10.13.242.236                                                <span class="p">|</span>
</span><span class='line'><span class="p">|</span> admin_state_up      <span class="p">|</span> True                                                         <span class="p">|</span>
</span><span class='line'><span class="p">|</span> connection_limit    <span class="p">|</span> -1                                                           <span class="p">|</span>
</span><span class='line'><span class="p">|</span> description         <span class="p">|</span> Kubernetes external service a6ffa4dadf99711e68ea2fa163e0b082 <span class="p">|</span>
</span><span class='line'><span class="p">|</span> id                  <span class="p">|</span> 9bf2a580-2ba4-4494-93fd-9b6969c55ac3                         <span class="p">|</span>
</span><span class='line'><span class="p">|</span> name                <span class="p">|</span> a6ffa4dadf99711e68ea2fa163e0b082                             <span class="p">|</span>
</span><span class='line'><span class="p">|</span> pool_id             <span class="p">|</span> 392917a6-ed61-4924-acb2-026cd4181755                         <span class="p">|</span>
</span><span class='line'><span class="p">|</span> port_id             <span class="p">|</span> e450b80b-6da1-4b31-a008-280abdc6400b                         <span class="p">|</span>
</span><span class='line'><span class="p">|</span> protocol            <span class="p">|</span> TCP                                                          <span class="p">|</span>
</span><span class='line'><span class="p">|</span> protocol_port       <span class="p">|</span> <span class="m">8086</span>                                                         <span class="p">|</span>
</span><span class='line'><span class="p">|</span> session_persistence <span class="p">|</span>                                                              <span class="p">|</span>
</span><span class='line'><span class="p">|</span> status              <span class="p">|</span> ACTIVE                                                       <span class="p">|</span>
</span><span class='line'><span class="p">|</span> status_description  <span class="p">|</span>                                                              <span class="p">|</span>
</span><span class='line'><span class="p">|</span> subnet_id           <span class="p">|</span> 73f8eb91-90cf-42f4-85d0-dcff44077313                         <span class="p">|</span>
</span><span class='line'><span class="p">|</span> tenant_id           <span class="p">|</span> 4d68886fea6e45b0bc2e05cd302cccb9                             <span class="p">|</span>
</span><span class='line'>+---------------------+--------------------------------------------------------------+
</span><span class='line'>
</span><span class='line'><span class="nv">$ </span>neutron lb-pool-show 392917a6-ed61-4924-acb2-026cd4181755
</span><span class='line'>+------------------------+--------------------------------------+
</span><span class='line'><span class="p">|</span> Field                  <span class="p">|</span> Value                                <span class="p">|</span>
</span><span class='line'>+------------------------+--------------------------------------+
</span><span class='line'><span class="p">|</span> admin_state_up         <span class="p">|</span> True                                 <span class="p">|</span>
</span><span class='line'><span class="p">|</span> description            <span class="p">|</span>                                      <span class="p">|</span>
</span><span class='line'><span class="p">|</span> health_monitors        <span class="p">|</span>                                      <span class="p">|</span>
</span><span class='line'><span class="p">|</span> health_monitors_status <span class="p">|</span>                                      <span class="p">|</span>
</span><span class='line'><span class="p">|</span> id                     <span class="p">|</span> 392917a6-ed61-4924-acb2-026cd4181755 <span class="p">|</span>
</span><span class='line'><span class="p">|</span> lb_method              <span class="p">|</span> ROUND_ROBIN                          <span class="p">|</span>
</span><span class='line'><span class="p">|</span> members                <span class="p">|</span> d0825cc2-46a3-43bd-af82-e9d8f1f85299 <span class="p">|</span>
</span><span class='line'><span class="p">|</span>                        <span class="p">|</span> 3f73d3bb-bc40-478d-8d0e-df05cdfb9734 <span class="p">|</span>
</span><span class='line'><span class="p">|</span> name                   <span class="p">|</span> a6ffa4dadf99711e68ea2fa163e0b082     <span class="p">|</span>
</span><span class='line'><span class="p">|</span> protocol               <span class="p">|</span> TCP                                  <span class="p">|</span>
</span><span class='line'><span class="p">|</span> provider               <span class="p">|</span> haproxy                              <span class="p">|</span>
</span><span class='line'><span class="p">|</span> status                 <span class="p">|</span> ACTIVE                               <span class="p">|</span>
</span><span class='line'><span class="p">|</span> status_description     <span class="p">|</span>                                      <span class="p">|</span>
</span><span class='line'><span class="p">|</span> subnet_id              <span class="p">|</span> 73f8eb91-90cf-42f4-85d0-dcff44077313 <span class="p">|</span>
</span><span class='line'><span class="p">|</span> tenant_id              <span class="p">|</span> 4d68886fea6e45b0bc2e05cd302cccb9     <span class="p">|</span>
</span><span class='line'><span class="p">|</span> vip_id                 <span class="p">|</span> 9bf2a580-2ba4-4494-93fd-9b6969c55ac3 <span class="p">|</span>
</span><span class='line'>+------------------------+--------------------------------------+
</span><span class='line'>
</span><span class='line'><span class="nv">$ </span>neutron lb-member-list
</span><span class='line'>+--------------------------------------+--------------+---------------+--------+----------------+--------+
</span><span class='line'><span class="p">|</span> id                                   <span class="p">|</span> address      <span class="p">|</span> protocol_port <span class="p">|</span> weight <span class="p">|</span> admin_state_up <span class="p">|</span> status <span class="p">|</span>
</span><span class='line'>+--------------------------------------+--------------+---------------+--------+----------------+--------+
</span><span class='line'><span class="p">|</span> 3f73d3bb-bc40-478d-8d0e-df05cdfb9734 <span class="p">|</span> 10.13.241.89 <span class="p">|</span>         <span class="m">30051</span> <span class="p">|</span>      <span class="m">1</span> <span class="p">|</span> True           <span class="p">|</span> ACTIVE <span class="p">|</span>
</span><span class='line'><span class="p">|</span> d0825cc2-46a3-43bd-af82-e9d8f1f85299 <span class="p">|</span> 10.13.241.10 <span class="p">|</span>         <span class="m">30051</span> <span class="p">|</span>      <span class="m">1</span> <span class="p">|</span> True           <span class="p">|</span> ACTIVE <span class="p">|</span>
</span><span class='line'>+--------------------------------------+--------------+---------------+--------+----------------+--------+
</span></code></pre></td></tr></table></div></figure>


<p>Kubernetes created a TCP load balancer with the VIP 10.13.242.236 and port 8086. There are two pool members associated with the load balancer: 10.13.241.89 and 10.13.241.10. These are the IP addresses of the nodes in my two-node Kubernetes cluster. The traffic is forwarded to the NodePort 30051 of these two nodes.</p>

<p>The load balancer created by Kubernetes is a plain TCP round-robin load balancer. It doesn&rsquo;t offer SSL termination or HTTP routing. Besides that, Kubernetes will create a separate load balancer for each service. This can become quite costly when the number of your services increases. Instead of letting Kubernetes manage the load balancer, you can go back to deploying NodePort services and provision and configure an external load balancer yourself. Another option is leveraging the Kubernetes Ingress resource that we will discuss in the next section.</p>

<h2>Ingress</h2>

<p>The <code>Ingress</code> resource type was introduced in Kubernetes version 1.1. The Kubernetes cluster must have an <a href="https://github.com/kubernetes/ingress/tree/master/controllers/nginx">Ingress controller</a> deployed in order for you to be able to create Ingress resources. What is the Ingress controller? The Ingress controller is deployed as a Docker container on top of Kubernetes. Its Docker image contains a load balancer like nginx or HAProxy and a controller daemon. The controller daemon receives the desired Ingress configuration from Kubernetes. It generates an nginx or HAProxy configuration file and restarts the load balancer process for changes to take effect. In other words, Ingress controller is a load balancer managed by Kubernetes.</p>

<p>The Kubernetes Ingress provides features typical for a load balancer: HTTP routing, sticky sessions, SSL termination, SSL passthrough, TCP and UDP load balancing &hellip; At the moment not every Ingress controller implements all the available features. You have to consult the documentation of your Ingress controller to learn about its capabilities.</p>

<p>Let&rsquo;s expose our InfluxDB application to the outside world via Ingress. An example Ingress definition looks like this:</p>

<figure class='code'><figcaption><span>influxdb-ingress.yml </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">apiVersion</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">extensions/v1beta1</span>
</span><span class='line'><span class="l-Scalar-Plain">kind</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Ingress</span>
</span><span class='line'><span class="l-Scalar-Plain">metadata</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'><span class="l-Scalar-Plain">spec</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">rules</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">host</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb.kube.example.com</span>
</span><span class='line'>      <span class="l-Scalar-Plain">http</span><span class="p-Indicator">:</span>
</span><span class='line'>        <span class="l-Scalar-Plain">paths</span><span class="p-Indicator">:</span>
</span><span class='line'>          <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">backend</span><span class="p-Indicator">:</span>
</span><span class='line'>              <span class="l-Scalar-Plain">serviceName</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'>              <span class="l-Scalar-Plain">servicePort</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">8086</span>
</span></code></pre></td></tr></table></div></figure>


<p>Our DNS is setup to resolve *.kube.example.com to the IP address 10.13.241.10. This is the IP address of the Kubernetes node where the Ingress controller is running. As we already mentioned when discussing the hostPort, the Ingress listens for the incoming connections on two hostPorts 80 and 443 for the HTTP and HTTPS requests, respectively. Let&rsquo;s check that we can reach the InfluxDB application via Ingress:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl -v http://influxdb.kube.example.com/ping
</span></code></pre></td></tr></table></div></figure>


<p>When everything is setup correctly, the InfluxDB will respond with HTTP 204 No Content.</p>

<p>There&rsquo;s a difference between the LoadBalancer service and the Ingress in how the traffic routing is realized. In the case of the LoadBalancer service, the traffic that enters through the external load balancer is forwarded to the kube-proxy that in turn forwards the traffic to the selected pods. In contrast, the Ingress load balancer forwards the traffic straight to the selected pods which is more efficient.</p>

<h2>Conclusion</h2>

<p>Overall, when exposing pods to the outside of the Kubernetes cluster, the Ingress seems to be a very flexible and convenient solution. Unfortunately, it&rsquo;s also the less mature among the discussed approaches. When choosing the NodePort service, you might want to deploy a load balancer in front of your cluster as well. If you are hosting Kubernetes on one of the supported clouds, the LoadBalancer service is another option for you.</p>

<p>How do you route the external traffic to the Kubernetes pods? Glad to hear about your experience in the Comments section below!</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[TripleO Installer, Production Ready?]]></title>
    <link href="http://alesnosek.com/blog/2017/01/15/tripleo-installer-production-ready/"/>
    <updated>2017-01-15T23:13:32-08:00</updated>
    <id>http://alesnosek.com/blog/2017/01/15/tripleo-installer-production-ready</id>
    <content type="html"><![CDATA[<p><a href="https://wiki.openstack.org/wiki/TripleO">TripleO</a> is an OpenStack deployment and management tool we&rsquo;ve been using on the production systems for a while now. As TripleO is an upstream project for the Red Hat OpenStack Platform Director one would expect a decently working tool able to manage large-scale OpenStack deployments. What is our experience with TripleO?</p>

<!-- more -->


<h2>Introduction</h2>

<p>Six months have passed since we deployed a private cloud in our company. Our cloud is based on the RDO distribution of OpenStack Mitaka running on top of RHEL 7. I have to say that we&rsquo;re very happy with our cloud-based environment. OpenStack simplified the management of virtual machines and boosted the productivity of our engineering team which enjoys the self-service provided by the OpenStack APIs. Our test automation creates and destroys many virtual machines a day making sure that our software product is tested in a clean and well-defined environment. OpenStack quickly became a critical part of our infrastructure.</p>

<p>Hence we were less pleased when the last week a routine maintenance of the OpenStack cluster turned into an unplanned downtime of two compute nodes. But before we get to the problem itself let me introduce you to the specifics of how we manage the OpenStack cluster.</p>

<h2>Overcloud maintenance is a challenge</h2>

<p>A cloud life-cycle management tool of choice in the RDO distribution is TripleO. I published an article about my initial experience with TripleO a while ago: <a href="http://alesnosek.com/blog/2016/03/27/tripleo-installer-the-good/">TripleO Installer - the Good, the Bad and the Ugly</a>. Overall, the way how TripleO configures the OpenStack cluster is rather less flexible. After spending time on customizing and patching TripleO we decided that there must be an easier way. Eventually, we implemented our own set of Ansible scripts that allow an additional fine-grained configuration of OpenStack nodes. After the <code>openstack overcloud deploy</code> command is complete we run our Ansible scripts to apply an additional configuration to the overcloud. There are two benefits to this approach. First, we don&rsquo;t have to patch TripleO scripts which will be upgraded in the next release of OpenStack. And second, we can keep using Ansible which is our favorite configuration tool.</p>

<p>Having updated the overcloud using TripleO several times we realized that the update procedure is rather unreliable. Some time the TripleO update would fail with an error. Other time the overcloud update would just hang forever. Probably due to the undeterministic behaviour of the Puppet scripts that constitute a substantial part of TripleO we experienced random errors that would not occur again after restarting the update procedure. Situation got worse after we configured the overcloud Keystone to authenticate OpenStack users against Active Directory. The overcloud update would not run into completion anymore due to a defect in the Puppet scripts.</p>

<p>Because fixing the TripleO scripts would require additional effort and the overcloud update would remain a risky operation either way we concluded that we will require a downtime when updating the overcloud. During the downtime period the existing virtual machines are fully operational only the OpenStack services that allow users to create or delete virtual machines or other cloud resources are not available. In the case of our private cloud this was an acceptable albeit not ideal solution.</p>

<p>In summary, we can depict our OpenStack maintenance process like this:</p>

<p><img src="http://alesnosek.com/images/posts/openstack_maintenance_process.svg" width="500" height="700" title="OpenStack Maintenance Process" ></p>

<h2>TripleO installer and the resulting downtime</h2>

<p>On all our OpenStack nodes we use bonded network interfaces to protect the nodes against network failures. In network interface bonding a pair of physical network interfaces is combined into a single logical interface. This provides redundancy by allowing failover from one physical interface to another in the case of failure.</p>

<p>It happened to us that on two of our compute nodes one physical network interface per bond was not working. In this situation the network connection is still functional but not redundant anymore. Unfortunately, for the TripleO installer this was not good enough. Normally, during the overcloud update the <code>os-net-config</code> utility configures the node networking. Due to the single network interface down <code>os-net-config</code> failed to create a correct network configuration. A &ldquo;safe&rdquo; default configuration was generated instead which configured all available network interfaces to use DHCP. Unfortunately, we prefer a static network configuration of overcloud nodes and so no DHCP server was available. Hence this &ldquo;safe&rdquo; default configuration rendered the two compute nodes unreachable including all the virtual machines that were running on top of them!</p>

<p>We were able to fix the networking issue on the first compute node quickly. However, the physical network interfaces on the second compute node were seriously falling apart. Unfortunately:</p>

<blockquote><p>The TripleO installer requires that all the overcloud nodes are reachable during the overcloud update.</p></blockquote>


<p>In the opposite case the update just stays hanging. It turned out that it was not possible to bring all the network interfaces on the second compute node up but eventually we were able to get at least the management interface working. This allowed us to re-run the overcloud update during which we fooled the TripleO installer to believe that the configuration of the problematic compute node was applied sucessfully. After exceeding the two-hour maintanance window by several hours we were finally done.</p>

<h2>Conclusion</h2>

<p>Here I&rsquo;d like to summarize our six-months long experience with the TripleO installer:</p>

<ol>
<li>In our experience, the configuration of the OpenStack cluster using only the TripleO installer is not flexible enough. As a workaround, we ended up writing a bunch of Ansible scripts.</li>
<li>The overcloud update can take a very long time to complete and it can fail because of random errors. Also during the update operation all the overcloud nodes have to be reachable by the TripleO installer. For this reason, I personally cannot imagine using TripleO to manage a cluster with more than one hundred nodes.</li>
<li>As we experienced, the TripleO installer can easily break a working OpenStack cluster. This is a big no-no for a production system.</li>
</ol>


<p>Overall, I think that the TripleO installer in the Mitaka version of OpenStack would need more work to become production ready. In the meantime, we&rsquo;re continuing with patching of what we have.</p>

<p>In the future, there are other projects that could replace the TripleO installer. I found the <a href="https://github.com/openstack/kolla-ansible">kolla-ansible</a> and <a href="https://github.com/openstack/kolla-kubernetes">kolla-kubernetes</a> rather promising.</p>
]]></content>
  </entry>

</feed>
