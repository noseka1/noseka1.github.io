<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Ales Nosek - The Software Practitioner]]></title>
  <link href="http://alesnosek.com/atom.xml" rel="self"/>
  <link href="http://alesnosek.com/"/>
  <updated>2017-07-30T13:14:56-07:00</updated>
  <id>http://alesnosek.com/</id>
  <author>
    <name><![CDATA[Ales Nosek]]></name>

  </author>
  <generator uri="http://octopress.org/">Octopress</generator>


  <entry>
    <title type="html"><![CDATA[Passed Red Hat Container Certifications EX270 and EX276]]></title>
    <link href="http://alesnosek.com/blog/2017/07/29/passed-red-hat-container-certifications-ex270-and-ex276/"/>
    <updated>2017-07-29T22:44:56-07:00</updated>
    <id>http://alesnosek.com/blog/2017/07/29/passed-red-hat-container-certifications-ex270-and-ex276</id>
    <content type="html"><![CDATA[<p>Recently I passed two Red Hat certification exams: <a href="https://www.redhat.com/en/services/training/ex270-red-hat-certificate-expertise-atomic-host-container-administration">EX270 Red Hat Certificate of Expertise in Atomic Host Container Administration</a> and the <a href="https://www.redhat.com/en/services/training/ex276-red-hat-certificate-expertise-containerized-application-development">EX276 Red Hat Certificate of Expertise in Containerized Application Development</a>. I&rsquo;d like to share some of my experience with you in this blog post.</p>

<!-- more -->


<p><img class="right" src="http://alesnosek.com/images/posts/redhat_logo.png" width="150" height="150"></p>

<p>As you might already know, all the Red hat certification exams are purely practical. You&rsquo;ll have to configure the provided virtual machines based on the list of instructions given to you in the exam.</p>

<p><img class="right" src="http://alesnosek.com/images/posts/docker_logo.png" width="100" height="100"></p>

<p>From my experience, I can say that the scope of exams EX270 and EX276 is highly overlapping. Both exams are focused on working with Docker, concretely: creating Docker images using Dockerfiles, pulling and pushing images into Docker registries, running Docker containers, linking Docker containers, creating and sharing volumes between containers, exposing network ports. This pretty much covers the content of the EX276 exam and the major part of the EX270 exam. In addition, the exam EX270 includes several tasks related to Red Hat Atomic Host.</p>

<p>To prepare for the exams, I used the online courses Managing Containers with Red Hat Enterprise Linux Atomic Host (RH270R) and Containerizing Software Applications (DO276R) that are included in my <a href="https://www.redhat.com/en/services/training/learning-subscription">Red Hat Learning Subscription</a>. In comparison to prep materials for other exams, these materials are not that large but they covered the exam requirements well.</p>

<p>And how did I score? I achieved 260 points on the EX270 exam and 291 points on the EX276 exam. For both exams, the minimum passing score was 210 points and the maximum score was 300. Subjectively, I found the difficulty of the EX270 to be on par with the other Red Hat exams I did in the past. On the other hand, the EX276 appeared to me as a rather easy exam. I was able to complete all my tasks, including double-checking, 45 minutes before the exam end.</p>

<p>Passing these two exams concluded my <a href="https://www.redhat.com/en/services/certification/rhca">RHCA</a> journey and earned me the Red Hat Certified Architect certificate. Here is the obligatory link to my current list of certifications on the <a href="https://www.redhat.com/rhtapps/certification/verify/?certId=160-216-727">Verify a Red Hat Certified Professional</a> website.</p>

<p>I believe that many folks are on their way to become RHCA. How are you doing on your journey? Would you like to share your experiences with the Red Hat certification program? Please, leave your comments below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Check-and-Set Operation and Transactions in Consul]]></title>
    <link href="http://alesnosek.com/blog/2017/07/25/check-and-set-operation-and-transactions-in-consul/"/>
    <updated>2017-07-25T23:07:07-07:00</updated>
    <id>http://alesnosek.com/blog/2017/07/25/check-and-set-operation-and-transactions-in-consul</id>
    <content type="html"><![CDATA[<p>In the <a href="http://alesnosek.com/blog/2017/07/15/first-look-at-the-key-value-store-in-consul/">previous</a> blog post, we were checking out the basic functionality of the key-value store in Consul. In this article, we will explore two of the more advanced features of Consul&rsquo;s key-value store, namely: Check-and-Set operation and transactions.</p>

<!-- more -->


<p><img class="right" src="http://alesnosek.com/images/posts/consul_logo.png" width="200" height="300"></p>

<p>For our experimenting, let&rsquo;s start a one-node Consul cluster. The meaning of the individual command-line parameters was described in the <a href="http://alesnosek.com/blog/2017/07/15/first-look-at-the-key-value-store-in-consul/">previous</a> article:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul agent -ui -server -data-dir mydata -advertise 127.0.0.1 -bootstrap-expect 1
</span></code></pre></td></tr></table></div></figure>


<p>In a short moment, the one-node Consul cluster should be up and ready. In the following, we&rsquo;re going to leverage Consul&rsquo;s <a href="https://www.consul.io/api/index.html">HTTP API</a> as not all the desired functionality is exposed via the command-line client. First, let&rsquo;s verify that the Consul cluster is working properly. For that, we&rsquo;ll ask it to provide us with a list of cluster nodes:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl http://localhost:8500/v1/catalog/nodes?pretty
</span><span class='line'><span class="o">[</span>
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>        <span class="s2">&quot;ID&quot;</span>: <span class="s2">&quot;be79786e-749d-758c-2b65-824c1e956788&quot;</span>,
</span><span class='line'>        <span class="s2">&quot;Node&quot;</span>: <span class="s2">&quot;zihadlo&quot;</span>,
</span><span class='line'>        <span class="s2">&quot;Address&quot;</span>: <span class="s2">&quot;127.0.0.1&quot;</span>,
</span><span class='line'>        <span class="s2">&quot;Datacenter&quot;</span>: <span class="s2">&quot;dc1&quot;</span>,
</span><span class='line'>        <span class="s2">&quot;TaggedAddresses&quot;</span>: <span class="o">{</span>
</span><span class='line'>            <span class="s2">&quot;lan&quot;</span>: <span class="s2">&quot;127.0.0.1&quot;</span>,
</span><span class='line'>            <span class="s2">&quot;wan&quot;</span>: <span class="s2">&quot;127.0.0.1&quot;</span>
</span><span class='line'>        <span class="o">}</span>,
</span><span class='line'>        <span class="s2">&quot;Meta&quot;</span>: <span class="o">{}</span>,
</span><span class='line'>        <span class="s2">&quot;CreateIndex&quot;</span>: 5,
</span><span class='line'>        <span class="s2">&quot;ModifyIndex&quot;</span>: 6
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>The response from Consul contains information about the single node which is what we expected.</p>

<h2>Check-and-Set operation</h2>

<p>The purpose of the Check-and-Set operation is to avoid lost updates when multiple clients are simultaneously trying to update a value of the same key. Check-and-Set operation allows the update to happen only if the value has not been changed since the client last read it. If the current value does not match what the client previously read, the client will receive a conflicting update error message and will have to retry the read-update cycle.</p>

<p>The Check-and-Set operation can be used to implement a shared counter, semaphore or a distributed lock. Let&rsquo;s demonstrate how to create a basic distributed lock using the Check-and-Set operation. We&rsquo;ll start with creating a key that will represent our lock:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl --request PUT http://localhost:8500/v1/kv/mylock --data <span class="s2">&quot;&quot;</span>
</span><span class='line'><span class="nb">true</span>
</span></code></pre></td></tr></table></div></figure>


<p>We created the <code>mylock</code> key holding an empty value. The empty value signalizes that the lock is not taken. Before trying to acquire the lock, each client has to check whether the lock is unlocked:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl http://localhost:8500/v1/kv/mylock?pretty
</span><span class='line'><span class="o">[</span>
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>        <span class="s2">&quot;LockIndex&quot;</span>: 0,
</span><span class='line'>        <span class="s2">&quot;Key&quot;</span>: <span class="s2">&quot;mylock&quot;</span>,
</span><span class='line'>        <span class="s2">&quot;Flags&quot;</span>: 0,
</span><span class='line'>        <span class="s2">&quot;Value&quot;</span>: null,
</span><span class='line'>        <span class="s2">&quot;CreateIndex&quot;</span>: 5638,
</span><span class='line'>        <span class="s2">&quot;ModifyIndex&quot;</span>: 5638
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>The value of the key in the Consul&rsquo;s response is still empty (null) which indicates that nobody is holding the lock. The second important item in the Consul&rsquo;s response is the <code>ModifyIndex</code>. Each key in the key-value store has its own <code>ModifyIndex</code>. The <code>ModifyIndex</code> is incremented by Consul each time the respective key is modified.</p>

<p>After verifying that the lock is not taken, the client can try to acquire it:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl --request PUT http://localhost:8500/v1/kv/mylock?cas<span class="o">=</span><span class="m">5638</span> --data <span class="s2">&quot;client1&quot;</span>
</span><span class='line'><span class="nb">true</span>
</span></code></pre></td></tr></table></div></figure>


<p>The client is trying to update the value of the key <code>mylock</code>. The value of the <code>ModifyIndex</code> is passed along as the query parameter <code>cas=5638</code> (cas meaning Check-and-Set). Because the query parameter <code>cas=5638</code> is specified in the request, Consul will update the value of the <code>mylock</code> key only if the current <code>ModifyIndex</code> of the <code>mylock</code> key matches 5638. In other words, the key has not been updated since the client last read it. In our example, the update was successful and the client is now holding the lock. Note that an arbitrary non-empty value can be stored under the <code>mylock</code> key. We chose to use the identification of the client that has acquired the lock.</p>

<p>Let&rsquo;s pretend that at the same time a second client was competing for the lock. The <code>client2</code> was trying to acquire the lock by sending this request to Consul including the same query parameter <code>cas=5638</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl --request PUT http://localhost:8500/v1/kv/mylock?cas<span class="o">=</span><span class="m">5638</span> --data <span class="s2">&quot;client2&quot;</span>
</span><span class='line'><span class="nb">false</span>
</span></code></pre></td></tr></table></div></figure>


<p>Consul&rsquo;s response sent to <code>client2</code> shows that Consul refused to update the <code>mylock</code> value as, in the meantime, this value has been modified. In order to check the current status of the lock, <code>client2</code> can follow up with a get request:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl http://localhost:8500/v1/kv/mylock?pretty
</span><span class='line'><span class="o">[</span>
</span><span class='line'>    <span class="o">{</span>
</span><span class='line'>        <span class="s2">&quot;LockIndex&quot;</span>: 0,
</span><span class='line'>        <span class="s2">&quot;Key&quot;</span>: <span class="s2">&quot;mylock&quot;</span>,
</span><span class='line'>        <span class="s2">&quot;Flags&quot;</span>: 0,
</span><span class='line'>        <span class="s2">&quot;Value&quot;</span>: <span class="s2">&quot;Y2xpZW50MQ==&quot;</span>,
</span><span class='line'>        <span class="s2">&quot;CreateIndex&quot;</span>: 5638,
</span><span class='line'>        <span class="s2">&quot;ModifyIndex&quot;</span>: 5801
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'><span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>In Consul&rsquo;s response we can see that the lock is currently being held by <code>client1</code>. Until <code>client1</code> hasn&rsquo;t released the lock, <code>client2</code> must not try to acquire it. It can only periodically check the status of the lock and wait until it is released. To release the lock, <code>clent1</code> will simply set its value to an empty-value:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>curl --request PUT http://localhost:8500/v1/kv/mylock --data <span class="s2">&quot;&quot;</span>
</span><span class='line'><span class="nb">true</span>
</span></code></pre></td></tr></table></div></figure>


<p>There are two more comments to add. First, the lock we implemented is purely advisory. All the clients working with the lock have to follow the same rules for the lock to function properly. Each client has to check that the lock was not acquired by somebody else before trying to acquire it. A misbehaved client can easily break the lock. Second, if the client holding the lock fails to release it (e.g. client crashes before releasing the lock), the lock will remain locked and no other client will be able to acquire it. More robust locks that are automatically released in the case of client failure can be implemented using the Consul&rsquo;s <a href="https://www.consul.io/docs/internals/sessions.html">sessions</a> along with the acquire and release operations.</p>

<h2>Leveraging the parameter cas=0</h2>

<p>In our lock implementation, we created an opened lock first and the lock acquisition comprised of two steps. In the first step, the client read the current <code>ModifyIndex</code> of the lock. In the second step, the client tried to update the lock while passing the <code>ModifyIndex</code> as a <code>cas</code> query parameter. When implementing the lock, we could have alternatively leveraged the fact that if the <code>cas</code> parameter is set to <code>0</code>, Consul will only create the key in the key-value store if it does not already exist. The state of our lock would then correspond to the existence or non-existence of the respective key in the key-value store. In order to acquire the lock, the client would send a request to create the key:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl --request PUT localhost:8500/v1/kv/mykey2?cas<span class="o">=</span><span class="m">0</span> --data <span class="s1">&#39;client1&#39;</span>
</span><span class='line'><span class="nb">true</span>
</span></code></pre></td></tr></table></div></figure>


<p>And to release the lock, the client would simply remove the respective key from the key-value store:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl --request DELETE localhost:8500/v1/kv/mykey2
</span><span class='line'><span class="nb">true</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Transactions</h2>

<p><a href="https://www.consul.io/api/txn.html">Transactions</a> in Consul manage updates or selects of multiple keys within a single, atomic transaction. A list of operations that will be executed in the transaction is specified in the body of the HTTP request. First, let&rsquo;s create a list of operations and save it as a file <code>transaction1.txt</code>:</p>

<figure class='code'><figcaption><span>transaction1.txt </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>
</span><span class='line'>  <span class="o">{</span>
</span><span class='line'>    <span class="s2">&quot;KV&quot;</span>: <span class="o">{</span>
</span><span class='line'>      <span class="s2">&quot;Verb&quot;</span>: <span class="s2">&quot;set&quot;</span>,
</span><span class='line'>      <span class="s2">&quot;Key&quot;</span>: <span class="s2">&quot;foo&quot;</span>,
</span><span class='line'>      <span class="s2">&quot;Value&quot;</span>: <span class="s2">&quot;MQ==&quot;</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>,
</span><span class='line'>  <span class="o">{</span>
</span><span class='line'>    <span class="s2">&quot;KV&quot;</span>: <span class="o">{</span>
</span><span class='line'>      <span class="s2">&quot;Verb&quot;</span>: <span class="s2">&quot;set&quot;</span>,
</span><span class='line'>      <span class="s2">&quot;Key&quot;</span>: <span class="s2">&quot;bar&quot;</span>,
</span><span class='line'>      <span class="s2">&quot;Value&quot;</span>: <span class="s2">&quot;Mg==&quot;</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Our transaction doesn&rsquo;t do anything spectacular. It just creates two key-value pairs <code>foo=1</code> and <code>bar=2</code>. Let&rsquo;s submit the transaction to Consul:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>curl --request PUT <span class="s1">&#39;localhost:8500/v1/txn?pretty&#39;</span> --data @transaction1.txt
</span><span class='line'><span class="o">{</span>
</span><span class='line'>    <span class="s2">&quot;Results&quot;</span>: <span class="o">[</span>
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>            <span class="s2">&quot;KV&quot;</span>: <span class="o">{</span>
</span><span class='line'>                <span class="s2">&quot;LockIndex&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;Key&quot;</span>: <span class="s2">&quot;foo&quot;</span>,
</span><span class='line'>                <span class="s2">&quot;Flags&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;Value&quot;</span>: null,
</span><span class='line'>                <span class="s2">&quot;CreateIndex&quot;</span>: 7267,
</span><span class='line'>                <span class="s2">&quot;ModifyIndex&quot;</span>: 7267
</span><span class='line'>            <span class="o">}</span>
</span><span class='line'>        <span class="o">}</span>,
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>            <span class="s2">&quot;KV&quot;</span>: <span class="o">{</span>
</span><span class='line'>                <span class="s2">&quot;LockIndex&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;Key&quot;</span>: <span class="s2">&quot;bar&quot;</span>,
</span><span class='line'>                <span class="s2">&quot;Flags&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;Value&quot;</span>: null,
</span><span class='line'>                <span class="s2">&quot;CreateIndex&quot;</span>: 7267,
</span><span class='line'>                <span class="s2">&quot;ModifyIndex&quot;</span>: 7267
</span><span class='line'>            <span class="o">}</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>    <span class="o">]</span>,
</span><span class='line'>    <span class="s2">&quot;Errors&quot;</span>: null
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The transaction completed successfully. In the response from Consul, we can find the list of results. The order of results corresponds to the order of operations that we submitted in our request. The value of the <code>ModifyIndex</code> <code>7267</code> is the same for both keys <code>foo</code> and <code>bar</code> as they were updated in the same transaction.</p>

<p>Next, let&rsquo;s see what happens if one of the operations in the transaction fails. To demonstrate this, we&rsquo;ll create a transaction that consists of two operations. The first operation updates the key <code>foo</code> to value <code>10</code>. The second operation updates the key <code>bar</code> to value <code>20</code> but only if the <code>ModifyIndex</code> of <code>bar</code> matches 100. We know that this condition is not fulfilled and the update should fail.</p>

<figure class='code'><figcaption><span>transaction2.txt </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>
</span><span class='line'>  <span class="o">{</span>
</span><span class='line'>    <span class="s2">&quot;KV&quot;</span>: <span class="o">{</span>
</span><span class='line'>      <span class="s2">&quot;Verb&quot;</span>: <span class="s2">&quot;set&quot;</span>,
</span><span class='line'>      <span class="s2">&quot;Key&quot;</span>: <span class="s2">&quot;foo&quot;</span>,
</span><span class='line'>      <span class="s2">&quot;Value&quot;</span>: <span class="s2">&quot;MTA=&quot;</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>,
</span><span class='line'>  <span class="o">{</span>
</span><span class='line'>    <span class="s2">&quot;KV&quot;</span>: <span class="o">{</span>
</span><span class='line'>      <span class="s2">&quot;Verb&quot;</span>: <span class="s2">&quot;cas&quot;</span>,
</span><span class='line'>      <span class="s2">&quot;Index&quot;</span>: 100,
</span><span class='line'>      <span class="s2">&quot;Key&quot;</span>: <span class="s2">&quot;bar&quot;</span>,
</span><span class='line'>      <span class="s2">&quot;Value&quot;</span>: <span class="s2">&quot;MjA=&quot;</span>
</span><span class='line'>    <span class="o">}</span>
</span><span class='line'>  <span class="o">}</span>
</span><span class='line'><span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Let&rsquo;s submit the transaction to Consul:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>curl --request PUT <span class="s1">&#39;localhost:8500/v1/txn?pretty&#39;</span> --data @transaction2.txt
</span><span class='line'><span class="o">{</span>
</span><span class='line'>    <span class="s2">&quot;Results&quot;</span>: null,
</span><span class='line'>    <span class="s2">&quot;Errors&quot;</span>: <span class="o">[</span>
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>            <span class="s2">&quot;OpIndex&quot;</span>: 1,
</span><span class='line'>            <span class="s2">&quot;What&quot;</span>: <span class="s2">&quot;failed to set key \&quot;bar\&quot;, index is stale&quot;</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'>    <span class="o">]</span>
</span><span class='line'><span class="o">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>The transaction failed, indeed. The returned error list contains all errors that occurred during the transaction processing. The operations that failed are denoted by the <code>OpIndex</code> which starts from value 0. In the example output we can see that the second operation in our transaction failed because of the stale index. Let&rsquo;s check the values of the keys <code>foo</code> and <code>bar</code> after the failed transaction:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv get foo
</span><span class='line'>1
</span><span class='line'><span class="nv">$ </span>./consul kv get bar
</span><span class='line'>2
</span></code></pre></td></tr></table></div></figure>


<p>As expected, due to the failed udpate the entire transaction has been rolled back. Keys <code>foo</code> and <code>bar</code> retained their original values <code>1</code> and <code>2</code>.</p>

<h2>Conclusion</h2>

<p>In this blog post, we explored the Check-and-Set operation supported by Consul and used it to implement a simple distributed lock. In the second part of the article, we poked into the transaction capabilities of Consul.</p>

<p>And what about you? How is your experience with using Consul for distributed locking or leader election? Did you get a chance to use transactions? I would like to hear your experiences, feel free to use the comment section below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[First Look at the Key-Value Store in Consul]]></title>
    <link href="http://alesnosek.com/blog/2017/07/15/first-look-at-the-key-value-store-in-consul/"/>
    <updated>2017-07-15T20:38:38-07:00</updated>
    <id>http://alesnosek.com/blog/2017/07/15/first-look-at-the-key-value-store-in-consul</id>
    <content type="html"><![CDATA[<p>If you are developing a distributed application that consists of multiple services, you might be thinking about how to manage the ever growing application configuration data. Instead of maintaining individual configuration files for each service, you can store all your configuration data in a key-value store. In this blog post we&rsquo;ll check out the key-value store in Consul.</p>

<!-- more -->


<p><img class="right" src="http://alesnosek.com/images/posts/consul_logo.png" width="200" height="300"></p>

<p>Consul is an open-source product developed by <a href="https://www.hashicorp.com/">HashiCorp</a> and licensed under the <a href="https://github.com/hashicorp/consul/blob/master/LICENSE">MPL 2.0</a>. While Consul uses an open core business model, it comes with a great deal of functionality in its free edition. The top two features of Consul would be the service discovery combined with health checking and the key-value store functionality that we are going to review in this article. They both come handy when building distributed applications.</p>

<h2>Getting started</h2>

<p>HashiCorp products are known for its thorough documentation and the Consul&rsquo;s <a href="https://www.consul.io/docs/index.html">documenation</a> is not an exception.</p>

<p>What I like about Consul is its installation. Written in the Go language, Consul is distributed as a single statically linked binary. Download <a href="https://www.consul.io/downloads.html">links</a> for various platforms are provided. After unzipping the distribution archive you can directly run the <code>consul</code> executable.</p>

<p>Let&rsquo;s put togher a command-line to start the Consul cluster. Our test cluster consists of a single node (<code>-bootstrap-expect 1</code>). For a production deployment, you should be looking at a cluster of three or five Consul nodes that is able to survive node failures. We will make the Consul Web UI available at <code>http://localhost:8500</code> by appending the <code>-ui</code> parameter. Consul needs a location where it will persist its data. In our example, we instruct Consul to create a directory <code>mydata</code> and store all its data in this directory. After a bit of typing, the complete commmand-line to start the Consul cluster looks as follows:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul agent -ui -server -data-dir mydata -advertise 127.0.0.1 -bootstrap-expect 1
</span></code></pre></td></tr></table></div></figure>


<p>In several seconds the one-node Consul cluster is up and running:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">==</span>&gt; WARNING: BootstrapExpect Mode is specified as 1<span class="p">;</span> this is the same as Bootstrap mode.
</span><span class='line'><span class="o">==</span>&gt; WARNING: Bootstrap mode enabled! Do not <span class="nb">enable </span>unless <span class="nv">necessary</span>
</span><span class='line'><span class="o">==</span>&gt; Starting Consul agent...
</span><span class='line'><span class="o">==</span>&gt; Consul agent running!
</span><span class='line'>           Version: <span class="s1">&#39;v0.8.5&#39;</span>
</span><span class='line'>           Node ID: <span class="s1">&#39;be79786e-749d-758c-2b65-824c1e956788&#39;</span>
</span><span class='line'>         Node name: <span class="s1">&#39;zihadlo&#39;</span>
</span><span class='line'>        Datacenter: <span class="s1">&#39;dc1&#39;</span>
</span><span class='line'>            Server: <span class="nb">true</span> <span class="o">(</span>bootstrap: <span class="nb">true</span><span class="o">)</span>
</span><span class='line'>       Client Addr: 127.0.0.1 <span class="o">(</span>HTTP: 8500, HTTPS: -1, DNS: 8600<span class="o">)</span>
</span><span class='line'>      Cluster Addr: 127.0.0.1 <span class="o">(</span>LAN: 8301, WAN: 8302<span class="o">)</span>
</span><span class='line'>    Gossip encrypt: <span class="nb">false</span>, RPC-TLS: <span class="nb">false</span>, TLS-Incoming: <span class="nb">false</span>
</span><span class='line'>
</span><span class='line'><span class="o">==</span>&gt; Log data will now stream in as it occurs:
</span><span class='line'>
</span><span class='line'>    2017/07/15 20:37:57 <span class="o">[</span>INFO<span class="o">]</span> raft: Initial configuration <span class="o">(</span><span class="nv">index</span><span class="o">=</span>1<span class="o">)</span>: <span class="o">[{</span>Suffrage:Voter ID:127.0.0.1:8300 Address:127.0.0.1:8300<span class="o">}]</span>
</span><span class='line'>    2017/07/15 20:37:57 <span class="o">[</span>INFO<span class="o">]</span> raft: Node at 127.0.0.1:8300 <span class="o">[</span>Follower<span class="o">]</span> entering Follower state <span class="o">(</span>Leader: <span class="s2">&quot;&quot;</span><span class="o">)</span>
</span><span class='line'>    2017/07/15 20:37:57 <span class="o">[</span>INFO<span class="o">]</span> serf: EventMemberJoin: zihadlo 127.0.0.1
</span><span class='line'>    2017/07/15 20:37:57 <span class="o">[</span>INFO<span class="o">]</span> consul: Adding LAN server zihadlo <span class="o">(</span>Addr: tcp/127.0.0.1:8300<span class="o">)</span> <span class="o">(</span>DC: dc1<span class="o">)</span>
</span><span class='line'>    2017/07/15 20:37:57 <span class="o">[</span>INFO<span class="o">]</span> serf: EventMemberJoin: zihadlo.dc1 127.0.0.1
</span><span class='line'>    2017/07/15 20:37:57 <span class="o">[</span>INFO<span class="o">]</span> consul: Handled member-join event <span class="k">for</span> server <span class="s2">&quot;zihadlo.dc1&quot;</span> in area <span class="s2">&quot;wan&quot;</span>
</span><span class='line'>    2017/07/15 20:37:57 <span class="o">[</span>INFO<span class="o">]</span> agent: Started DNS server 127.0.0.1:8600 <span class="o">(</span>udp<span class="o">)</span>
</span><span class='line'>    2017/07/15 20:37:57 <span class="o">[</span>INFO<span class="o">]</span> agent: Started DNS server 127.0.0.1:8600 <span class="o">(</span>tcp<span class="o">)</span>
</span><span class='line'>    2017/07/15 20:37:57 <span class="o">[</span>INFO<span class="o">]</span> agent: Started HTTP server on 127.0.0.1:8500
</span><span class='line'>    2017/07/15 20:38:02 <span class="o">[</span>WARN<span class="o">]</span> raft: Heartbeat timeout from <span class="s2">&quot;&quot;</span> reached, starting election
</span><span class='line'>    2017/07/15 20:38:02 <span class="o">[</span>INFO<span class="o">]</span> raft: Node at 127.0.0.1:8300 <span class="o">[</span>Candidate<span class="o">]</span> entering Candidate state in term 2
</span><span class='line'>    2017/07/15 20:38:02 <span class="o">[</span>INFO<span class="o">]</span> raft: Election won. Tally: 1
</span><span class='line'>    2017/07/15 20:38:02 <span class="o">[</span>INFO<span class="o">]</span> raft: Node at 127.0.0.1:8300 <span class="o">[</span>Leader<span class="o">]</span> entering Leader state
</span><span class='line'>    2017/07/15 20:38:02 <span class="o">[</span>INFO<span class="o">]</span> consul: cluster leadership acquired
</span><span class='line'>    2017/07/15 20:38:02 <span class="o">[</span>INFO<span class="o">]</span> consul: New leader elected: zihadlo
</span><span class='line'>    2017/07/15 20:38:02 <span class="o">[</span>INFO<span class="o">]</span> consul: member <span class="s1">&#39;zihadlo&#39;</span> joined, marking health alive
</span><span class='line'>    2017/07/15 20:38:02 <span class="o">[</span>INFO<span class="o">]</span> agent: Synced service <span class="s1">&#39;consul&#39;</span>
</span></code></pre></td></tr></table></div></figure>


<p>In the log output, Consul informs us that the Consul API is available at 127.0.0.1:8500. That&rsquo;s where the Consul client will connect to by default. In the following, you want to make sure that you&rsquo;re running the Consul commands on the same box as you started your Consul cluster.</p>

<p>The single <code>consul</code> binary provides the server as well as the client functionality. Let&rsquo;s list our current cluster members to verify that the client can connect to the cluster:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul members
</span><span class='line'>Node     Address         Status  Type    Build  Protocol  DC
</span><span class='line'>zihadlo  127.0.0.1:8301  alive   server  0.8.5  <span class="m">2</span>         dc1
</span></code></pre></td></tr></table></div></figure>


<h2>Basic CRUD with Consul</h2>

<p>In this section, we&rsquo;re going to exercise the basic Create, Read, Update and Delete functionality of the Consul key-store. First, let&rsquo;s store the value <code>12345</code> under the key <code>foo</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv put foo 12345
</span><span class='line'>Success! Data written to: foo
</span></code></pre></td></tr></table></div></figure>


<p>Great, the value is saved in the store. To retrieve the value under the key <code>foo</code> from Consul we can type:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv get foo
</span><span class='line'>12345
</span></code></pre></td></tr></table></div></figure>


<p>By the way, Consul doesn&rsquo;t impose any restrictions on what kind of data you may store. Only the size of the data is limited to 512KB of data per key. It&rsquo;s up to your application, what data format you choose to use. For example, you can decide to store numbers, strings, JSON-formatted data or arbitrary binary data. For instance, when designing a centralized configuration management solution for your application, you have the flexibility of storing individual configuration options as key-value pairs or decide to save entire configuration files as values in Consul.</p>

<p>To replace the value, simply put a new value in Consul under the existing key:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv put foo bar
</span><span class='line'>Success! Data written to: foo
</span></code></pre></td></tr></table></div></figure>


<p>The value has been successfully updated as we can see:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv get foo
</span><span class='line'>bar
</span></code></pre></td></tr></table></div></figure>


<p>To remove the value from the key-value store you can use the <code>delete</code> command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv delete foo
</span><span class='line'>Success! Deleted key: foo
</span></code></pre></td></tr></table></div></figure>


<p>To verify that the value is really gone, try to retrieve it:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv get foo
</span><span class='line'>Error! No key exists at: foo
</span></code></pre></td></tr></table></div></figure>


<h2>Hierarchical keys and prefix matching</h2>

<p>Keys in Consul can be organized in a hierarchy where different levels of the hierarchy are separated by the slash character (<code>/</code>). For example, you can create a database that holds the population numbers in different continents and countries (in millions of inhabitants) like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv put europe 743.1
</span><span class='line'><span class="nv">$ </span>./consul kv put europe/germany 82.67
</span><span class='line'><span class="nv">$ </span>./consul kv put europe/france 66.9
</span><span class='line'><span class="nv">$ </span>./consul kv put asia 4436
</span><span class='line'><span class="nv">$ </span>./consul kv put asia/india 1324
</span></code></pre></td></tr></table></div></figure>


<p>Now that you organized your keys hierarchically, you can use the Consul&rsquo;s prefix matching to discover the keys on the single level of hierarchy. For example, to retrive the keys with the prefix <code>e</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv get -recurse -keys e
</span><span class='line'>europe
</span><span class='line'>europe/
</span></code></pre></td></tr></table></div></figure>


<p>Prefix matching can be used to retrieve the values, too. For example, to retrieve the population numbers in Europe, you can type:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv get -recurse e
</span><span class='line'>europe:743.1
</span><span class='line'>europe/france:66.9
</span><span class='line'>europe/germany:82.67
</span></code></pre></td></tr></table></div></figure>


<p>Note that when retrieving the keys recursively, only the keys on the single level of hierarchy were returned whereas when retrieving the values recursively, values on all the nested levels of hierarchy were returned.</p>

<p>To obtain the population numbers for the European countries, you can append a slash to the keys name (<code>europe/</code>):</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv get -recurse europe/
</span><span class='line'>europe/france:66.9
</span><span class='line'>europe/germany:82.67
</span></code></pre></td></tr></table></div></figure>


<p>And if you are interested only in the European countries that start with letter <code>g</code>, you can use the prefix <code>europe/g</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>./consul kv get -recurse europe/g
</span><span class='line'>europe/germany:82.67
</span></code></pre></td></tr></table></div></figure>


<h2>Export/import of key-value pairs</h2>

<p>Another useful feaure of the Consul&rsquo;s key-value store is the bulk export and import of key-value pairs. To export the entire key-value store database, you can type:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv <span class="nb">export</span>
</span><span class='line'><span class="o">[</span>
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>                <span class="s2">&quot;key&quot;</span>: <span class="s2">&quot;asia&quot;</span>,
</span><span class='line'>                <span class="s2">&quot;flags&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;value&quot;</span>: <span class="s2">&quot;NDQzNg==&quot;</span>
</span><span class='line'>        <span class="o">}</span>,
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>                <span class="s2">&quot;key&quot;</span>: <span class="s2">&quot;asia/india&quot;</span>,
</span><span class='line'>                <span class="s2">&quot;flags&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;value&quot;</span>: <span class="s2">&quot;MTMyNA==&quot;</span>
</span><span class='line'>        <span class="o">}</span>,
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>                <span class="s2">&quot;key&quot;</span>: <span class="s2">&quot;europe&quot;</span>,
</span><span class='line'>                <span class="s2">&quot;flags&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;value&quot;</span>: <span class="s2">&quot;NzQzLjE=&quot;</span>
</span><span class='line'>        <span class="o">}</span>,
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>                <span class="s2">&quot;key&quot;</span>: <span class="s2">&quot;europe/france&quot;</span>,
</span><span class='line'>                <span class="s2">&quot;flags&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;value&quot;</span>: <span class="s2">&quot;NjYuOQ==&quot;</span>
</span><span class='line'>        <span class="o">}</span>,
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>                <span class="s2">&quot;key&quot;</span>: <span class="s2">&quot;europe/germany&quot;</span>,
</span><span class='line'>                <span class="s2">&quot;flags&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;value&quot;</span>: <span class="s2">&quot;ODIuNjc=&quot;</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'><span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Consul exports the key-value pairs into the JSON format which is currently the only supported format. In the sample output, you can see that all the values are <a href="https://en.wikipedia.org/wiki/Base64">base64</a> encoded. The base64 encoding is commonly used in the text-based formats like JSON and XML to allow embedding of binary data.</p>

<p>You can export a subset of the key-value pairs by specifying the prefix. For instance, to export the data pertaining Europe, you can speficy the <code>europe</code> prefix:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>./consul kv <span class="nb">export </span>europe
</span><span class='line'><span class="o">[</span>
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>                <span class="s2">&quot;key&quot;</span>: <span class="s2">&quot;europe&quot;</span>,
</span><span class='line'>                <span class="s2">&quot;flags&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;value&quot;</span>: <span class="s2">&quot;NzQzLjE=&quot;</span>
</span><span class='line'>        <span class="o">}</span>,
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>                <span class="s2">&quot;key&quot;</span>: <span class="s2">&quot;europe/france&quot;</span>,
</span><span class='line'>                <span class="s2">&quot;flags&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;value&quot;</span>: <span class="s2">&quot;NjYuOQ==&quot;</span>
</span><span class='line'>        <span class="o">}</span>,
</span><span class='line'>        <span class="o">{</span>
</span><span class='line'>                <span class="s2">&quot;key&quot;</span>: <span class="s2">&quot;europe/germany&quot;</span>,
</span><span class='line'>                <span class="s2">&quot;flags&quot;</span>: 0,
</span><span class='line'>                <span class="s2">&quot;value&quot;</span>: <span class="s2">&quot;ODIuNjc=&quot;</span>
</span><span class='line'>        <span class="o">}</span>
</span><span class='line'><span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>To import the JSON-formatted data back to the Consul key-value store, you can use the command <code>./consul kv import</code>.</p>

<h2>Web UI</h2>

<p>Besides the commmand-line client, you can access Consul through its beautiful Web interface. Point your web browser to <a href="http://localhost:8500">http://localhost:8500</a>.</p>

<p><img class="right" src="http://alesnosek.com/images/posts/consul_ui.png"></p>

<h2>Conclusion</h2>

<p>In this blog post, we reviewed the basics of the key-value store in Consul. There are many other cool features of the key-value store that we didn&rsquo;t cover like atomic key updates using Check-and-Set operations, <a href="https://www.consul.io/api/txn.html">transactions</a>, <a href="https://www.consul.io/docs/commands/lock.html">locks</a> or <a href="https://www.consul.io/docs/commands/watch.html">watches</a>. Also, I recommend to you to take a look at the Consul&rsquo;s great <a href="https://www.consul.io/api/index.html">RESTful API</a> that allows you to interact with Consul programatically.</p>

<p>If you&rsquo;re looking for a key-value store that would enhance your distributed application, Consul is definitely a candidate to consider. Besides that, Consul will be ready when you later on realize that service discovery is what you need to address next.</p>

<p>Are you considering or already using Consul at your company? I would like to hear your experiences, please leave your comments below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Acing the Red Hat OpenStack Certification Exams]]></title>
    <link href="http://alesnosek.com/blog/2017/06/26/acing-the-red-hat-openstack-certification-exams/"/>
    <updated>2017-06-26T19:54:19-07:00</updated>
    <id>http://alesnosek.com/blog/2017/06/26/acing-the-red-hat-openstack-certification-exams</id>
    <content type="html"><![CDATA[<p>Recently I passed two OpenStack certification exams from Red Hat: <a href="https://www.redhat.com/en/services/training/ex210-red-hat-certified-system-administrator-red-hat-openstack-exam">EX210 Red Hat Certified System Administrator in Red Hat OpenStack exam</a> and the consecutive <a href="https://www.redhat.com/en/services/training/ex310-red-hat-certified-engineer-red-hat-openstack-exam">EX310 Red Hat Certified Engineer in Red Hat OpenStack exam</a>. In this blog post, I&rsquo;m going to share how I - as a software practitioner - got the job done.</p>

<!-- more -->


<p><img class="right" src="http://alesnosek.com/images/posts/redhat_openstack.jpg" width="250" height="300"></p>

<p>All exams in the Red Hat certification program are purely practical. The first exam EX210 focuses on deployment and administration of OpenStack which includes installation of OpenStack using the Red Hat OpenStack Platform Director, creating OpenStack users, projects, managing user roles, uploading images into Glance, creating Cinder volumes, adding Neutron networks and launching stacks using Heat. The second EX310 exam includes deploying the Ceph storage on multiple nodes, integrating Ceph with OpenStack Nova, Glance and Cinder and configuring various Neutron resources like networks, load balancers and routers.</p>

<p>To prepare for the exams, I used the online courses Red Hat OpenStack Administration I, II, III (<a href="https://www.redhat.com/en/services/training/cl110-red-hat-openstack-administration-i">CL110</a>, <a href="https://www.redhat.com/en/services/training/cl210-red-hat-openstack-administration-ii">CL210</a>, <a href="https://www.redhat.com/en/services/training/cl310-red-hat-openstack-administration-iii">CL310</a>) that are included in my <a href="https://www.redhat.com/en/services/training/learning-subscription">Red Hat Learning Subscription</a>. They covered the exam requirements very well.</p>

<p>I encountered a glitch right when scheduling the exam. As I realized, the OpenStack exams EX210 and EX310 are offered in the selected exam locations only and my San Diego location, where I so far completed all of my Red Hat exams, was not included. Surprise, surprise! However, as I was already planning to visit Prague during my upcoming vacation, I decided to take my exams in Prague, for Prague - as a city of the kings - had the EX210 and EX310 exams available.</p>

<p>As the two OpenStack exams partially overlap, it was a good idea to be preparing for both of them at the same time. I even chose to take the exams on the two consecutive days Thursday and Friday. And how did I score? Well, I made 300 out of 300 points in each of the exams. It could not be any better and I was glad that the sometimes painfully gained experience with OpenStack made itself apparent.</p>

<p>The updated list of my certifications can be found on the <a href="https://www.redhat.com/rhtapps/certification/verify/?certId=160-216-727">Verify a Red Hat Certified Professional</a> website.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[What I Learned at Red Hat Summit 2017]]></title>
    <link href="http://alesnosek.com/blog/2017/05/05/what-i-learned-at-red-hat-summit-2017/"/>
    <updated>2017-05-05T18:46:14-07:00</updated>
    <id>http://alesnosek.com/blog/2017/05/05/what-i-learned-at-red-hat-summit-2017</id>
    <content type="html"><![CDATA[<p>I had the great opportunity to visit the Red Hat Summit 2017. It was hosted in the Boston Convention and Exhibition Center in Boston in May 2-4, 2017. This blog post summarizes the interesting things I learned at the summit.</p>

<!-- more -->


<h2>Major announcements</h2>

<p><img class="right" src="http://alesnosek.com/images/posts/rh_summit.png" width="200" height="200"></p>

<p><strong><a href="https://www.redhat.com/en/about/press-releases/red-hat-and-aws-extend-strategic-alliance-package-access-aws-services-within-red-hat-openshift">Red Hat OpenShift &amp; Amazon Web Services</a>.</strong> Red Hat will make AWS services accessible directly from the OpenShift web console. From within the OpenShift web console developers will be able to provision and configure AWS services such as CloudFront, ElastiCache, ELB, RDS, EMR, RedShift, S3 and Lambda.</p>

<p><strong><a href="https://www.redhat.com/en/about/press-releases/red-hat-unveils-end-end-cloud-native-development-environment-red-hat-openshiftio">Red Hat OpenShift.io</a>.</strong> <a href="https://openshift.io/">OpenShift.io</a> is an online development environment for building container-based applications. OpenShift.io can create a project in GitHub for you to store your source code. The source code editing can be done in the integrated Eclipse Che. OpenShift.io can create a project in OpenShift Online in order to build your application and deploy it. Jenkins pipelines are used to orchestrate the CI/CD process. Currently, OpenShift.io is available in a limited developer preview. You can sign up at <a href="https://openshift.io">https://openshift.io</a></p>

<h2>Sessions attended</h2>

<ul>
<li><strong>The future Red Hat Middleware portfolio: Stacks and services and solutions</strong>

<ul>
<li>Netflix OSS is interesting and Red Hat will support it because customers like it. However, some of the Netflix OSS features might be more efficiently implemented directly in Kubernetes instead of on top of Kubernetes. Netflix had to make architectural choices based on how AWS looked several years ago. AWS evolved since that time.</li>
<li>Architectural evolution: monolith -> n-tier architecture -> microservices.</li>
</ul>
</li>
<li><strong>Red Hat container technology strategy</strong>

<ul>
<li>Kubernetes has won. You may have just not realized it yet.</li>
<li>There are two reasons why the Kubernetes open source project is winning. First, it brings value to people. Second, people are excited to work on it.</li>
<li>Kubernetes = kernel of the cloud operating systems</li>
</ul>
</li>
<li><strong>Reproducible development to live applications with Java and Red Hat CDK</strong>

<ul>
<li>When developing containerized applications, developers can run them locally (outside of a container), locally on <a href="https://github.com/minishift/minishift">MiniShift</a>, or hosted on <a href="https://www.openshift.com/">OpenShift Online</a>.</li>
<li><a href="https://developers.redhat.com/products/cdk">Red Hat Container Development Kit</a> can help you develop container-based applications quickly. It uses MiniShift under the hood.</li>
</ul>
</li>
<li><strong>Modern Java and DevOps lightning talks</strong>

<ul>
<li>You can issue <code>oc cluster up</code> to create a local OpenShift all-in-one cluster (requires Origin >= 1.3).</li>
<li><a href="https://projects.eclipse.org/proposals/eclipse-microprofile">Eclipse MicroProfile</a> project is aimed at optimizing Enterprise Java for the microservices architecture. It focuses, among others, on application configuration, health-checking, fault tolerance and security.</li>
<li>You can use API Gateway (e.g <a href="https://apigee.com/about/cp/api-gateway">Apigee</a>, <a href="https://www.3scale.net/">3scale</a>) to dynamically route traffic into different OpenShift namespaces (test, staging, production).</li>
</ul>
</li>
<li><strong>Atomic BOF</strong>

<ul>
<li>In the future, the classic RHEL will be derived from the RHEL Atomic Host. It means that the new features will appear in the RHEL Atomic Host before being included into RHEL.</li>
</ul>
</li>
<li><strong>Stepping off a cliff: Common sense approaches to cloud security</strong>

<ul>
<li>VMs in the cloud are created and destroyed dynamically. This is one of the challenges for the security team.</li>
</ul>
</li>
<li><strong>Wicked fast PaaS: Performance tuning of OpenShift and Docker</strong>

<ul>
<li>RHEL 7.4 should support OverlayFS. For Docker storage, the OverlayFS is more memory efficient than the LVM thin pool provisioning, as with OverlayFS, the pages in the page cache can be shared between multiple containers.</li>
<li>Beginning with OpenShift 3.5, the container image metadata will be stored only in the Docker registry. In previous versions of OpenShift, a duplicate of the image metadata was stored in etcd, too.</li>
</ul>
</li>
<li><strong>The Truth about Microservices</strong>

<ul>
<li>&ldquo;Building a single microservice is easy. Building a microservices architecture is hard.&rdquo;</li>
</ul>
</li>
<li><strong>The hardest part of microservices is your data</strong>

<ul>
<li><a href="http://debezium.io/">Debezium</a> monitors the changes committed to the database (MySQL, MongoDB, PostgreSQL). For each database change, Debezium publishes an event to the Kafka broker. To consume the change events, an application can create a Kafka consumer that will consume all events for the topics associated with the database. In summary, Debezium turns a database transaction log into a Kafka stream that other applications can consume.</li>
</ul>
</li>
<li><strong>Reactive systems with Eclipse Vert.x and Red Hat OpenShift</strong>

<ul>
<li><a href="http://vertx.io/">Vert.x</a> is a toolkit for building reactive applications on the JVM. It can discover services on OpenShift and Kubernetes.</li>
</ul>
</li>
<li><strong>Container infrastructure trends: Optimizing for production workloads</strong>

<ul>
<li><a href="https://github.com/projectatomic/skopeo">skopeo</a> is a command line utility that allows you to inspect Docker images and image registries. It retrieves the required information from the repository metadata without the need to download the actual image. For example, with skopeo you can find out which tags are available for the given repository.</li>
<li><a href="https://github.com/projectatomic/buildah">buildah</a> a tool for building Docker images. It can build container images without using the Docker daemon. Ansible-container and OpenShift&rsquo;s S2I will be modified to use buildah under the hood.</li>
</ul>
</li>
<li><strong>Function as a Service (Faas) - why you should care and what you need to know</strong>

<ul>
<li>Architectural evolution: service -> microservice -> function.</li>
<li>Good serverless use-cases: processing web-hooks, scheduled tasks (a la cron), data transformation (converting small images).</li>
<li>Serverless architecture challenges: cannot use a larger programming framework to implement the function, as this would be too slow to initialize, increased latency in comparison to a long-running server process, large variance in latency, very complex at scale, debugging of functions is hard.</li>
<li>Red Hat participates on development of <a href="https://funktion.fabric8.io/">Funktion</a> that is part of the project <a href="https://fabric8.io/">fabric8</a>.</li>
</ul>
</li>
</ul>

]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Two Years of Technical Blogging]]></title>
    <link href="http://alesnosek.com/blog/2017/04/22/two-years-of-technical-blogging/"/>
    <updated>2017-04-22T15:06:57-07:00</updated>
    <id>http://alesnosek.com/blog/2017/04/22/two-years-of-technical-blogging</id>
    <content type="html"><![CDATA[<p>Wow, the time goes by so fast. It&rsquo;s been two years since I began writing this blog. Why did I start and am I having fun? Let&rsquo;s take a closer look.</p>

<!-- more -->


<p>I think that there are two kinds of authors. The first kind are the extroverted authors, that can fill pages and pages of paper without difficulty. They love to communicate and writing is yet another way of communication for them. The second kind of authors would be the introverted authors. Communication is not necessarily second nature for them. Before creating an article, they are concerned whether they really have something interesting to share.</p>

<p>It seems to me that in the developer community the introverted authors prevail and I personally count myself in this category, too. Only after a decade of software development, and now heading towards software architecture, I finally decided to start my own technical blog.</p>

<p><img class="right" src="http://alesnosek.com/images/posts/octopress_github.png" width="200" height="200"></p>

<h2>Getting my blog started</h2>

<p>Coming late to the game, there were plenty of technical blogs out there to draw inspiration from. I quickly settled on <a href="http://octopress.org/">Octopress</a> as the engine for my blog. Octopress is a blog generator. I can write articles using the Markdown format and Octopress generates the entire website for me. As the generated website is completely static I&rsquo;m hosting it on GitHub. No PHP or MySQL databases are needed to run my blog. Currently, the development of the Octopress engine is stalled for more than a year, however, I still like to ride this horse even when it might be dead.</p>

<p>Before I get going I typically need to read at least one book on the subject. Regarding technical blogging, I would recommend the <a href="http://technicalblogging.com/book/">Technical Blogging</a> book from The Pragmatic Programmers. I read the first several chapters before creating this blog and it was a great eye opener for me.</p>

<h2>Why is technical blogging fun?</h2>

<p>Technical blogging takes some effort, however, I&rsquo;m really enjoying it. Here are my top five reasons why I like technical blogging:</p>

<ol>
<li><em>Blog gives others insight into what you&rsquo;re working on</em>. People with similar interests will find you if you show what you&rsquo;re passionate about.</li>
<li><em>Blog as a contribution to open source.</em> Creating articles and tutorials about open source software is a form of contribution to the software we all love.</li>
<li><em>Blogging helps you to more deeply understand the subject you are writing about.</em> The best way to learn something is to teach it. In addition to teaching, writing about something is also a good way to expand your learning.</li>
<li><em>Blog is a documentation you can refer to in the future.</em> It happened to me several times that I had to come back to my own article to refresh my memory on the subject.</li>
<li><em>Blogging advances your writing skills.</em> Concise and easy to understand written communication is certainly appreciated by other developers on your team.</li>
</ol>


<h2>Let me know how I&rsquo;m doing</h2>

<p>Feedback is always greatly appreciated. If you find some time, I would be happy to hear how I&rsquo;m doing as a blogger. You can leave your comments in the comment section below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Passed the OpenShift EX280 certification!]]></title>
    <link href="http://alesnosek.com/blog/2017/04/04/passed-the-openshift-ex280-certification/"/>
    <updated>2017-04-04T23:00:22-07:00</updated>
    <id>http://alesnosek.com/blog/2017/04/04/passed-the-openshift-ex280-certification</id>
    <content type="html"><![CDATA[<p>It&rsquo;s been more than half a year since I started working with OpenShift. Today I successfully passed the <a href="https://www.redhat.com/en/services/training/ex280-red-hat-certificate-expertise-platform-service-exam">EX280 Red Hat Certificate of Expertise in Platform-as-a-Service exam</a> and earned a certificate. I&rsquo;m going to share a few details about the exam in this blog post.</p>

<!-- more -->


<p>Similar to the RHCSA/RHCE exams that I <a href="http://alesnosek.com/blog/2016/11/07/rhcsa-slash-rhce-exam-experience/">completed</a> some time ago, the EX280 OpenShift exam is also purely practical. You will have to install OpenShift 3.0, configure it and deploy multiple containerized applications on it.</p>

<p>For preparation I used the <a href="https://www.redhat.com/en/services/training/do280-openshift-enterprise-administration">DO280 OpenShift Enterprise Administration</a> materials that were included in my <a href="https://www.redhat.com/en/services/training/learning-subscription">Red Hat Learning Subscription</a>. I practiced the provided lab exercices over and over again until I gained a good confidence.</p>

<p><img class="right" src="http://alesnosek.com/images/posts/openshift_container_platform.png" width="250" height="300"></p>

<p>The exam took three hours and I have to say that I was very busy typing the whole time. Despite of my best effort I ran out of time with three tasks left untouched. How happy I was when I received my exam results. Passing score for the exam was 210 points. I made it through with 225 points.</p>

<p>I truly enjoy the Red Hat certification program and want to keep growing my collection of certificates. The list of my current certifications can be found on the <a href="https://www.redhat.com/rhtapps/certification/verify/?certId=160-216-727">Verify a Red Hat Certified Professional</a> website.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[An Introduction to Building on OpenShift]]></title>
    <link href="http://alesnosek.com/blog/2017/03/19/an-introduction-to-building-on-openshift/"/>
    <updated>2017-03-19T10:48:32-07:00</updated>
    <id>http://alesnosek.com/blog/2017/03/19/an-introduction-to-building-on-openshift</id>
    <content type="html"><![CDATA[<p>For years a Jenkins server has been driving the software builds in our company. Some time ago, we deployed an OpenShift cluster. The primary purpose of our OpenShift cluster was to support the efforts of dockerizing our software products. However, as OpeShift is a complete PaaS solution we started thinking about leveraging OpenShift for software builds, too. In this blog post I&rsquo;d like to share what we learned about building on OpenShift so far.</p>

<!-- more -->


<p>Before we begin talking about OpenShift, let&rsquo;s briefly discuss our current build environment. In the center of our build environment there is a Jenkins server. In Jenkins, we maintain numerous jobs to build our software, run automated tests, drive various devops tasks and much more. Jenkins is our central place from where the automated processes are started and monitored. As Jenkins is greatly extensible via plugins, we were able to easily integrate Jenkins with other tools, too.</p>

<h2>Building the OpenShift way</h2>

<p><img class="right" src="http://alesnosek.com/images/posts/openshift_logo.gif" width="200" height="200"></p>

<p>In OpenShift, one has to create a <em>BuildConfig</em> resource to describe the <a href="https://docs.openshift.org/latest/dev_guide/builds/index.html">build process</a>. The BuildConfig resource in OpenShift is roughly equivalent to a job definition in Jenkins. When creating a BuildConfig resource, a build strategy has to be chosen. The build strategy resembles a job type in Jenkins. Currently, there are four build strategies available in OpenShift:</p>

<p><strong>Source-to-Image strategy</strong>. Allows you to create a container image starting from the application source code. During the build process, the source code is downloaded into a container and compiled there. The finished binary artifacts are installed into the container. The complete container image is then pushed into the Docker registry from where it can be deployed as an application on OpenShift.</p>

<p><strong>Docker strategy</strong>. The input of the build process is a Dockerfile. OpenShift will execute a Docker build using the provided Dockerfile and upload the resulting image into the Docker registry from where it can be deployed.</p>

<p><strong>Custom strategy</strong>. Custom strategy could be compared to a free style job in Jenkins. The outcome of the build doesn&rsquo;t have to be a Docker image. Instead, the custom strategy allows you to create JARs, tarballs, RPMs or other artifacts which you have to upload to the repository of your choice by the end of the build.</p>

<p><strong>Pipeline strategy</strong>. In OpenShift 3.3, a new build strategy was introduced called <em>Pipeline</em>. This strategy doesn&rsquo;t really build anything but enables you to implement workflows on OpenShift. The great article <a href="https://blog.openshift.com/openshift-3-3-pipelines-deep-dive/">OpenShift 3.3 Pipelines - Deep Dive</a> describes how the Pipeline strategy works. In summary, you can create a BuildConfig in OpenShift that contains a definition of a Jenkins pipeline (using the Groovy DSL language). Based on this definition, OpenShift will create a pipeline job in Jenkins and execute it. Among other things, the Jenkins job can trigger a build on OpenShift, verify that the build succeeded and trigger a deployment. This approach allows OpenShift to leverage Jenkins pipelines to orchestrate a more involved CI/CD workflow possibly encompassing a conditional execution of multiple OpenShift builds and deployments.</p>

<p>An alternative to using the Pipeline strategy in OpenShift would be defining the pipeline job directly in Jenkins. With the <a href="https://plugins.jenkins.io/openshift-pipeline">OpenShift pipeline plugin</a> installed, one can trigger OpenShift operations from within the pipeline job.</p>

<p>As I didn&rsquo;t really work with the OpenShift strategies much I&rsquo;m not going to elaborate any further. Instead, in the next section, I&rsquo;m going to mention two Jenkins plugins that we are successfully using to run builds on OpenShift.</p>

<h2>Builds on Openshift driven by Jenkins</h2>

<p><img class="right" src="http://alesnosek.com/images/posts/jenkins_logo.png" width="200" height="200"></p>

<p>There are two Jenkins plugins that can leverage OpenShift containers as build slaves:</p>

<p><strong><a href="https://wiki.jenkins-ci.org/display/JENKINS/Swarm+Plugin">Swarm plugin</a></strong>. The Swarm plugin consists of two parts: a Jenkins plugin and a CLI client. Jenkins plugin exposes an endpoint where the CLI clients can register themselves. A CLI client acts as a Jenkins slave. It runs indefinitely within a Docker container and provides Jenkins with a configurable number of build executors. While the plugin is called a Swarm plugin it doesn&rsquo;t really need any Swarm orchestration. It can happily run in a Docker container on OpenShift.</p>

<p><strong><a href="https://wiki.jenkins-ci.org/display/JENKINS/Kubernetes+Plugin">Kubernetes plugin</a></strong>. Works perfectly with OpenShift. In contrast to the Swarm plugin, Kubernetes plugin spins up a new Docker slave for each job on the fly and destroys it as soon as the job has finished running.</p>

<p>Because the Jenkins workspace is created inside of the container, it will be deleted as soon as the Docker container is terminated. If you&rsquo;d like to reuse the same workspace for subsequent builds, I&rsquo;d like to offer you two options how to create persistent workspaces:</p>

<ol>
<li><p>You can attach a volume of type <em>hostPath</em> to your slave pods and place your workspace on that volume. At the same time you have to speficy a <em>nodeSelector</em> on your slave pods that would instruct OpenShift to schedule all your slave pods onto the same OpenShift node. With this approach the Jenkins slave can access its workspace on the local storage. Unfortunately, all the slaves that need to share a workspace have to run on the same OpenShift node which can get overloaded.</p></li>
<li><p>You can attach a volume with the <em>ReadWriteMany</em> capability to your slave pods and place your workspace on this volume. Eligible volume types are NFS, GlusterFS or CephFS. Using this method a Jenkins slave running on any node in the cluster can access the shared workspace. The downside is that the access is over the network and hence slower than an access to the local storage.</p></li>
</ol>


<h2>Conclusion</h2>

<p>In the this blog post we reviewed different approaches how to leverage an OpenShift cluster for software builds. On one hand, builds can be defined within OpenShift by creating the BuildConfig resources. This approach might be less flexible than using a full-fledged build server like Jenkins, however, one can be sure that the builds will work on any OpenShift cluster including the public cloud. On the other hand, we have seen that in an environment where Jenkins is already the king, we can leverage the Swarm or Kubernetes plugin to allow Jenkins to schedule build jobs on OpenShift.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Accessing Kubernetes Pods from Outside of the Cluster]]></title>
    <link href="http://alesnosek.com/blog/2017/02/14/accessing-kubernetes-pods-from-outside-of-the-cluster/"/>
    <updated>2017-02-14T23:36:37-08:00</updated>
    <id>http://alesnosek.com/blog/2017/02/14/accessing-kubernetes-pods-from-outside-of-the-cluster</id>
    <content type="html"><![CDATA[<p>There are several ways how to expose your application running on the Kubernetes cluster to the outside world. When reading the <a href="https://kubernetes.io/docs/">Kubernetes documentation</a> I had a hard time ordering the different approaches in my head. I created this blog post for my future reference but will be happy if it can be of any use to you. Without further ado let&rsquo;s discuss the <em>hostNetwork</em>, <em>hostPort</em>, <em>NodePort</em>, <em>LoadBalancer</em> and <em>Ingress</em> features of Kubernetes.</p>

<!-- more -->


<h2>hostNetwork: true</h2>

<p>The <code>hostNetwork</code> setting applies to the Kubernetes pods. When a pod is configured with <code>hostNetwork: true</code>, the applications running in such a pod can directly see the network interfaces of the host machine where the pod was started. An application that is configured to listen on all network interfaces will in turn be accessible on all network interfaces of the host machine. Here is an example definition of a pod that uses host networking:</p>

<figure class='code'><figcaption><span>influxdb-hostnetwork.yml </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">apiVersion</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">v1</span>
</span><span class='line'><span class="l-Scalar-Plain">kind</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Pod</span>
</span><span class='line'><span class="l-Scalar-Plain">metadata</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'><span class="l-Scalar-Plain">spec</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">hostNetwork</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">true</span>
</span><span class='line'>  <span class="l-Scalar-Plain">containers</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'>      <span class="l-Scalar-Plain">image</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span></code></pre></td></tr></table></div></figure>


<p>You can start the pod with the following command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>kubectl create -f influxdb-hostnetwork.yml
</span></code></pre></td></tr></table></div></figure>


<p>You can check that the InfluxDB application is running with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl -v http://kubenode01.example.com:8086/ping
</span></code></pre></td></tr></table></div></figure>


<p>Remember to replace the host name in the above URL with the host name or IP address of the Kubernetes node where your pod has been scheduled to run. InfluxDB will respond with HTTP 204 No Content when working properly.</p>

<p>Note that every time the pod is restarted Kubernetes can reschedule the pod onto a different node and so the application will change its IP address. Besides that two applications requiring the same port cannot run on the same node. This can lead to port conflicts when the number of applications running on the cluster grows. On top of that, creating a pod with <code>hostNetwork: true</code> on OpenShift is a privileged operation. For these reasons, the host networking is not a good way to make your applications accessible from outside of the cluster.</p>

<p>What is the host networking good for? For cases where a direct access to the host networking is required. For example, the Kubernetes networking plugin Flannel can be deployed as a daemon set on all nodes of the Kubernetes cluster. Due to <code>hostNetwork: true</code> the Flannel has full control of the networking on every node in the cluster allowing it to manage the overlay network to which the pods with <code>hostNetwork: false</code> are connected to.</p>

<h2>hostPort</h2>

<p>The <code>hostPort</code> setting applies to the Kubernetes containers. The container port will be exposed to the external network at <em>&lt;hostIP>:&lt;hostPort></em>, where the <em>hostIP</em> is the IP address of the Kubernetes node where the container is running and the <em>hostPort</em> is the port requested by the user. Here comes a sample pod definition:</p>

<figure class='code'><figcaption><span>influxdb-hostport.yml </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">apiVersion</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">v1</span>
</span><span class='line'><span class="l-Scalar-Plain">kind</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Pod</span>
</span><span class='line'><span class="l-Scalar-Plain">metadata</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'><span class="l-Scalar-Plain">spec</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">containers</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'>      <span class="l-Scalar-Plain">image</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'>      <span class="l-Scalar-Plain">ports</span><span class="p-Indicator">:</span>
</span><span class='line'>        <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">containerPort</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">8086</span>
</span><span class='line'>          <span class="l-Scalar-Plain">hostPort</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">8086</span>
</span></code></pre></td></tr></table></div></figure>


<p>The hostPort feature allows to expose a single container port on the host IP. Using the hostPort to expose an application to the outside of the Kubernetes cluster has the same drawbacks as the hostNetwork approach discussed in the previous section. The host IP can change when the container is restarted, two containers using the same hostPort cannot be scheduled on the same node and the usage of the hostPort is considered a privileged operation on OpenShift.</p>

<p>What is the hostPort used for? For example, the nginx based <a href="https://github.com/kubernetes/ingress/tree/master/controllers/nginx">Ingress controller</a> is deployed as a set of containers running on top of Kubernetes. These containers are configured to use hostPorts 80 and 443 to allow the inbound traffic on these ports from the outside of the Kubernetes cluster.</p>

<h2>NodePort</h2>

<p>The <code>NodePort</code> setting applies to the Kubernetes services. By default Kubernetes services are accessible at the ClusterIP which is an internal IP address reachable from inside of the Kubernetes cluster only. The ClusterIP enables the applications running within the pods to access the service. To make the service accessible from outside of the cluster a user can create a service of type NodePort. At first, let&rsquo;s review the definition of the pod that we&rsquo;ll expose using a NodePort service:</p>

<figure class='code'><figcaption><span>influxdb-pod.yml </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">apiVersion</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">v1</span>
</span><span class='line'><span class="l-Scalar-Plain">kind</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Pod</span>
</span><span class='line'><span class="l-Scalar-Plain">metadata</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'>  <span class="l-Scalar-Plain">labels</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'><span class="l-Scalar-Plain">spec</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">containers</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'>      <span class="l-Scalar-Plain">image</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'>      <span class="l-Scalar-Plain">ports</span><span class="p-Indicator">:</span>
</span><span class='line'>        <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">containerPort</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">8086</span>
</span></code></pre></td></tr></table></div></figure>


<p>When creating a NodePort service, the user can specify a port from the range 30000-32767, and each Kubernetes node will proxy that port to the pods selected by the service. A sample definition of a NodePort service looks as follows:</p>

<figure class='code'><figcaption><span>influxdb-nodeport.yml </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">kind</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Service</span>
</span><span class='line'><span class="l-Scalar-Plain">apiVersion</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">v1</span>
</span><span class='line'><span class="l-Scalar-Plain">metadata</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'><span class="l-Scalar-Plain">spec</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">type</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">NodePort</span>
</span><span class='line'>  <span class="l-Scalar-Plain">ports</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">port</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">8086</span>
</span><span class='line'>      <span class="l-Scalar-Plain">nodePort</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">30000</span>
</span><span class='line'>  <span class="l-Scalar-Plain">selector</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span></code></pre></td></tr></table></div></figure>


<p>Note that on OpenShift more privileges are required to create a NodePort service. After the service has been created, the kube-proxy component that runs on each node of the Kubernetes cluster and listens on all network interfaces is instructed to accept connections on port 30000. The incoming traffic is forwarded by the kube-proxy to the selected pods in a round-robin fashion. You should be able to access the InfluxDB application from outside of the cluster using the command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl -v http://kubenode01.example.com:30000/ping
</span></code></pre></td></tr></table></div></figure>


<p>The NodePort service represents a static endpoint through which the selected pods can be reached. If you prefer serving your application on a different port than the 30000-32767 range, you can deploy an external load balancer in front of the Kubernetes nodes and forward the traffic to the NodePort on each of the Kubernetes nodes. This gives you an extra resiliency for the case that some of the Kubernetes nodes becomes unavailable, too. If you&rsquo;re hosting your Kubernetes cluster on one of the supported cloud providers like AWS, Azure or GCE, Kubernetes can provision an external load balancer for you. We&rsquo;ll take a look at how to do it in the next section.</p>

<h2>LoadBalancer</h2>

<p>The <code>LoadBalancer</code> setting applies to the Kubernetes service. In order to be able to create a service of type LoadBalancer, a cloud provider has to be enabled in the configuration of the Kubernetes cluster. As of version 1.6, Kubernetes can provision load balancers on AWS, Azure, CloudStack, GCE and OpenStack. Here is an example definition of the LoadBalancer service:</p>

<figure class='code'><figcaption><span>influxdb-loadbalancer.yml </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">kind</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Service</span>
</span><span class='line'><span class="l-Scalar-Plain">apiVersion</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">v1</span>
</span><span class='line'><span class="l-Scalar-Plain">metadata</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'><span class="l-Scalar-Plain">spec</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">type</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">LoadBalancer</span>
</span><span class='line'>  <span class="l-Scalar-Plain">ports</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">port</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">8086</span>
</span><span class='line'>  <span class="l-Scalar-Plain">selector</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span></code></pre></td></tr></table></div></figure>


<p>Let&rsquo;s take a look at what Kubernetes created for us:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>kubectl get svc influxdb
</span><span class='line'>NAME       CLUSTER-IP     EXTERNAL-IP     PORT<span class="o">(</span>S<span class="o">)</span>          AGE
</span><span class='line'>influxdb   10.97.121.42   10.13.242.236   8086:30051/TCP   39s
</span></code></pre></td></tr></table></div></figure>


<p>In the command output we can read that the influxdb service is internally reachable at the ClusterIP 10.97.121.42. Next, Kubernetes allocated a NodePort 30051. Because we didn&rsquo;t specify a desired NodePort number, Kubernetes picked one for us. We can check the reachability of the InfluxDB application through the NodePort with the command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl -v http://kubenode01.example.com:30051/ping
</span></code></pre></td></tr></table></div></figure>


<p>Finally, Kubernetes reached out to the cloud provider to provision a load balancer. The VIP of the load balancer is 10.13.242.236 as it is shown in the command output. Now we can access the InfluxDB application through the load balancer like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl -v http://10.13.242.236:8086/ping
</span></code></pre></td></tr></table></div></figure>


<p>My cloud provider is OpenStack. Let&rsquo;s examine how the provisioned load balancer on OpenStack looks like:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>neutron lb-vip-show 9bf2a580-2ba4-4494-93fd-9b6969c55ac3
</span><span class='line'>+---------------------+--------------------------------------------------------------+
</span><span class='line'><span class="p">|</span> Field               <span class="p">|</span> Value                                                        <span class="p">|</span>
</span><span class='line'>+---------------------+--------------------------------------------------------------+
</span><span class='line'><span class="p">|</span> address             <span class="p">|</span> 10.13.242.236                                                <span class="p">|</span>
</span><span class='line'><span class="p">|</span> admin_state_up      <span class="p">|</span> True                                                         <span class="p">|</span>
</span><span class='line'><span class="p">|</span> connection_limit    <span class="p">|</span> -1                                                           <span class="p">|</span>
</span><span class='line'><span class="p">|</span> description         <span class="p">|</span> Kubernetes external service a6ffa4dadf99711e68ea2fa163e0b082 <span class="p">|</span>
</span><span class='line'><span class="p">|</span> id                  <span class="p">|</span> 9bf2a580-2ba4-4494-93fd-9b6969c55ac3                         <span class="p">|</span>
</span><span class='line'><span class="p">|</span> name                <span class="p">|</span> a6ffa4dadf99711e68ea2fa163e0b082                             <span class="p">|</span>
</span><span class='line'><span class="p">|</span> pool_id             <span class="p">|</span> 392917a6-ed61-4924-acb2-026cd4181755                         <span class="p">|</span>
</span><span class='line'><span class="p">|</span> port_id             <span class="p">|</span> e450b80b-6da1-4b31-a008-280abdc6400b                         <span class="p">|</span>
</span><span class='line'><span class="p">|</span> protocol            <span class="p">|</span> TCP                                                          <span class="p">|</span>
</span><span class='line'><span class="p">|</span> protocol_port       <span class="p">|</span> <span class="m">8086</span>                                                         <span class="p">|</span>
</span><span class='line'><span class="p">|</span> session_persistence <span class="p">|</span>                                                              <span class="p">|</span>
</span><span class='line'><span class="p">|</span> status              <span class="p">|</span> ACTIVE                                                       <span class="p">|</span>
</span><span class='line'><span class="p">|</span> status_description  <span class="p">|</span>                                                              <span class="p">|</span>
</span><span class='line'><span class="p">|</span> subnet_id           <span class="p">|</span> 73f8eb91-90cf-42f4-85d0-dcff44077313                         <span class="p">|</span>
</span><span class='line'><span class="p">|</span> tenant_id           <span class="p">|</span> 4d68886fea6e45b0bc2e05cd302cccb9                             <span class="p">|</span>
</span><span class='line'>+---------------------+--------------------------------------------------------------+
</span><span class='line'>
</span><span class='line'><span class="nv">$ </span>neutron lb-pool-show 392917a6-ed61-4924-acb2-026cd4181755
</span><span class='line'>+------------------------+--------------------------------------+
</span><span class='line'><span class="p">|</span> Field                  <span class="p">|</span> Value                                <span class="p">|</span>
</span><span class='line'>+------------------------+--------------------------------------+
</span><span class='line'><span class="p">|</span> admin_state_up         <span class="p">|</span> True                                 <span class="p">|</span>
</span><span class='line'><span class="p">|</span> description            <span class="p">|</span>                                      <span class="p">|</span>
</span><span class='line'><span class="p">|</span> health_monitors        <span class="p">|</span>                                      <span class="p">|</span>
</span><span class='line'><span class="p">|</span> health_monitors_status <span class="p">|</span>                                      <span class="p">|</span>
</span><span class='line'><span class="p">|</span> id                     <span class="p">|</span> 392917a6-ed61-4924-acb2-026cd4181755 <span class="p">|</span>
</span><span class='line'><span class="p">|</span> lb_method              <span class="p">|</span> ROUND_ROBIN                          <span class="p">|</span>
</span><span class='line'><span class="p">|</span> members                <span class="p">|</span> d0825cc2-46a3-43bd-af82-e9d8f1f85299 <span class="p">|</span>
</span><span class='line'><span class="p">|</span>                        <span class="p">|</span> 3f73d3bb-bc40-478d-8d0e-df05cdfb9734 <span class="p">|</span>
</span><span class='line'><span class="p">|</span> name                   <span class="p">|</span> a6ffa4dadf99711e68ea2fa163e0b082     <span class="p">|</span>
</span><span class='line'><span class="p">|</span> protocol               <span class="p">|</span> TCP                                  <span class="p">|</span>
</span><span class='line'><span class="p">|</span> provider               <span class="p">|</span> haproxy                              <span class="p">|</span>
</span><span class='line'><span class="p">|</span> status                 <span class="p">|</span> ACTIVE                               <span class="p">|</span>
</span><span class='line'><span class="p">|</span> status_description     <span class="p">|</span>                                      <span class="p">|</span>
</span><span class='line'><span class="p">|</span> subnet_id              <span class="p">|</span> 73f8eb91-90cf-42f4-85d0-dcff44077313 <span class="p">|</span>
</span><span class='line'><span class="p">|</span> tenant_id              <span class="p">|</span> 4d68886fea6e45b0bc2e05cd302cccb9     <span class="p">|</span>
</span><span class='line'><span class="p">|</span> vip_id                 <span class="p">|</span> 9bf2a580-2ba4-4494-93fd-9b6969c55ac3 <span class="p">|</span>
</span><span class='line'>+------------------------+--------------------------------------+
</span><span class='line'>
</span><span class='line'><span class="nv">$ </span>neutron lb-member-list
</span><span class='line'>+--------------------------------------+--------------+---------------+--------+----------------+--------+
</span><span class='line'><span class="p">|</span> id                                   <span class="p">|</span> address      <span class="p">|</span> protocol_port <span class="p">|</span> weight <span class="p">|</span> admin_state_up <span class="p">|</span> status <span class="p">|</span>
</span><span class='line'>+--------------------------------------+--------------+---------------+--------+----------------+--------+
</span><span class='line'><span class="p">|</span> 3f73d3bb-bc40-478d-8d0e-df05cdfb9734 <span class="p">|</span> 10.13.241.89 <span class="p">|</span>         <span class="m">30051</span> <span class="p">|</span>      <span class="m">1</span> <span class="p">|</span> True           <span class="p">|</span> ACTIVE <span class="p">|</span>
</span><span class='line'><span class="p">|</span> d0825cc2-46a3-43bd-af82-e9d8f1f85299 <span class="p">|</span> 10.13.241.10 <span class="p">|</span>         <span class="m">30051</span> <span class="p">|</span>      <span class="m">1</span> <span class="p">|</span> True           <span class="p">|</span> ACTIVE <span class="p">|</span>
</span><span class='line'>+--------------------------------------+--------------+---------------+--------+----------------+--------+
</span></code></pre></td></tr></table></div></figure>


<p>Kubernetes created a TCP load balancer with the VIP 10.13.242.236 and port 8086. There are two pool members associated with the load balancer: 10.13.241.89 and 10.13.241.10. These are the IP addresses of the nodes in my two-node Kubernetes cluster. The traffic is forwarded to the NodePort 30051 of these two nodes.</p>

<p>The load balancer created by Kubernetes is a plain TCP round-robin load balancer. It doesn&rsquo;t offer SSL termination or HTTP routing. Besides that, Kubernetes will create a separate load balancer for each service. This can become quite costly when the number of your services increases. Instead of letting Kubernetes manage the load balancer, you can go back to deploying NodePort services and provision and configure an external load balancer yourself. Another option is leveraging the Kubernetes Ingress resource that we will discuss in the next section.</p>

<h2>Ingress</h2>

<p>The <code>Ingress</code> resource type was introduced in Kubernetes version 1.1. The Kubernetes cluster must have an <a href="https://github.com/kubernetes/ingress/tree/master/controllers/nginx">Ingress controller</a> deployed in order for you to be able to create Ingress resources. What is the Ingress controller? The Ingress controller is deployed as a Docker container on top of Kubernetes. Its Docker image contains a load balancer like nginx or HAProxy and a controller daemon. The controller daemon receives the desired Ingress configuration from Kubernetes. It generates an nginx or HAProxy configuration file and restarts the load balancer process for changes to take effect. In other words, Ingress controller is a load balancer managed by Kubernetes.</p>

<p>The Kubernetes Ingress provides features typical for a load balancer: HTTP routing, sticky sessions, SSL termination, SSL passthrough, TCP and UDP load balancing &hellip; At the moment not every Ingress controller implements all the available features. You have to consult the documentation of your Ingress controller to learn about its capabilities.</p>

<p>Let&rsquo;s expose our InfluxDB application to the outside world via Ingress. An example Ingress definition looks like this:</p>

<figure class='code'><figcaption><span>influxdb-ingress.yml </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">apiVersion</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">extensions/v1beta1</span>
</span><span class='line'><span class="l-Scalar-Plain">kind</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Ingress</span>
</span><span class='line'><span class="l-Scalar-Plain">metadata</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'><span class="l-Scalar-Plain">spec</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">rules</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">host</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb.kube.example.com</span>
</span><span class='line'>      <span class="l-Scalar-Plain">http</span><span class="p-Indicator">:</span>
</span><span class='line'>        <span class="l-Scalar-Plain">paths</span><span class="p-Indicator">:</span>
</span><span class='line'>          <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">backend</span><span class="p-Indicator">:</span>
</span><span class='line'>              <span class="l-Scalar-Plain">serviceName</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">influxdb</span>
</span><span class='line'>              <span class="l-Scalar-Plain">servicePort</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">8086</span>
</span></code></pre></td></tr></table></div></figure>


<p>Our DNS is setup to resolve *.kube.example.com to the IP address 10.13.241.10. This is the IP address of the Kubernetes node where the Ingress controller is running. As we already mentioned when discussing the hostPort, the Ingress listens for the incoming connections on two hostPorts 80 and 443 for the HTTP and HTTPS requests, respectively. Let&rsquo;s check that we can reach the InfluxDB application via Ingress:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl -v http://influxdb.kube.example.com/ping
</span></code></pre></td></tr></table></div></figure>


<p>When everything is setup correctly, the InfluxDB will respond with HTTP 204 No Content.</p>

<p>There&rsquo;s a difference between the LoadBalancer service and the Ingress in how the traffic routing is realized. In the case of the LoadBalancer service, the traffic that enters through the external load balancer is forwarded to the kube-proxy that in turn forwards the traffic to the selected pods. In contrast, the Ingress load balancer forwards the traffic straight to the selected pods which is more efficient.</p>

<h2>Conclusion</h2>

<p>Overall, when exposing pods to the outside of the Kubernetes cluster, the Ingress seems to be a very flexible and convenient solution. Unfortunately, it&rsquo;s also the less mature among the discussed approaches. When choosing the NodePort service, you might want to deploy a load balancer in front of your cluster as well. If you are hosting Kubernetes on one of the supported clouds, the LoadBalancer service is another option for you.</p>

<p>How do you route the external traffic to the Kubernetes pods? Glad to hear about your experience in the Comments section below!</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[TripleO Installer, Production Ready?]]></title>
    <link href="http://alesnosek.com/blog/2017/01/15/tripleo-installer-production-ready/"/>
    <updated>2017-01-15T23:13:32-08:00</updated>
    <id>http://alesnosek.com/blog/2017/01/15/tripleo-installer-production-ready</id>
    <content type="html"><![CDATA[<p><a href="https://wiki.openstack.org/wiki/TripleO">TripleO</a> is an OpenStack deployment and management tool we&rsquo;ve been using on the production systems for a while now. As TripleO is an upstream project for the Red Hat OpenStack Platform Director one would expect a decently working tool able to manage large-scale OpenStack deployments. What is our experience with TripleO?</p>

<!-- more -->


<h2>Introduction</h2>

<p>Six months have passed since we deployed a private cloud in our company. Our cloud is based on the RDO distribution of OpenStack Mitaka running on top of RHEL 7. I have to say that we&rsquo;re very happy with our cloud-based environment. OpenStack simplified the management of virtual machines and boosted the productivity of our engineering team which enjoys the self-service provided by the OpenStack APIs. Our test automation creates and destroys many virtual machines a day making sure that our software product is tested in a clean and well-defined environment. OpenStack quickly became a critical part of our infrastructure.</p>

<p>Hence we were less pleased when the last week a routine maintenance of the OpenStack cluster turned into an unplanned downtime of two compute nodes. But before we get to the problem itself let me introduce you to the specifics of how we manage the OpenStack cluster.</p>

<h2>Overcloud maintenance is a challenge</h2>

<p>A cloud life-cycle management tool of choice in the RDO distribution is TripleO. I published an article about my initial experience with TripleO a while ago: <a href="http://alesnosek.com/blog/2016/03/27/tripleo-installer-the-good/">TripleO Installer - the Good, the Bad and the Ugly</a>. Overall, the way how TripleO configures the OpenStack cluster is rather less flexible. After spending time on customizing and patching TripleO we decided that there must be an easier way. Eventually, we implemented our own set of Ansible scripts that allow an additional fine-grained configuration of OpenStack nodes. After the <code>openstack overcloud deploy</code> command is complete we run our Ansible scripts to apply an additional configuration to the overcloud. There are two benefits to this approach. First, we don&rsquo;t have to patch TripleO scripts which will be upgraded in the next release of OpenStack. And second, we can keep using Ansible which is our favorite configuration tool.</p>

<p>Having updated the overcloud using TripleO several times we realized that the update procedure is rather unreliable. Some time the TripleO update would fail with an error. Other time the overcloud update would just hang forever. Probably due to the undeterministic behaviour of the Puppet scripts that constitute a substantial part of TripleO we experienced random errors that would not occur again after restarting the update procedure. Situation got worse after we configured the overcloud Keystone to authenticate OpenStack users against Active Directory. The overcloud update would not run into completion anymore due to a defect in the Puppet scripts.</p>

<p>Because fixing the TripleO scripts would require additional effort and the overcloud update would remain a risky operation either way we concluded that we will require a downtime when updating the overcloud. During the downtime period the existing virtual machines are fully operational only the OpenStack services that allow users to create or delete virtual machines or other cloud resources are not available. In the case of our private cloud this was an acceptable albeit not ideal solution.</p>

<p>In summary, we can depict our OpenStack maintenance process like this:</p>

<p><img src="http://alesnosek.com/images/posts/openstack_maintenance_process.svg" width="500" height="700" title="OpenStack Maintenance Process" ></p>

<h2>TripleO installer and the resulting downtime</h2>

<p>On all our OpenStack nodes we use bonded network interfaces to protect the nodes against network failures. In network interface bonding a pair of physical network interfaces is combined into a single logical interface. This provides redundancy by allowing failover from one physical interface to another in the case of failure.</p>

<p>It happened to us that on two of our compute nodes one physical network interface per bond was not working. In this situation the network connection is still functional but not redundant anymore. Unfortunately, for the TripleO installer this was not good enough. Normally, during the overcloud update the <code>os-net-config</code> utility configures the node networking. Due to the single network interface down <code>os-net-config</code> failed to create a correct network configuration. A &ldquo;safe&rdquo; default configuration was generated instead which configured all available network interfaces to use DHCP. Unfortunately, we prefer a static network configuration of overcloud nodes and so no DHCP server was available. Hence this &ldquo;safe&rdquo; default configuration rendered the two compute nodes unreachable including all the virtual machines that were running on top of them!</p>

<p>We were able to fix the networking issue on the first compute node quickly. However, the physical network interfaces on the second compute node were seriously falling apart. Unfortunately:</p>

<blockquote><p>The TripleO installer requires that all the overcloud nodes are reachable during the overcloud update.</p></blockquote>


<p>In the opposite case the update just stays hanging. It turned out that it was not possible to bring all the network interfaces on the second compute node up but eventually we were able to get at least the management interface working. This allowed us to re-run the overcloud update during which we fooled the TripleO installer to believe that the configuration of the problematic compute node was applied sucessfully. After exceeding the two-hour maintanance window by several hours we were finally done.</p>

<h2>Conclusion</h2>

<p>Here I&rsquo;d like to summarize our six-months long experience with the TripleO installer:</p>

<ol>
<li>In our experience, the configuration of the OpenStack cluster using only the TripleO installer is not flexible enough. As a workaround, we ended up writing a bunch of Ansible scripts.</li>
<li>The overcloud update can take a very long time to complete and it can fail because of random errors. Also during the update operation all the overcloud nodes have to be reachable by the TripleO installer. For this reason, I personally cannot imagine using TripleO to manage a cluster with more than one hundred nodes.</li>
<li>As we experienced, the TripleO installer can easily break a working OpenStack cluster. This is a big no-no for a production system.</li>
</ol>


<p>Overall, I think that the TripleO installer in the Mitaka version of OpenStack would need more work to become production ready. In the meantime, we&rsquo;re continuing with patching of what we have.</p>

<p>In the future, there are other projects that could replace the TripleO installer. I found the <a href="https://github.com/openstack/kolla-ansible">kolla-ansible</a> and <a href="https://github.com/openstack/kolla-kubernetes">kolla-kubernetes</a> rather promising.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Git - Getting the Timing Right]]></title>
    <link href="http://alesnosek.com/blog/2017/01/02/git-getting-the-timing-right/"/>
    <updated>2017-01-02T16:50:48-08:00</updated>
    <id>http://alesnosek.com/blog/2017/01/02/git-getting-the-timing-right</id>
    <content type="html"><![CDATA[<p>Do you work on a development team that is distributed across several time zones? Got confused by the dates that Git shows in the commit logs? In this blog post we&rsquo;re going to review some basics about how Git deals with time.</p>

<!-- more -->


<h2>Pay attention to the commit timestamps</h2>

<p>Let&rsquo;s assume that two developers work in the same Git repository. The first developer Prasad is located in Bangalore, India. His colleague Joe is located in San Diego, US. The Git log they created looks as follows:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>git log
</span><span class='line'>commit 0f7a87b545d23870aa3dfe2665e242fd1a807445
</span><span class='line'>Author: Joe Smith &lt;jsmith@sandiego.us&gt;
</span><span class='line'>Date:   Tue Dec <span class="m">6</span> 11:41:44 <span class="m">2016</span> -0800
</span><span class='line'>
</span><span class='line'>    Commit 3
</span><span class='line'>
</span><span class='line'>commit 62d95dfbc185ad60cd3ce2a4a3d02a12c1fb7dea
</span><span class='line'>Author: Prasad Gupta &lt;pgupta@bangalore.in&gt;
</span><span class='line'>Date:   Tue Dec <span class="m">6</span> 21:45:51 <span class="m">2016</span> +0530
</span><span class='line'>
</span><span class='line'>    Commit 2
</span><span class='line'>
</span><span class='line'>commit 106db71e39e3b25fb3fa3df4c55cc8f063e78ff5
</span><span class='line'>Author: Prasad Gupta &lt;pgupta@bangalore.in&gt;
</span><span class='line'>Date:   Tue Dec <span class="m">6</span> 21:45:00 <span class="m">2016</span> +0530
</span><span class='line'>
</span><span class='line'>    Commit 1
</span></code></pre></td></tr></table></div></figure>


<p>Git orders the commits based on the commit timestamps with the most recent commit shown on top. In the sample logs above you can see that all three commits were made on the same day Tuesday, December 6 2016. What might look a little bit odd is the order of the commits in the logs. <code>Commit 3</code> made at 11:41 should have actually appeared below the commits <code>Commit 1</code> and <code>Commit 2</code> made at 21:45, right? Wrong!</p>

<p>In the commit logs, Git displays the timestamp in the format <em>localtime + timezone offset</em>. When reading the timestamps it&rsquo;s important to take the timezone offset into account. Using the timezone offset you can convert the timestamp of the <code>Commit 2</code> and <code>Commit 3</code> into the UTC time in order to compare them. Because the <code>Commit 2</code> was made at 21:45 +0530 (= 16:15 UTC) and <code>Commit 3</code> was made at 11:41 -0800 (= 19:41 UTC) the <code>Commit 3</code> was created after the commits <code>Commit 1</code> and <code>Commit 2</code> and the chronological order displayed by Git is indeed correct.</p>

<h2>Check the time settings on the developer machines</h2>

<p>The timestamp recorded in the Git commit is based solely on the current time on the machine where the commit was created. Even if you have a corporate Git server where you push all your commits to you have to know that the Git server doesn&rsquo;t modify the timestamps in any way. You have to encourage your developers to have the time on their machines set correctly. This includes the correct local time as well as the time zone. On the Linux machines equipped with <a href="https://www.freedesktop.org/wiki/Software/systemd/">systemd</a> these time settings can be changed using the <code>timedatectl</code> command. Use <code>date</code> command to validate your settings:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>date
</span><span class='line'>Mon Jan  <span class="m">2</span> 20:33:04 PST 2017
</span></code></pre></td></tr></table></div></figure>


<p>The output shows my correct local time and the correct time zone (PST) as I&rsquo;m located on the west coast of the US.</p>

<h2>Author date and commit date</h2>

<p>In the aforementioned example with the <code>git log</code> command I simplified the situation a little bit. There are actually two different timestamps recorded by Git for each commit: the <em>author date</em> and the <em>commit date</em>. When the commit is created both the timestamps are set to the current time of the machine where the commit was made. The author date denotes the time when the commit was originally made and it never changes. The commit date is updated every time the commit is being modified for example when rebasing or cherry-picking.</p>

<p>By default <code>git log</code> orders the logs according to the commit date, however, the author date is actually displayed in the output. This can easily lead to confusion when there are commits present for which the commit date and the author date actually differ. The parameter <code>--author-date-order</code> can be used to order the commits based on the author timestamp:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>git log --author-date-order
</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Controlling a Multi-Service Application with systemd]]></title>
    <link href="http://alesnosek.com/blog/2016/12/04/controlling-a-multi-service-application-with-systemd/"/>
    <updated>2016-12-04T23:15:39-08:00</updated>
    <id>http://alesnosek.com/blog/2016/12/04/controlling-a-multi-service-application-with-systemd</id>
    <content type="html"><![CDATA[<p>Is your application delivered as a set of services running on top of Linux? Did you think about writing a custom controller service that would start your application services in the correct order and monitor their health? Please, stop thinking about it! In this blog post I would like to convince you that you can leverage the existing <em>systemd</em> service manager to control your application services to your greatest benefit.</p>

<!-- more -->


<p><a href="https://www.freedesktop.org/wiki/Software/systemd/">systemd</a> is the default service manager on all major Linux distributions. We&rsquo;re going to demonstrate how it can be used to control a custom multi-service application.</p>

<p><img class="right" src="http://alesnosek.com/images/posts/systemd-logo.png" width="200" height="300"></p>

<p>Our application consists of three services: Service 1, Service 2 and Service 3. The following set of requirements should be met when controlling the services using systemd:</p>

<ol>
<li>User can permanently enable/disable any of the services</li>
<li>User can start and stop the services as a group</li>
<li>User can start and stop each service independently</li>
<li>Service 2 should start before the Service 3 can start</li>
<li>Service 3 should be stopped before the Service 2 can be stopped</li>
<li>Services 1 and 2 should start in parallel to speed up the application startup</li>
<li>Components should be monitored and restarted in the case of failure</li>
</ol>


<p>Required service startup order:</p>

<p><img src="http://alesnosek.com/images/posts/systemd_startup_dependencies.svg" width="300" height="400" title="Service startup order" ></p>

<h2>Creating the systemd unit files</h2>

<p>Let&rsquo;s begin creating the systemd unit files. First, we&rsquo;ll define a pseudo-service called <code>app</code>. This service doesn&rsquo;t run any deamon. Instead, it will allow us to start/stop the three application services at once.</p>

<figure class='code'><figcaption><span>app.service </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='ini'><span class='line'><span class="k">[Unit]</span>
</span><span class='line'><span class="na">Description</span><span class="o">=</span><span class="s">Application</span>
</span><span class='line'>
</span><span class='line'><span class="k">[Service]</span>
</span><span class='line'><span class="c1"># The dummy program will exit</span>
</span><span class='line'><span class="na">Type</span><span class="o">=</span><span class="s">oneshot</span>
</span><span class='line'><span class="c1"># Execute a dummy program</span>
</span><span class='line'><span class="na">ExecStart</span><span class="o">=</span><span class="s">/bin/true</span>
</span><span class='line'><span class="c1"># This service shall be considered active after start</span>
</span><span class='line'><span class="na">RemainAfterExit</span><span class="o">=</span><span class="s">yes</span>
</span><span class='line'>
</span><span class='line'><span class="k">[Install]</span>
</span><span class='line'><span class="c1"># Components of this application should be started at boot time</span>
</span><span class='line'><span class="na">WantedBy</span><span class="o">=</span><span class="s">multi-user.target</span>
</span></code></pre></td></tr></table></div></figure>


<p>In the next step, we&rsquo;ll create systemd unit files for the three services that constitute our application. I included some explanatory comments in the first <code>app-component1</code> service definition. The definitions of the remaining two services <code>app-component2</code> and <code>app-component3</code> follow the same schema.</p>

<figure class='code'><figcaption><span>app-component1.service </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='ini'><span class='line'><span class="k">[Unit]</span>
</span><span class='line'><span class="na">Description</span><span class="o">=</span><span class="s">Application Component 1</span>
</span><span class='line'><span class="c1"># When systemd stops or restarts the app.service, the action is propagated to this unit</span>
</span><span class='line'><span class="na">PartOf</span><span class="o">=</span><span class="s">app.service</span>
</span><span class='line'><span class="c1"># Start this unit after the app.service start</span>
</span><span class='line'><span class="na">After</span><span class="o">=</span><span class="s">app.service</span>
</span><span class='line'>
</span><span class='line'><span class="k">[Service]</span>
</span><span class='line'><span class="c1"># Pretend that the component is running</span>
</span><span class='line'><span class="na">ExecStart</span><span class="o">=</span><span class="s">/bin/sleep infinity</span>
</span><span class='line'><span class="c1"># Restart the service on non-zero exit code when terminated by a signal other than SIGHUP, SIGINT, SIGTERM or SIGPIPE</span>
</span><span class='line'><span class="na">Restart</span><span class="o">=</span><span class="s">on-failure</span>
</span><span class='line'>
</span><span class='line'><span class="k">[Install]</span>
</span><span class='line'><span class="c1"># This unit should start when app.service is starting</span>
</span><span class='line'><span class="na">WantedBy</span><span class="o">=</span><span class="s">app.service</span>
</span></code></pre></td></tr></table></div></figure>


<p>The definition of the service <code>app-component2</code> resembles the definition of service <code>app-component1</code>:</p>

<figure class='code'><figcaption><span>app-component2.service </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='ini'><span class='line'><span class="k">[Unit]</span>
</span><span class='line'><span class="na">Description</span><span class="o">=</span><span class="s">Application Component 2</span>
</span><span class='line'><span class="na">PartOf</span><span class="o">=</span><span class="s">app.service</span>
</span><span class='line'><span class="na">After</span><span class="o">=</span><span class="s">app.service</span>
</span><span class='line'>
</span><span class='line'><span class="k">[Service]</span>
</span><span class='line'><span class="na">ExecStart</span><span class="o">=</span><span class="s">/bin/sleep infinity</span>
</span><span class='line'><span class="na">Restart</span><span class="o">=</span><span class="s">on-failure</span>
</span><span class='line'>
</span><span class='line'><span class="k">[Install]</span>
</span><span class='line'><span class="na">WantedBy</span><span class="o">=</span><span class="s">app.service</span>
</span></code></pre></td></tr></table></div></figure>


<p>We would like the service <code>app-component3</code> to start after the service <code>app-component2</code>. Systemd provides the directive <code>After</code> to configure the start ordering. Note that we don&rsquo;t use a <code>Wants</code> directive to create a dependency of <code>app-component3</code> on <code>app-component2</code>. This <code>Wants</code> dependency would instruct systemd to start the <code>app-component2</code> whenever the <code>app-component3</code> should be started. The <code>app-component2</code> would be started even in the case that it was disabled before. This is however not what we wanted as we require the user to be able to permanently disable any of the components. If <code>app-component2</code> is not enabled, systemd should just skip it when starting the application services.</p>

<figure class='code'><figcaption><span>app-component3.service </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='ini'><span class='line'><span class="k">[Unit]</span>
</span><span class='line'><span class="na">Description</span><span class="o">=</span><span class="s">Application Component 3</span>
</span><span class='line'><span class="na">PartOf</span><span class="o">=</span><span class="s">app.service</span>
</span><span class='line'><span class="na">After</span><span class="o">=</span><span class="s">app.service</span>
</span><span class='line'><span class="c1"># This unit should start after the app-component2 started</span>
</span><span class='line'><span class="na">After</span><span class="o">=</span><span class="s">app-component2.service</span>
</span><span class='line'>
</span><span class='line'><span class="k">[Service]</span>
</span><span class='line'><span class="na">ExecStart</span><span class="o">=</span><span class="s">/bin/sleep infinity</span>
</span><span class='line'><span class="na">Restart</span><span class="o">=</span><span class="s">on-failure</span>
</span><span class='line'>
</span><span class='line'><span class="k">[Install]</span>
</span><span class='line'><span class="na">WantedBy</span><span class="o">=</span><span class="s">app.service</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Adding the application services to systemd</h2>

<p>After we finished the creation of the four systemd unit files, we need to copy them to the systemd configuration directory:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>sudo cp app-component1.service app-component2.service app-component3.service app.service /etc/systemd/system/
</span></code></pre></td></tr></table></div></figure>


<p>Next, we have to ask systemd to reload its configuration:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>sudo systemctl daemon-reload
</span></code></pre></td></tr></table></div></figure>


<p>If everything went well, you should be able to see the new services in the unit file list:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>systemctl list-unit-files app*
</span><span class='line'>UNIT FILE              STATE
</span><span class='line'>app-component1.service disabled
</span><span class='line'>app-component2.service disabled
</span><span class='line'>app-component3.service disabled
</span><span class='line'>app.service            disabled
</span><span class='line'>
</span><span class='line'><span class="m">4</span> unit files listed.
</span></code></pre></td></tr></table></div></figure>


<h2>Testing the application services</h2>

<p>After all the hard work we&rsquo;re now ready to exercise our configuration. First, let&rsquo;s enable all the application services:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>sudo systemctl <span class="nb">enable </span>app app-component1 app-component2 app-component3
</span><span class='line'>Created symlink from /etc/systemd/system/multi-user.target.wants/app.service to /etc/systemd/system/app.service.
</span><span class='line'>Created symlink from /etc/systemd/system/app.service.wants/app-component1.service to /etc/systemd/system/app-component1.service.
</span><span class='line'>Created symlink from /etc/systemd/system/app.service.wants/app-component2.service to /etc/systemd/system/app-component2.service.
</span><span class='line'>Created symlink from /etc/systemd/system/app.service.wants/app-component3.service to /etc/systemd/system/app-component3.service.
</span></code></pre></td></tr></table></div></figure>


<p>To start all the services, you can type:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>sudo systemctl start app
</span></code></pre></td></tr></table></div></figure>


<p>The systemd status command should display all the services as <code>active</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>sudo systemctl status app*
</span></code></pre></td></tr></table></div></figure>


<p>Note that while the component services are marked as <code>running</code>, the pseudo-service <code>app</code> is showed as <code>exited</code>. This is an expected behaviour as the service <code>app</code> is of type <code>oneshot</code>. (I&rsquo;m not showing the output of the systemctl status command here).</p>

<p>Next, let&rsquo;s check the starting order of the services. Systemd logs its messages into the journal. Let&rsquo;s list the recent log entries with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>journalctl -e
</span></code></pre></td></tr></table></div></figure>


<p>In our sample output we can see that the service <code>app-component3</code> was indeed started after the service <code>app-component2</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Dec 05 20:34:57 localhost.localdomain systemd[1]: Starting Application...
</span><span class='line'>Dec 05 20:34:57 localhost.localdomain systemd[1]: Started Application.
</span><span class='line'>Dec 05 20:34:57 localhost.localdomain systemd[1]: Started Application Component 1.
</span><span class='line'>Dec 05 20:34:57 localhost.localdomain systemd[1]: Starting Application Component 1...
</span><span class='line'>Dec 05 20:34:57 localhost.localdomain systemd[1]: Started Application Component 2.
</span><span class='line'>Dec 05 20:34:57 localhost.localdomain systemd[1]: Starting Application Component 2...
</span><span class='line'>Dec 05 20:34:57 localhost.localdomain systemd[1]: Started Application Component 3.
</span><span class='line'>Dec 05 20:34:57 localhost.localdomain systemd[1]: Starting Application Component 3...</span></code></pre></td></tr></table></div></figure>


<p>Now we&rsquo;re going to check that we can disable any service independently of other services. Let&rsquo;s stop the service <code>app-component2</code> and disable it:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>sudo systemctl stop app-component2
</span><span class='line'><span class="nv">$ </span>sudo systemctl disable app-component2
</span><span class='line'>Removed symlink /etc/systemd/system/app.service.wants/app-component2.service.
</span></code></pre></td></tr></table></div></figure>


<p>When we stop and start our application the service <code>app-component2</code> will remain disabled.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>sudo systemctl stop app
</span><span class='line'><span class="nv">$ </span>sudo systemctl start app
</span></code></pre></td></tr></table></div></figure>


<p>Unfortunately, I realized that when using the <code>restart</code> command, systemd will enable the service <code>app-component2</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>sudo systemctl restart app
</span></code></pre></td></tr></table></div></figure>


<p>The behaviour of the systemd <code>stop</code> command followed by the <code>start</code> command is not consistent with the behaviour of the systemd <code>restart</code> command. As a workaround, you can use the systemd <code>mask</code> command to really disable the application service, for example:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>sudo systemctl mask app-component2
</span></code></pre></td></tr></table></div></figure>


<p>This way, the service <code>app-component2</code> remains disabled no matter what.</p>

<h2>Conclusion</h2>

<p>systemd provides a feature-rich service manager. Instead of implementing a home-grown solution you might want to think about using systemd to control your application services. Some of the benefits of opting for systemd are:</p>

<ol>
<li>systemd is a standard service manager any Linux administrator is familiar with</li>
<li>systemd is battle tested to the extent your proprietary solution probably cannot be</li>
<li>You can discover many other useful systemd features which you can employ right away</li>
</ol>


<p>Have fun!</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[RHCSA/RHCE Exam Experience]]></title>
    <link href="http://alesnosek.com/blog/2016/11/07/rhcsa-slash-rhce-exam-experience/"/>
    <updated>2016-11-07T23:45:38-08:00</updated>
    <id>http://alesnosek.com/blog/2016/11/07/rhcsa-slash-rhce-exam-experience</id>
    <content type="html"><![CDATA[<p>Today was the great day when I passed the RHCE certification exam. If you&rsquo;re thinking about getting a Linux certification or you&rsquo;re already working towards RHCSA/RHCE, this blog post is for you.</p>

<!-- more -->


<p>The RHCSA/RHCE certification is a part of the <a href="https://en.wikipedia.org/wiki/Red_Hat_Certification_Program">Red Hat Certification Program</a>. You have to pass two practical exams, RHCSA (Red Hat Certified System Administrator) and RHCE (Red Hat Certified Engineer).</p>

<h2>Modus operandi</h2>

<p>Both the RHCSA and RHCE exams are hands on. You will be presented with one or more RHEL7 virtual machines and a list of Linux configuration tasks. Your goal is to set up the virtual machines as described in the tasks. After your exam is finished, the Red Hat test automation will reboot the virtual machines and run the grading scripts that will compute your score. An hour or two after the exam end you will receive your exam results in the email.</p>

<p>The exam strictly tests your practical skills which makes it in my opinion very valuable. Several years ago I passed some of the exams from the Oracle Java certification path. The major part of the Java exams was based on the multiple choice questions. It was less fun for me to prepare for it and I think that a hands on exam would test the Java development skills much accurately.</p>

<p>If you&rsquo;re a hiring manager searching for the Linux talent, you should know that:</p>

<blockquote><p>Job candidates that passed the RHCSA/RHCE certification really demonstrated their Linux administration skills at some point.</p></blockquote>


<h2>Tips</h2>

<p>You can find a ton of tips for the RHCSA/RHCE exams on the Internet already. I&rsquo;d like to share some more advice here.</p>

<p>The RHEL7 virtual machines you will be working on have manual pages installed. Before taking the exam, make sure you know where to look for the useful configuration examples. For instance, when configuring the network interfaces you can refer to:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>man <span class="m">5</span> nmcli-examples
</span></code></pre></td></tr></table></div></figure>


<p>Full examples of commands to configure the firewall can be found at:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>man <span class="m">5</span> firewalld.richlanguage
</span></code></pre></td></tr></table></div></figure>


<p>When wrestling with the Apache server configuration you can leverage the Apache manual. After you installed the <code>httpd-manual</code> RPM package, you can browse the manual using the <em>elinks</em> web browser:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>yum install elinks httpd-manual
</span><span class='line'>elinks /usr/share/httpd/manual/index.html
</span></code></pre></td></tr></table></div></figure>


<p>If it is feasible for you, make yourself a favor and buy the <a href="https://www.redhat.com/en/services/training/learning-subscription">Red Hat Learning Subscription</a>. You will get access to the training materials that walk you through the tasks very similar to what you&rsquo;ll see on the real exam. It includes mock exams. And you&rsquo;ll be provided with the virtual machines that you can practice on over and over again until you&rsquo;re ready to take the test.</p>

<p>I found the Red Hat learning materials to be of a high quality. Besides the prep for the exam, I&rsquo;m using them to learn about further technologies from the Red Hat portfolio.</p>

<h2>Is the RHCSA/RHCE for me?</h2>

<p>In my opinion, the RHCSA/RHCE is one of the best Linux certifications available. If you mean it with Linux seriously - and it doesn&rsquo;t necessarily have to be the Red Hat flavor of Linux - go get certified!</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[5 Linux Commands You Didn't Know You Needed]]></title>
    <link href="http://alesnosek.com/blog/2016/11/01/5-linux-commands-you-didnt-know-you-needed/"/>
    <updated>2016-11-01T23:11:44-07:00</updated>
    <id>http://alesnosek.com/blog/2016/11/01/5-linux-commands-you-didnt-know-you-needed</id>
    <content type="html"><![CDATA[<p>When preparing for the RHCSA and RHCE exams, I found several useful commands I was not really aware of. In this blog post I&rsquo;ll share them with you.</p>

<!-- more -->


<h2>findmnt</h2>

<p>The <code>findmnt</code> command is part of the essential package <em>util-linux</em> and hence is available on pretty much all Linux systems. It can print all mounted filesystems in the tree-like format. I found the output of <code>findmnt</code> command more readable than the output provided by the more popular <code>mount</code> command. This is an example of how the filesystem mounts on a Ceph node look like:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>findmnt
</span><span class='line'>TARGET                           SOURCE     FSTYPE     OPTIONS
</span><span class='line'>/                                /dev/sda2  xfs        rw,relatime,seclabel,attr2,inode64,noquota
</span><span class='line'>/sys                           sysfs      sysfs      rw,nosuid,nodev,noexec,relatime,seclabel
</span><span class='line'> /sys/kernel/security         securityfs securityfs rw,nosuid,nodev,noexec,relatime
</span><span class='line'> /sys/fs/cgroup               tmpfs      tmpfs      ro,nosuid,nodev,noexec,seclabel,mode<span class="o">=</span>755
</span><span class='line'>  /sys/fs/cgroup/systemd     cgroup     cgroup     rw,nosuid,nodev,noexec,relatime,xattr,release_agent<span class="o">=</span>/usr/lib/systemd/systemd-cgroups-agent,name<span class="o">=</span>systemd
</span><span class='line'>  /sys/fs/cgroup/cpu,cpuacct cgroup     cgroup     rw,nosuid,nodev,noexec,relatime,cpuacct,cpu
</span><span class='line'>  /sys/fs/cgroup/perf_event  cgroup     cgroup     rw,nosuid,nodev,noexec,relatime,perf_event
</span><span class='line'>  /sys/fs/cgroup/devices     cgroup     cgroup     rw,nosuid,nodev,noexec,relatime,devices
</span><span class='line'>  /sys/fs/cgroup/blkio       cgroup     cgroup     rw,nosuid,nodev,noexec,relatime,blkio
</span><span class='line'>  /sys/fs/cgroup/cpuset      cgroup     cgroup     rw,nosuid,nodev,noexec,relatime,cpuset
</span><span class='line'>  /sys/fs/cgroup/hugetlb     cgroup     cgroup     rw,nosuid,nodev,noexec,relatime,hugetlb
</span><span class='line'>  /sys/fs/cgroup/net_cls     cgroup     cgroup     rw,nosuid,nodev,noexec,relatime,net_cls
</span><span class='line'>  /sys/fs/cgroup/memory      cgroup     cgroup     rw,nosuid,nodev,noexec,relatime,memory
</span><span class='line'>  /sys/fs/cgroup/freezer     cgroup     cgroup     rw,nosuid,nodev,noexec,relatime,freezer
</span><span class='line'> /sys/fs/pstore               pstore     pstore     rw,nosuid,nodev,noexec,relatime
</span><span class='line'> /sys/fs/selinux              selinuxfs  selinuxfs  rw,relatime
</span><span class='line'> /sys/kernel/debug            debugfs    debugfs    rw,relatime
</span><span class='line'> /sys/kernel/config           configfs   configfs   rw,relatime
</span><span class='line'>/proc                          proc       proc       rw,nosuid,nodev,noexec,relatime
</span><span class='line'> /proc/sys/fs/binfmt_misc     systemd-1  autofs     rw,relatime,fd<span class="o">=</span>26,pgrp<span class="o">=</span>1,timeout<span class="o">=</span>300,minproto<span class="o">=</span>5,maxproto<span class="o">=</span>5,direct
</span><span class='line'> /proc/fs/nfsd                nfsd       nfsd       rw,relatime
</span><span class='line'>/dev                           devtmpfs   devtmpfs   rw,nosuid,seclabel,size<span class="o">=</span>16307108k,nr_inodes<span class="o">=</span>4076777,mode<span class="o">=</span>755
</span><span class='line'> /dev/shm                     tmpfs      tmpfs      rw,nosuid,nodev,seclabel
</span><span class='line'> /dev/pts                     devpts     devpts     rw,nosuid,noexec,relatime,seclabel,gid<span class="o">=</span>5,mode<span class="o">=</span>620,ptmxmode<span class="o">=</span>000
</span><span class='line'> /dev/mqueue                  mqueue     mqueue     rw,relatime,seclabel
</span><span class='line'> /dev/hugepages               hugetlbfs  hugetlbfs  rw,relatime,seclabel
</span><span class='line'>/run                           tmpfs      tmpfs      rw,nosuid,nodev,seclabel,mode<span class="o">=</span>755
</span><span class='line'> /run/user/1002               tmpfs      tmpfs      rw,nosuid,nodev,relatime,seclabel,size<span class="o">=</span>3265340k,mode<span class="o">=</span>700,uid<span class="o">=</span>1002,gid<span class="o">=</span>1002
</span><span class='line'>/var/lib/nfs/rpc_pipefs        rpc_pipefs rpc_pipefs rw,relatime
</span><span class='line'>/var/lib/ceph/osd/ceph-1       /dev/sdb1  xfs        rw,noatime,seclabel,attr2,inode64,logbsize<span class="o">=</span>256k,sunit<span class="o">=</span>512,swidth<span class="o">=</span>512,noquota
</span><span class='line'>/var/lib/ceph/osd/ceph-3       /dev/sdc1  xfs        rw,noatime,seclabel,attr2,inode64,logbsize<span class="o">=</span>256k,sunit<span class="o">=</span>512,swidth<span class="o">=</span>512,noquota
</span><span class='line'>/var/lib/ceph/osd/ceph-10      /dev/sdg1  xfs        rw,noatime,seclabel,attr2,inode64,logbsize<span class="o">=</span>256k,sunit<span class="o">=</span>512,swidth<span class="o">=</span>512,noquota
</span><span class='line'>/var/lib/ceph/osd/ceph-9       /dev/sdf1  xfs        rw,noatime,seclabel,attr2,inode64,logbsize<span class="o">=</span>256k,sunit<span class="o">=</span>512,swidth<span class="o">=</span>512,noquota
</span><span class='line'>/var/lib/ceph/osd/ceph-4       /dev/sdd1  xfs        rw,noatime,seclabel,attr2,inode64,logbsize<span class="o">=</span>256k,sunit<span class="o">=</span>512,swidth<span class="o">=</span>512,noquota
</span><span class='line'>/var/lib/ceph/osd/ceph-7       /dev/sde1  xfs        rw,noatime,seclabel,attr2,inode64,logbsize<span class="o">=</span>256k,sunit<span class="o">=</span>512,swidth<span class="o">=</span>512,noquota
</span></code></pre></td></tr></table></div></figure>


<h2>ss</h2>

<p>The <code>ss</code> (soscket statistics) command is a replacement for the good old <code>netstat</code> command. It comes in the <em>iproute</em> package which is an essential part of all modern Linux distributions. I found <code>ss</code> command available on systems where the <code>netstat</code> command was missing. Here is a sample output of the <code>ss</code> command running on my Linux desktop:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>sudo ss -tlnp <span class="p">|</span> cat
</span><span class='line'>State      Recv-Q Send-Q Local Address:Port               Peer Address:Port
</span><span class='line'>LISTEN     <span class="m">0</span>      <span class="m">100</span>          *:15929                    *:*                   users:<span class="o">((</span><span class="s2">&quot;skype&quot;</span>,pid<span class="o">=</span>25943,fd<span class="o">=</span>49<span class="o">))</span>
</span><span class='line'>LISTEN     <span class="m">0</span>      <span class="m">50</span>           *:39964                    *:*                   users:<span class="o">((</span><span class="s2">&quot;java&quot;</span>,pid<span class="o">=</span>6015,fd<span class="o">=</span>146<span class="o">))</span>
</span><span class='line'>LISTEN     <span class="m">0</span>      <span class="m">5</span>      192.168.122.1:53                       *:*                   users:<span class="o">((</span><span class="s2">&quot;dnsmasq&quot;</span>,pid<span class="o">=</span>1625,fd<span class="o">=</span>6<span class="o">))</span>
</span><span class='line'>LISTEN     <span class="m">0</span>      <span class="m">128</span>          *:22                       *:*                   users:<span class="o">((</span><span class="s2">&quot;sshd&quot;</span>,pid<span class="o">=</span>1442,fd<span class="o">=</span>3<span class="o">))</span>
</span><span class='line'>LISTEN     <span class="m">0</span>      <span class="m">128</span>         :::22                      :::*                   users:<span class="o">((</span><span class="s2">&quot;sshd&quot;</span>,pid<span class="o">=</span>1442,fd<span class="o">=</span>4<span class="o">))</span>
</span></code></pre></td></tr></table></div></figure>


<p>I&rsquo;m switching from using the <code>netstat</code> command to <code>ss</code>. What about you?</p>

<h2>ip</h2>

<p>After years of using the <code>ifconfig</code> utility, it took me some effort to move to its modern replacement - the <code>ip</code> command. Recently, I discovered two useful features of the <code>ip</code> utility.</p>

<p>To obtain a detailed information about the packets transferred by individual network interfaces, use the <code>-s</code> (statistics) parameter. For example:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>ip -s link
</span><span class='line'>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <span class="m">65536</span> qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1
</span><span class='line'>    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
</span><span class='line'>    RX: bytes  packets  errors  dropped overrun mcast
</span><span class='line'>    <span class="m">1416492</span>    <span class="m">20158</span>    <span class="m">0</span>       <span class="m">0</span>       <span class="m">0</span>       0
</span><span class='line'>    TX: bytes  packets  errors  dropped carrier collsns
</span><span class='line'>    <span class="m">1416492</span>    <span class="m">20158</span>    <span class="m">0</span>       <span class="m">0</span>       <span class="m">0</span>       0
</span><span class='line'>2: enp0s31f6: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span class="m">1500</span> qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000
</span><span class='line'>    link/ether 18:66:da:21:33:87 brd ff:ff:ff:ff:ff:ff
</span><span class='line'>    RX: bytes  packets  errors  dropped overrun mcast
</span><span class='line'>    <span class="m">13776256661</span> <span class="m">26486602</span> <span class="m">0</span>       <span class="m">0</span>       <span class="m">0</span>       1661059
</span><span class='line'>    TX: bytes  packets  errors  dropped carrier collsns
</span><span class='line'>    <span class="m">2313484427</span> <span class="m">9811792</span>  <span class="m">0</span>       <span class="m">0</span>       <span class="m">0</span>       0
</span></code></pre></td></tr></table></div></figure>


<p>To figure out which network interface would be used to send a packet to the specified IP address:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>ip route get 192.168.0.1
</span><span class='line'>192.168.0.1 via 10.5.0.1 dev enp0s31f6  src 10.5.0.225
</span><span class='line'>    cache
</span></code></pre></td></tr></table></div></figure>


<p>When sending a packet to the target destination <code>192.168.0.1</code>, the kernel will route the packet via the <code>enp0s31f6</code> interface. The IP <code>10.5.0.1</code> is my default route.</p>

<h2>lscpu</h2>

<p>On modern machines the output of <code>cat /proc/cpuinfo</code> can be really long. To find out what CPU configuration a machine comes with I prefer to use the <code>lscpu</code> command. This is an example output of the <code>lscpu</code> command running on an OpenStack compute node:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>lscpu
</span><span class='line'>Architecture:          x86_64
</span><span class='line'>CPU op-mode<span class="o">(</span>s<span class="o">)</span>:        32-bit, 64-bit
</span><span class='line'>Byte Order:            Little Endian
</span><span class='line'>CPU<span class="o">(</span>s<span class="o">)</span>:                32
</span><span class='line'>On-line CPU<span class="o">(</span>s<span class="o">)</span> list:   0-31
</span><span class='line'>Thread<span class="o">(</span>s<span class="o">)</span> per core:    2
</span><span class='line'>Core<span class="o">(</span>s<span class="o">)</span> per socket:    8
</span><span class='line'>Socket<span class="o">(</span>s<span class="o">)</span>:             2
</span><span class='line'>NUMA node<span class="o">(</span>s<span class="o">)</span>:          2
</span><span class='line'>Vendor ID:             GenuineIntel
</span><span class='line'>CPU family:            6
</span><span class='line'>Model:                 63
</span><span class='line'>Model name:            Intel<span class="o">(</span>R<span class="o">)</span> Xeon<span class="o">(</span>R<span class="o">)</span> CPU E5-2640 v3 @ 2.60GHz
</span><span class='line'>Stepping:              2
</span><span class='line'>CPU MHz:               1200.062
</span><span class='line'>BogoMIPS:              5198.45
</span><span class='line'>Virtualization:        VT-x
</span><span class='line'>L1d cache:             32K
</span><span class='line'>L1i cache:             32K
</span><span class='line'>L2 cache:              256K
</span><span class='line'>L3 cache:              20480K
</span><span class='line'>NUMA node0 CPU<span class="o">(</span>s<span class="o">)</span>:     0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30
</span><span class='line'>NUMA node1 CPU<span class="o">(</span>s<span class="o">)</span>:     1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31
</span></code></pre></td></tr></table></div></figure>


<p>In the above output, the interesting lines are the <code>Socket(s)</code>, <code>Core(s) per socket</code>, <code>Thread(s) per core</code> and <code>CPU(s)</code>. In our case, we&rsquo;re looking at a machine with 2 physical CPUs (Sockets), each of them having 8 physical cores (Cores per socket). Each of the physical cores has 2 processing threads (Threads per core) aka logical CPUs due to the Hyper-Threading technology. In total, there are 32 logical CPUs available to the Linux scheduler to schedule a task on.</p>

<h2>lspci</h2>

<p>The last command in our overview is the <code>lspci</code> command. If you ever wondered which kernel driver is controlling your hardware device, you can find out with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>lspci -k
</span><span class='line'>
</span><span class='line'>...
</span><span class='line'>
</span><span class='line'>01:05.0 VGA compatible controller: Advanced Micro Devices, Inc. <span class="o">[</span>AMD/ATI<span class="o">]</span> RS880 <span class="o">[</span>Radeon HD 4250<span class="o">]</span>
</span><span class='line'>        Subsystem: ASUSTeK Computer Inc. M5A88-V EVO
</span><span class='line'>        Kernel driver in use: radeon
</span><span class='line'>01:05.1 Audio device: Advanced Micro Devices, Inc. <span class="o">[</span>AMD/ATI<span class="o">]</span> RS880 HDMI Audio <span class="o">[</span>Radeon HD <span class="m">4200</span> Series<span class="o">]</span>
</span><span class='line'>        Subsystem: ASUSTeK Computer Inc. M5A88-V EVO
</span><span class='line'>        Kernel driver in use: snd_hda_intel
</span><span class='line'>02:00.0 FireWire <span class="o">(</span>IEEE 1394<span class="o">)</span>: VIA Technologies, Inc. VT6315 Series Firewire Controller
</span><span class='line'>        Subsystem: ASUSTeK Computer Inc. M5A88-V EVO
</span><span class='line'>        Kernel driver in use: firewire_ohci
</span><span class='line'>02:00.1 IDE interface: VIA Technologies, Inc. VT6415 PATA IDE Host Controller <span class="o">(</span>rev a0<span class="o">)</span>
</span><span class='line'>        Subsystem: ASUSTeK Computer Inc. Motherboard
</span><span class='line'>        Kernel driver in use: pata_via
</span><span class='line'>03:00.0 USB controller: ASMedia Technology Inc. ASM1042 SuperSpeed USB Host Controller
</span><span class='line'>        Subsystem: ASUSTeK Computer Inc. P8B WS Motherboard
</span><span class='line'>        Kernel driver in use: xhci_hcd
</span><span class='line'>05:00.0 Network controller: Realtek Semiconductor Co., Ltd. RTL8192CE PCIe Wireless Network Adapter <span class="o">(</span>rev 01<span class="o">)</span>
</span><span class='line'>        Subsystem: Realtek Semiconductor Co., Ltd. RTL8192CE PCIe Wireless Network Adapter
</span><span class='line'>        Kernel driver in use: rtl8192ce
</span><span class='line'>06:00.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL8111/8168/8411 PCI Express Gigabit Ethernet Controller <span class="o">(</span>rev 06<span class="o">)</span>
</span><span class='line'>        Subsystem: ASUSTeK Computer Inc. P8P67 and other motherboards
</span><span class='line'>        Kernel driver in use: r8169
</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[First Impressions about ansible-container]]></title>
    <link href="http://alesnosek.com/blog/2016/09/12/first-impressions-about-ansible-container/"/>
    <updated>2016-09-12T20:06:49-07:00</updated>
    <id>http://alesnosek.com/blog/2016/09/12/first-impressions-about-ansible-container</id>
    <content type="html"><![CDATA[<p>At Red Hat summit I learned about the new project <a href="https://www.ansible.com/ansible-container">ansible-container</a>. I was very excited and looked forward to building Docker containers with Ansible instead of the Dockerfiles. The project seemed to came just on time as in our company we&rsquo;re starting to Dockerize our software products. How did <em>ansible-container</em> work out for us? Read on!</p>

<!-- more -->


<p>The ansible-container is a very young project. The first release 0.1.0 came out on July 28, 2016. The project&rsquo;s Git repository is hosted on <a href="https://github.com/ansible/ansible-container">GitHub</a>. Additionaly, there&rsquo;s a repository with examples of how to use ansible-container available <a href="https://github.com/ansible/ansible-container-examples">here</a>.</p>

<h2>How ansible-container works</h2>

<p>Here is how you build Docker containers with ansible-container. After crafting your Ansible playbook, you run the command <code>ansible-container build</code> which roughly executes these steps:</p>

<ol>
<li>Spin up a <em>builder container</em> and mount your Ansible playbook directory as a volume inside of this container.</li>
<li>Spin up a <em>base container</em> from the Docker image of your choice.</li>
<li>Run ansible-playbook inside of the <em>builder container</em>. Ansible will connect to the <em>base container</em> and configure it accordingly to the playbook.</li>
<li>Shutdown the <em>base container</em> and commit it as a new image.</li>
</ol>


<p>Besides building one Docker image at a time, it&rsquo;s rather straight forward to build a group of images at once. Under the hood, ansible-container leverages Docker Compose to start any number of containers to configure them with Ansible.</p>

<p>After your images are built, you can start them as containers on the local machine using the <code>ansible-container run</code> command. And finally, the <code>ansible-container shipit</code> command can help you to deploy your containers to Kubernetes or OpenShift.</p>

<h2>Evaluating ansible-container</h2>

<p>Software development in our company is organized in a way that can probably be found in many other places, too. We maintain base software libraries in a project called <em>platform</em>. On top of <em>platform</em>, many different product components are developed. We distribute our software in the form of RPM packages. As a first step on our way towards containers, we are going to install the RPMs on top of the Docker base images.</p>

<p>After spending a week with ansible-container, we decided that we are not going to adopt it at this time. The main reasons were:</p>

<ol>
<li><p>We would have to introduce another dependency to our build process. Our build machines would need to have Ansible, ansible-container and Docker Compose installed in order to build Docker containers. We prefer to keep our build dependencies to the minimum.</p></li>
<li><p>We need to build around 20 containers. During the container build we run a shell script to install an RPM package and to do a few more modifications. Our container build process is rather simple and Dockerfiles can just get the job done. For a more involved build process, Ansible would be preferable.</p></li>
<li><p>The ansible-container project shows its potential while it&rsquo;s still in its infancy. The builds with ansible-container take longer than the Dockerfile builds that leverage caching of image layers. The build specification file <code>container.yml</code> is not flexible enough for us. For example, it&rsquo;s not easy to build only a subset of the declared containers. Also it&rsquo;s not possible to build in one pass containers which inherit from each other.</p></li>
</ol>


<h2>Conclusion</h2>

<p>The ansible-container project is definitely very promising and we will keep our eye on it. For now, we&rsquo;re moving forward with generating and running Dockerfiles.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[What I learned from The Open Organization]]></title>
    <link href="http://alesnosek.com/blog/2016/07/04/what-i-learned-from-the-open-organization/"/>
    <updated>2016-07-04T21:04:22-07:00</updated>
    <id>http://alesnosek.com/blog/2016/07/04/what-i-learned-from-the-open-organization</id>
    <content type="html"><![CDATA[<p>Red Hat is one of the most successful software companies that makes billions of dollars selling open source software. The book <em>The Open Organization</em> written by Jim Whitehurst, CEO of Red Hat, provides a great insight into the company culture and how things get done at Red Hat. This blogpost lists some of the ideas I learned from this book.</p>

<!-- more -->


<h2>Motivating and inspiring</h2>

<ul>
<li><p>A company should have a purpose. The purpose is not a goal a company wants to achieve but the reason why it exists. For instance, Red Hat&rsquo;s mission statement says: <em>To be the catalyst in communities of customers, contributors, and partners creating better technology the open source way</em>. This statement enables Red Hat to attract the most talented people that share the belief that open source is fundamentally good for the world.</p></li>
<li><p>Its important to hire passionate people. When interviewing, ask the candidates what they are passionate about and think how it resonates with the purpose of your company.</p></li>
<li><p>Admitting mistakes builds your credibility and authority to lead.</p></li>
<li><p>As a manager, if you want to have engaged employees, you need to share context and knowledge. Be prepared to explain your decisions.</p></li>
<li><p>As an employee, if your manager is not providing the appropriate context, ask for it. Most bosses are happy to share. They just havent yet figured out that it belongs to what they should be doing.</p></li>
</ul>


<h2>Getting things done</h2>

<ul>
<li><p>The Open organization embraces meritocracy where the leaders are chosen based on their contributions to the project, not based on their titles or longevity within the company.</p></li>
<li><p>If your intention is just to stick your nose into every little thing so you can be front and center, people see it. At Red Hat, one of the greatest insults to your ego comes when you put something on one of the internal discussion threads and receive nothing back - neither positive nor negative. That means the team is likely ignoring you, which means youve failed in some way.</p></li>
<li><p>In the discussions, people argue and criticize not because they are trying to be difficult, but because they are so passionate about the subject. A debate involving multiple views from different people leads to better decisions made than a meeting where the goal for everyone is to be nice to each other, and every idea is treated as a good one.</p></li>
</ul>


<h2>Setting direction</h2>

<ul>
<li><p>The decisions that came from top down are typically harder to turn into real actions. The more open you make your decision-making process, the easier is to execute the decisions you made.</p></li>
<li><p>Encourage your team to bring up ideas and proposals. Discuss the ideas, regardless of whether you move forward with them.</p></li>
</ul>

]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Red Hat Summit 2016]]></title>
    <link href="http://alesnosek.com/blog/2016/07/04/red-hat-summit-2016/"/>
    <updated>2016-07-04T20:26:12-07:00</updated>
    <id>http://alesnosek.com/blog/2016/07/04/red-hat-summit-2016</id>
    <content type="html"><![CDATA[<p>I had the great opportunity to visit the Red Hat Summit 2016. Enjoy the photos attached.</p>

<!-- more -->


<p>The Red Hat summit was hosted at the Moscone Center in San Francisco in June 27-30, 2016. I greatly enjoyed the technical presentations by Red Hatters as well as the opportunity to discuss the OpenShift and Red Hat Atomic products directly with the lead engineers. At my company, we&rsquo;re looking at OpenShift+Atomic as the possible next platform to base our product upon. The frank opinions provided by the Red Hat engineers were very useful for our research. Next, take a look at the pics below.</p>

<p>General sessions and the Red Hat party at the end of the summit took place at Moscone Center North:</p>

<p><img class="center" src="http://alesnosek.com/images/posts/redhatsummit2016/20160629_135631.jpg">
<img class="center" src="http://alesnosek.com/images/posts/redhatsummit2016/20160628_134625.jpg"></p>

<p>Breakout sessions were hosted at the Moscone Center West:</p>

<p><img class="center" src="http://alesnosek.com/images/posts/redhatsummit2016/20160630_152426.jpg">
<img class="center" src="http://alesnosek.com/images/posts/redhatsummit2016/20160629_151110.jpg"></p>

<p>The big Red Hat logo at the entrance hall. The red and black colors really catch your eyes:</p>

<p><img class="center" src="http://alesnosek.com/images/posts/redhatsummit2016/20160628_195533.jpg"></p>

<p>Jim Whitehurst, president and CEO of Red Hat, and myself. Check out my <a href="http://alesnosek.com/blog/2016/07/04/what-i-learned-from-the-open-organization">next post</a> to find out, what I learned from the book I&rsquo;m holding in the picture.</p>

<p><img class="center" src="http://alesnosek.com/images/posts/redhatsummit2016/20160629_132338.jpg"></p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Deploying Kubernetes on OpenStack using Heat]]></title>
    <link href="http://alesnosek.com/blog/2016/06/26/deploying-kubernetes-on-openstack-using-heat/"/>
    <updated>2016-06-26T08:28:11-07:00</updated>
    <id>http://alesnosek.com/blog/2016/06/26/deploying-kubernetes-on-openstack-using-heat</id>
    <content type="html"><![CDATA[<p>Want to install Kubernetes on top of OpenStack? There are <a href="http://kubernetes.io/docs/getting-started-guides/">many ways</a> how to install a Kubernetes cluster. The upcoming Kubernetes 1.3 release comes with yet another method called <a href="http://kubernetes.io/docs/getting-started-guides/openstack-heat/">OpenStack Heat</a>. In this article, we&rsquo;re going to explore this deployment method when creating a minimum Kubernetes cluster on top of OpenStack.</p>

<!-- more -->


<p>In this tutorial, there are three OpenStack virtual machines involved. The first machine, called the <em>Kubernetes installer</em> machine, is created manually and is used for compiling Kubernetes from source and running the Kubernetes installer. The other two OpenStack machines, <em>Kubernetes master</em> and <em>Kubernetes node</em>, are created during the installation process.</p>

<p>The Kubernetes installer machine and both of the Kubernetes machines run on the CentOS-7-x86_64-GenericCloud-1605 image. You can download this image from the <a href="http://cloud.centos.org/centos/7/images/">CentOS image repository</a>. After I uploaded the CentOS 7 image into OpenStack, it has been assigned ID <code>17e4e783-321c-48c1-9308-6f99d67c5fa6</code> for me.</p>

<h2>Building Kubernetes from source</h2>

<p>First off, let&rsquo;s spin up a Kubernetes installer machine in OpenStack. I recommend using the <code>m1.large</code> flavor that comes with 8 GB of RAM. The compilation of Kubernetes is rather memory intensive.</p>

<p>To ensure consistent and reproducible builds, a Docker container is created at the beginning of the build process and the build proceeds within the container. So, let&rsquo;s quickly setup Docker on our build machine:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo yum install docker
</span></code></pre></td></tr></table></div></figure>


<p>Configure the Docker service to start on boot and then start it:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo systemctl <span class="nb">enable </span>docker
</span><span class='line'>sudo systemctl start docker
</span></code></pre></td></tr></table></div></figure>


<p>The Kubernetes build scripts expect that the <code>docker</code> command can successfully contact the Docker daemon. In the default CentOS configuration, the <code>sudo docker</code> is required in order to connect to the <code>/var/run/docker.sock</code> socket which is owned by the user root. To overcome the permission problem, let&rsquo;s create a wrapper script that will invoke the <code>docker</code> command using <code>sudo</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>mkdir bin
</span><span class='line'><span class="nb">echo</span> -e <span class="s1">&#39;#!/bin/bash\nexec sudo /usr/bin/docker &quot;$@&quot;&#39;</span> &gt; bin/docker
</span><span class='line'>chmod <span class="m">755</span> bin/docker
</span><span class='line'><span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span>~/bin:<span class="nv">$PATH</span>
</span></code></pre></td></tr></table></div></figure>


<p>You can test your changes with the <code>docker info</code> command which should work now.</p>

<p>Kubernetes is written in the Go language and its source code is stored in a Git repository. So, let&rsquo;s install the Go language environment and Git:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo yum install golang git
</span></code></pre></td></tr></table></div></figure>


<p>Next we&rsquo;ll clone the Kubernetes Git repository and start the build. The <code>quick-release</code> make target creates a build for the amd64 architecture only and doesn&rsquo;t run any tests.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>git clone https://github.com/kubernetes/kubernetes.git
</span><span class='line'><span class="nb">cd </span>kubernetes
</span><span class='line'>make quick-release
</span></code></pre></td></tr></table></div></figure>


<p>After about 15 minutes when the build was successful, you&rsquo;ll find the distribution tarballs <code>kubernetes.tar.gz</code> and <code>kubernetes-salt.tar.gz</code> in the <code>_output/release-tars</code> directory.</p>

<h2>Setting up the OpenStack CLI tools</h2>

<p>The Kubernetes installer uses the OpenStack CLI tools to talk to OpenStack in order to create a Kubernetes cluster. Before you can install the OpenStack CLI tools on CentOS 7, you have to enable the OpenStack Mitaka RPM repository:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo yum install centos-release-openstack-mitaka
</span></code></pre></td></tr></table></div></figure>


<p>Install the OpenStack CLI tools that are used by the Kubernetes installer when creating a cluster with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo yum install python-openstackclient python-swiftclient python-glanceclient python-novaclient python-heatclient
</span></code></pre></td></tr></table></div></figure>


<p>Next, you have to obtain your OpenStack <code>openrc.sh</code> file and source it into your environment:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>. openrc.sh
</span></code></pre></td></tr></table></div></figure>


<p>You should be able to talk to OpenStack now. For example, check if you can list the available OpenStack networks with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>openstack network list
</span></code></pre></td></tr></table></div></figure>


<h2>Configuring the Kubernetes installer</h2>

<p>In this section, we&rsquo;re going to more or less follow the instructions found in the chapter <a href="http://kubernetes.io/docs/getting-started-guides/openstack-heat/">OpenStack Heat</a> of the Kubernetes documentation.</p>

<p>When deploying the Kubernetes cluster, the installer executes the following steps that you can find in <code>cluster/openstack-heat/util.sh</code>:</p>

<ul>
<li>Upload the distribution tarballs <code>kubernetes.tar.gz</code> and <code>kubernetes-salt.tar.gz</code> into the <code>kubernetes</code> container in Swift</li>
<li>Upload the virtual machine image for the Kubernetes VMs into Glance</li>
<li>Add the user&rsquo;s keypair into Nova</li>
<li>Run a Heat script in order to create the Kubernetes VMs and put them on a newly created private network. Create a router connecting the private network with an external network.</li>
<li>At the first boot, the Kubernetes VMs download the distribution tarballs from Swift and install the Kubernetes software using Salt</li>
</ul>


<p>Let&rsquo;s create an <code>openstack-heat.sh</code> file with the configuration values for the Kubernetes installer:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nb">export </span><span class="nv">KUBERNETES_PROVIDER</span><span class="o">=</span>openstack-heat
</span><span class='line'><span class="nb">export </span><span class="nv">STACK_NAME</span><span class="o">=</span>kubernetes
</span><span class='line'><span class="nb">export </span><span class="nv">KUBERNETES_KEYPAIR_NAME</span><span class="o">=</span>mykeypair
</span><span class='line'><span class="nb">export </span><span class="nv">NUMBER_OF_MINIONS</span><span class="o">=</span>1
</span><span class='line'><span class="nb">export </span><span class="nv">MAX_NUMBER_OF_MINIONS</span><span class="o">=</span>1
</span><span class='line'><span class="nb">export </span><span class="nv">EXTERNAL_NETWORK</span><span class="o">=</span>gateway
</span><span class='line'><span class="nb">export </span><span class="nv">CREATE_IMAGE</span><span class="o">=</span><span class="nb">false</span>
</span><span class='line'><span class="nb">export </span><span class="nv">DOWNLOAD_IMAGE</span><span class="o">=</span><span class="nb">false</span>
</span><span class='line'><span class="nb">export </span><span class="nv">IMAGE_ID</span><span class="o">=</span>17e4e783-321c-48c1-9308-6f99d67c5fa6
</span><span class='line'><span class="nb">export </span><span class="nv">DNS_SERVER</span><span class="o">=</span>10.0.0.10
</span><span class='line'><span class="nb">export </span><span class="nv">SWIFT_SERVER_URL</span><span class="o">=</span>https://openstack.localdomain:13808/swift/v1
</span></code></pre></td></tr></table></div></figure>


<p>The above configuration will create exactly one Kubernetes master and one Kubernetes node. It will inject the keypair called <code>mykeypair</code> into both of them. Note that you have to ensure that the keypair <code>mykeypair</code> exists in Nova before proceeding. You probably want to change the name of the external network to a network available in your OpenStack. We&rsquo;re going to use the same CentOS 7 image for both of our Kubernetes VMs. This CentOS image has already been uploaded into OpenStack and in my case it was assigned ID <code>17e4e783-321c-48c1-9308-6f99d67c5fa6</code>. You also want to change the IP address of the DNS server to something that suits your environment. The Swift server URL is the public endpoint of your Swift server that you can obtain from the output of the command <code>openstack catalog show object-store</code>.</p>

<p>When your configuration is ready, you can source it into your environment:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>. openstack-heat.sh
</span></code></pre></td></tr></table></div></figure>


<p>Next, in my environment I had a problem where the IP range of the private network created by Kubernetes collided with the existing corporate network in my company. I had to directly edit the file <code>cluster/openstack-heat/kubernetes-heat/kubecluster.yaml</code> to change the <code>10.0.0.0/24</code> CIDR to something like <code>10.123.0.0/24</code>. If you don&rsquo;t have this problem you can safely use the default settings.</p>

<p>The Kubernetes cluster can leverage the underlying OpenStack cloud to attach existing Cinder volumes to the Kubernetes pods and to create external loadbalancers. For this to work, Kubernetes has to know how to connect to OpenStack APIs. With regard to the external loadbalancers, we also need to tell Kubernetes what Neutron subnet the loadbalancer&rsquo;s VIP should be placed on.</p>

<p>The OpenStack configuration can be found in the <em>cloud-config</em> script <code>cluster/openstack-heat/kubernetes-heat/fragments/configure-salt.yaml</code>. You can see that this script will create a configuration file <code>/srv/kubernetes/openstack.conf</code> on the Kubernetes machine which contains the OpenStack settings. In my case, I changed the original block:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[Global]
</span><span class='line'>auth-url=$OS_AUTH_URL
</span><span class='line'>username=$OS_USERNAME
</span><span class='line'>password=$OS_PASSWORD
</span><span class='line'>region=$OS_REGION_NAME
</span><span class='line'>tenant-id=$OS_TENANT_ID</span></code></pre></td></tr></table></div></figure>


<p>to read:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[Global]
</span><span class='line'>auth-url=$OS_AUTH_URL
</span><span class='line'>username=$OS_USERNAME
</span><span class='line'>password=$OS_PASSWORD
</span><span class='line'>region=$OS_REGION_NAME
</span><span class='line'>tenant-id=$OS_TENANT_ID
</span><span class='line'>domain-name=MyDomain # Keystone V3 domain
</span><span class='line'>[LoadBalancer]
</span><span class='line'>lb-version=v1
</span><span class='line'>subnet-id=73f8eb91-90cf-42f4-85d0-dcff44077313</span></code></pre></td></tr></table></div></figure>


<p>Besides adding the <code>LoadBalancer</code> section, I also appended the <code>domain-name</code> option to the end of the <code>Global</code> section, as in my OpenStack environment I want to authenticate against a non-default Keystone V3 domain.</p>

<h2>Installing the Kubernetes cluster</h2>

<p>After you&rsquo;ve sourced both the <code>openrc.sh</code> and <code>openstack-heat.sh</code> environment settings, you can kick off the installation of the Kubernetes cluster with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>./cluster/kube-up.sh
</span></code></pre></td></tr></table></div></figure>


<p>After about 25 minutes, you should have a Kubernetes cluster up and running. You can check the status of the Kubernetes pods with the command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>./cluster/kubectl.sh get pods --namespace kube-system
</span></code></pre></td></tr></table></div></figure>


<p>All pods should be running. The network topology of the Kubernetes cluster as displayed by Horizon:</p>

<p><img class="center" src="http://alesnosek.com/images/posts/kube.png"></p>

<h2>Accessing the Kubernetes cluster</h2>

<p><strong> Update 9/5/2016 </strong></p>

<p>At first, we will copy the <code>kubectl</code> client binary from the Kubernetes installer machine onto the remote host from where we are going to access our Kubernetes cluster:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>scp ./_output/release-stage/client/linux-amd64/kubernetes/client/bin/kubectl user@remote.host.com:
</span></code></pre></td></tr></table></div></figure>


<p>Remember to replace the <code>remote.host.com</code> with the name of your remote machine.</p>

<p>Next, we&rsquo;re going to start a kubectl proxy to allow access to Kubernetes APIs and the web UI from the remote host. The proxy can be brought up directly on the Kubernetes installer machine using the command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>./cluster/kubectl.sh proxy --address<span class="o">=</span>0.0.0.0 --port<span class="o">=</span><span class="m">8080</span> --accept-hosts<span class="o">=</span>.*
</span></code></pre></td></tr></table></div></figure>


<p>The proxy listens on port 8080 on all network interfaces and accepts connections from remote hosts with any IP address. This configuration is very unsecure but is good enough for our test environment. If your Kubernetes installer machine runs on the cloud, you might want to modify the security group rules to provide access to port 8080.</p>

<p>Now, we can access the Kubernetes APIs from the remote machine using the command:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>./kubectl --server http://kubernetes.installer.com:8080 cluster-info
</span></code></pre></td></tr></table></div></figure>


<p>The web UI of your Kubernetes cluster should be available at the URL:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>http://kubernetes.installer.com:8080/ui</span></code></pre></td></tr></table></div></figure>


<p>Note that you want to replace the <code>kubernetes.installer.com</code> with the name of your Kubernetes installer machine. When accessing the Kubernetes installer machine on port 8080, the request will be forwarded to your Kubernetes master node.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Test-Driving OpenStack Manila]]></title>
    <link href="http://alesnosek.com/blog/2016/05/22/test-driving-openstack-manila/"/>
    <updated>2016-05-22T16:59:30-07:00</updated>
    <id>http://alesnosek.com/blog/2016/05/22/test-driving-openstack-manila</id>
    <content type="html"><![CDATA[<p>Do you need to provision an NFS share for your Hadoop cluster? And what about creating a CIFS share to make your files accesible to the Windows clients? Manila is a provisioning and management service for shared file systems within OpenStack. Let&rsquo;s test-drive it in this blogpost.</p>

<!-- more -->


<p>In this introductory article, we&rsquo;re going to allocate a volume in Cinder and provide that volume as an NFS share to our Nova instances. For this, I&rsquo;m using the OpenStack Mitaka installed via TripleO on RHEL7. The Manila version included in the Mitaka release is version 2.0.</p>

<p>After installing Manila, the following Manila services are running on the controller nodes:</p>

<ul>
<li><em>openstack-manila-api</em> exposes REST APIs that the Manila client talks to.</li>
<li><em>openstack-manila-scheduler</em> makes provisioning decisions when creating a new share.</li>
<li><em>openstack-manila-share</em> comes with a host of drivers to talk to the storage systems.</li>
</ul>


<h2>Configuring the generic share driver</h2>

<p>In order for Manila to allocate shares on Cinder volumes, we&rsquo;ll have to configure Manila to use the <em>generic</em> share driver. For that we&rsquo;ll add a new Manila backend <code>generic_backend</code> into <code>/etc/manila/manila.conf</code>:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='ini'><span class='line'><span class="k">[DEFAULT]</span>
</span><span class='line'><span class="na">enabled_share_backends</span> <span class="o">=</span> <span class="s">generic_backend</span>
</span><span class='line'><span class="na">default_share_type</span> <span class="o">=</span> <span class="s">generic</span>
</span><span class='line'><span class="k">[generic_backend]</span>
</span><span class='line'><span class="na">share_driver</span> <span class="o">=</span> <span class="s">manila.share.drivers.generic.GenericShareDriver</span>
</span><span class='line'><span class="na">share_backend_name</span> <span class="o">=</span> <span class="s">generic_backend</span>
</span><span class='line'><span class="na">service_instance_name_template</span> <span class="o">=</span> <span class="s">manila_service_instance_%s</span>
</span><span class='line'><span class="na">service_image_name</span> <span class="o">=</span> <span class="s">manila-service-image-master</span>
</span><span class='line'><span class="na">driver_handles_share_servers</span> <span class="o">=</span> <span class="s">True</span>
</span><span class='line'><span class="na">service_instance_flavor_id</span> <span class="o">=</span> <span class="s">103</span>
</span><span class='line'><span class="na">connect_share_server_to_tenant_network</span> <span class="o">=</span> <span class="s">True</span>
</span><span class='line'><span class="na">service_instance_user</span> <span class="o">=</span> <span class="s">manila</span>
</span><span class='line'><span class="na">path_to_public_key</span> <span class="o">=</span> <span class="s">/etc/manila/id_rsa.pub</span>
</span><span class='line'><span class="na">path_to_private_key</span> <span class="o">=</span> <span class="s">/etc/manila/id_rsa</span>
</span><span class='line'><span class="na">manila_service_keypair_name</span> <span class="o">=</span> <span class="s">manila-service</span>
</span></code></pre></td></tr></table></div></figure>


<p>Before explaining the configuration settings, I&rsquo;ll briefly describe how the <em>generic</em> driver actually works. Behind the scenes, the generic driver creates a so called <em>service instance</em>. The service instance is a Nova instance owned by the Manila service. It&rsquo;s not even visible to the tenant users. Manila allocates a Cinder volume and asks Nova to attach that volume to the service instance. Afterwards, Manila connects to the service instance using SSH in order to create the filesytem on the attached Cinder volume and mount it and export that as a NFS/CIFS share to the tenant instances.</p>

<p>The service instance can be created by the OpenStack administrator or we can configure Manila to create the service instance by itself (option <code>driver_handles_share_servers = True</code>).</p>

<p>The service instance will be created from the image that we have to upload into Glance beforehand. I downloaded an existing Manila service image from <a href="http://tarballs.openstack.org/manila-image-elements/images/manila-service-image-master.qcow2">here</a>. This image is based on Ubuntu 14.04.4 LTS and includes the <code>manila</code> user account and the NFS and Samba server software packages. I uploaded this image into Glance under the name <code>manila-service-image-master</code>.</p>

<p>Next I&rsquo;ve chosen the size of the machine used for the service instance with <code>service_instance_flavor_id = 103</code>.</p>

<p>The service instance is connected to two networks. The first network is called a <em>service network</em> and is created by Manila before booting up the service instance. Manila uses this network for the SSH access to the service instance. The second network is a <em>share network</em>. The NFS server managed by Manila is accessible on this network. In our case, because we have configured <code>connect_share_server_to_tenant_network = True</code>, the share network will directly map to one of our tenant networks.</p>

<p>Finally, we have to generate a public/private key pair and tell Manila about it using the options <code>path_to_public_key</code> and <code>path_to_private_key</code>. Manila will upload this keypair into Nova under the name <code>manila-service</code>. When creating the service instance, Nova injects the public key into the instance and so allows Manila the SSH access.</p>

<p>In order to make our generic backend available to the Manila users, we&rsquo;re going to define a <code>generic</code> share type next.</p>

<h2>Defining a share type</h2>

<p>The <em>share type</em> has a similar purpose as the <em>volume type</em> in Cinder. It defines the backend used for the share creation. If there are multiple share backends available, an OpenStack administrator can define a separate share type for each of them. When creating a new share, the user can choose which share type to allocate the storage from.</p>

<p>To create a <code>generic</code> share type that maps to our <code>generic</code> backend you can run the following commands as an OpenStack administrator:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>manila <span class="nb">type</span>-create generic True
</span><span class='line'>manila <span class="nb">type</span>-key generic <span class="nb">set </span><span class="nv">share_backend_name</span><span class="o">=</span>generic_backend
</span></code></pre></td></tr></table></div></figure>


<h2>Creating a share and mounting it</h2>

<p>Finally, we&rsquo;re done with all the configuration and can start enjoying our share service. All the following commands are run as an ordinary tenant user.</p>

<p>At first, we&rsquo;d like to create a share network and map it to one of our tenant networks:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>manila share-network-create --neutron-net-id 4f179a8c-7068-4f0b-9be4-9cb11451b401 --neutron-subnet-id c7d753b0-039b-4f8c-9e0f-012651ff4ada --name management
</span></code></pre></td></tr></table></div></figure>


<p>Now we can create our first NFS share called <code>myshare</code> with the size 1 GB:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>manila create --name myshare --share-network management NFS 1
</span></code></pre></td></tr></table></div></figure>


<p>Creating the first share on a given tenant network takes longer as Manila has to spin up a new service instance in the background.</p>

<p>Eventually, the status of the share turns into <code>available</code> which means that the share is ready. The <code>manila show myshare</code> command will display the location from where we can mount the share. In our case, it is <code>10.13.243.173:/shares/share-b87367aa-3ef3-4282-a6b5-e45cab991b6c</code>. Before we can mount the share we have to allow access to it by modifying the access list:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>manila access-allow --access_level rw myshare ip 10.13.244.12
</span></code></pre></td></tr></table></div></figure>


<p>The above command provides an instance having the IP address 10.13.244.12 with a read-write access to the share. Note that the IP addresses 10.13.243.173 and 10.13.244.12 belong to the same network. Finally, we can SSH into the instance and mount the share with:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo mount -t nfs 10.13.243.173:/shares/share-b87367aa-3ef3-4282-a6b5-e45cab991b6c /mnt
</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[TripleO Installer - the Good, the Bad and the Ugly]]></title>
    <link href="http://alesnosek.com/blog/2016/03/27/tripleo-installer-the-good/"/>
    <updated>2016-03-27T19:39:15-07:00</updated>
    <id>http://alesnosek.com/blog/2016/03/27/tripleo-installer-the-good</id>
    <content type="html"><![CDATA[<p><a href="https://wiki.openstack.org/wiki/TripleO">TripleO</a> is an OpenStack deployment and management tool I&rsquo;ve been using since the Kilo release of OpenStack. It does its job pretty well, however not everything is perfect. My experience presented in this article applies more or less to the Red Hat&rsquo;s OpenStack director too, as the Red Hat OpenStack director is a downstream version of TripleO.</p>

<!-- more -->


<h2>The good things about TripleO</h2>

<h3>TripleO is a great idea</h3>

<p>TripleO, aka OpenStack-on-OpenStack, installs OpenStack cluster using OpenStack. At first, a minimum one-node OpenStack installation is created which is in turn used to provision a much bigger workload OpenStack cluster. I find this TripleO idea amazing. If OpenStack is the best way to manage your infrastructure, then why use something else to install it? As an administrator I would prefer to provision my OpenStack nodes with Ironic before introducing yet another tool like <a href="http://cobbler.github.io/">Cobbler</a> to do the same job. Needless to say that as the Ironic and Heat components improve, so improves the OpenStack installation experience.</p>

<p>One could argue that using the OpenStack to form an installer comes with a ton of complexity when installing the installer itself. In my experience, however, the installation of the undercloud OpenStack using the provided Puppet scripts doesn&rsquo;t impose any problem.</p>

<h3>TripleO has a vibrant community</h3>

<p>TripleO is used to continuously deploy and test the OpenStack cloud during its development. The RDO project adopted TripleO as their OpenStack installation tool. Red Hat derives their OpenStack director installer from the RDO project. A large community of TripleO users is a great plus.</p>

<h2>The bad things about TripleO</h2>

<h3>Configuration flexibility</h3>

<p>TripleO installer consists of a bunch of Heat templates to orchestrate the overcloud image provisioning and a number of Puppet and shell scripts for the following configuration of the overcloud nodes. These templates and scripts are heavily developed from release to release as the new TripleO features come in. To avoid the upgrade headaches, you should not modify the TripleO templates and scripts directly. Instead, TripleO provides extension points (via extra config) where you can put your customizations. This didn&rsquo;t work for me. My goal was to deploy an Ironic service in the overcloud OpenStack. For that to work, I needed to provision an additional undercloud network including a VIP for the load balancer. This was not possible without patching the Heat templates and Puppet scripts. I dread the day when I&rsquo;ll have to port these patches to the next TripleO release.</p>

<p>Furthermore, the current way to modify OpenStack configuration properties is less straight forward. To configure a property, I have to first grep through the Puppet scripts to find out whether the desired property is managed by Puppet or not. Afterwards, I grep through the TripleO Heat templates to find out whether TripleO provides a direct template parameter to set the Puppet variable or not. Afterwards, I can either pass the parameter to the TripleO template or I set the Puppet variable in the extra config section or I&rsquo;m on my own.</p>

<blockquote><p>I&#8217;d like to be able to easily modify any property in any configuration file on any OpenStack node.</p></blockquote>


<p>OpenStack comes with tons of configuration properties and I think it would be great to have a more straight forward way to configure them.</p>

<h3>Deployment control</h3>

<p>TripleO uses Heat to deploy and configure the overcloud OpenStack. Heat orchestrates the infrastracture based on the description provided by the user in the Heat templates. In the Heat templates, we tell Heat what our deployment should look like, but we have no control over the steps Heat will take to get to the desired state. I find this lack of control rather problematic.</p>

<blockquote><p>A fine-grained deployment control would be desirable.</p></blockquote>


<p>Let&rsquo;s say I have an overcloud consisting of 100 nodes. After changing the configuration in my Heat templates, I can only re-run the entire Heat configuration process and hope that I won&rsquo;t end up with a broken cloud. Instead, I&rsquo;d like to apply the configuration changes to a couple of nodes to make sure that everything works before I continue with the rest of the cloud. The ability to apply only part of the configuration would be useful as well.</p>

<h2>The ugly experience with TripleO</h2>

<p>I&rsquo;d like to share one scary experience I had with the TripleO installer. While using TripleO for a couple of months, I have to say that this was the only serious problem I&rsquo;ve encountered.</p>

<p>One day I uploaded an updated node image into the undercloud OpenStack. I was about to create new nodes in the overcloud cluster and wanted to have them provisioned with this new image. After starting the Heat stack update, it occurred to me that the processing took longer than usual. Well, after I SSHed into the overcloud nodes I realized why. Heat simply wiped out the entire disk content of the existing nodes and replaced it with the fresh disk image. Wow, my entire workload cloud was gone!</p>

<p>I learned that when you update the disk image in the undercloud, Heat will find out what nodes have to be updated and will simply replace their disk content with the new image. If you are orchestrating cloud deployments where your machines are cattle, this is what you want, however:</p>

<blockquote><p>The overcloud baremetal nodes are pets and should not be handled as cattle.</p></blockquote>


<p>To protect the overcloud nodes from deletion, I run the following command for each node against the undercloud Nova database:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">UPDATE</span> <span class="n">instances</span> <span class="k">SET</span> <span class="n">disable_terminate</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">WHERE</span> <span class="n">uuid</span> <span class="o">=</span> <span class="s1">&#39;&lt;uuid of the overcloud instance&gt;&#39;</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>


<p>So far, I haven&rsquo;t found a better way how to do it. This effectively prevents deleting the node whether by issuing a <code>nova delete</code> command or by Heat when updating the stack.</p>

<h2>Conclusion and suggestions</h2>

<p>TripleO installer is a great tool to deploy an OpenStack cloud. It&rsquo;s backed by a large user community and doesn&rsquo;t invent any new tools to install OpenStack.</p>

<p>On the other hand, I&rsquo;m somewhat sceptical about Heat being the right tool to do software configuration. Funneling the configuration options through the Heat templates down to the Puppet scripts seems cumbersome to me.</p>

<p>I&rsquo;d like to suggest the following approach: let Heat do the node provisioning, network configuration and perhaps a minimum node setup using cloud-init. At the end of the deployment, Heat would provide the information about the deployment in the format understandable to the configuration management tools like Puppet, Chef or Ansible. The configuration management tool then merges the facts provided by Heat with the tons of OpenStack configuration settings provided by the user. The following OpenStack installation, configuration, and later orchestration would solely be done by the configuration management tool more suitable for this job. Heat would not be involved at all in this stage.</p>
]]></content>
  </entry>

</feed>
