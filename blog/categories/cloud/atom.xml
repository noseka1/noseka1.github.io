<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: cloud | Ales Nosek - The Software Practitioner]]></title>
  <link href="http://alesnosek.com/blog/categories/cloud/atom.xml" rel="self"/>
  <link href="http://alesnosek.com/"/>
  <updated>2016-05-22T21:43:53-07:00</updated>
  <id>http://alesnosek.com/</id>
  <author>
    <name><![CDATA[Ales Nosek]]></name>

  </author>
  <generator uri="http://octopress.org/">Octopress</generator>


  <entry>
    <title type="html"><![CDATA[Test-Driving OpenStack Manila]]></title>
    <link href="http://alesnosek.com/blog/2016/05/22/test-driving-openstack-manila/"/>
    <updated>2016-05-22T16:59:30-07:00</updated>
    <id>http://alesnosek.com/blog/2016/05/22/test-driving-openstack-manila</id>
    <content type="html"><![CDATA[<p>Do you need to provision an NFS share for your Hadoop cluster? And what about creating a CIFS share to make your files accesible to the Windows clients? Manila is a provisioning and management service for shared file systems within OpenStack. Let&rsquo;s test-drive it in this blogpost.</p>

<!-- more -->


<p>In this introductory article, we&rsquo;re going to allocate a volume in Cinder and provide that volume as an NFS share to our Nova instances. For this, I&rsquo;m using the OpenStack Mitaka installed via TripleO on RHEL7. The Manila version included in the Mitaka release is version 2.0.</p>

<p>After installing Manila, the following Manila services are running on the controller nodes:</p>

<ul>
<li><em>openstack-manila-api</em> exposes REST APIs that the Manila client talks to.</li>
<li><em>openstack-manila-scheduler</em> makes provisioning decisions when creating a new share.</li>
<li><em>openstack-manila-share</em> comes with a host of drivers to talk to the storage systems.</li>
</ul>


<h2>Configuring the generic share driver</h2>

<p>In order for Manila to allocate shares on Cinder volumes, we&rsquo;ll have to configure Manila to use the <em>generic</em> share driver. For that we&rsquo;ll add a new Manila backend <code>generic_backend</code> into <code>/etc/manila/manila.conf</code>:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='ini'><span class='line'><span class="k">[DEFAULT]</span>
</span><span class='line'><span class="na">enabled_share_backends</span> <span class="o">=</span> <span class="s">generic_backend</span>
</span><span class='line'><span class="na">default_share_type</span> <span class="o">=</span> <span class="s">generic</span>
</span><span class='line'><span class="k">[generic_backend]</span>
</span><span class='line'><span class="na">share_driver</span> <span class="o">=</span> <span class="s">manila.share.drivers.generic.GenericShareDriver</span>
</span><span class='line'><span class="na">share_backend_name</span> <span class="o">=</span> <span class="s">generic_backend</span>
</span><span class='line'><span class="na">service_instance_name_template</span> <span class="o">=</span> <span class="s">manila_service_instance_%s</span>
</span><span class='line'><span class="na">service_image_name</span> <span class="o">=</span> <span class="s">manila-service-image-master</span>
</span><span class='line'><span class="na">driver_handles_share_servers</span> <span class="o">=</span> <span class="s">True</span>
</span><span class='line'><span class="na">service_instance_flavor_id</span> <span class="o">=</span> <span class="s">103</span>
</span><span class='line'><span class="na">connect_share_server_to_tenant_network</span> <span class="o">=</span> <span class="s">True</span>
</span><span class='line'><span class="na">service_instance_user</span> <span class="o">=</span> <span class="s">manila</span>
</span><span class='line'><span class="na">path_to_public_key</span> <span class="o">=</span> <span class="s">/etc/manila/id_rsa.pub</span>
</span><span class='line'><span class="na">path_to_private_key</span> <span class="o">=</span> <span class="s">/etc/manila/id_rsa</span>
</span><span class='line'><span class="na">manila_service_keypair_name</span> <span class="o">=</span> <span class="s">manila-service</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Before explaining the configuration settings, I&rsquo;ll briefly describe how the <em>generic</em> driver actually works. Behind the scenes, the generic driver creates a so called <em>service instance</em>. The service instance is a Nova instance owned by the Manila service. It&rsquo;s not even visible to the tenant users. Manila allocates a Cinder volume and asks Nova to attach that volume to the service instance. Afterwards, Manila connects to the service instance using SSH in order to create the filesytem on the attached Cinder volume and mount it and export that as a NFS/CIFS share to the tenant instances.</p>

<p>The service instance can be created by the OpenStack administrator or we can configure Manila to create the service instance by itself (option <code>driver_handles_share_servers = True</code>).</p>

<p>The service instance will be created from the image that we have to upload into Glance beforehand. I downloaded an existing Manila service image from <a href="http://tarballs.openstack.org/manila-image-elements/images/manila-service-image-master.qcow2">here</a>. This image is based on Ubuntu 14.04.4 LTS and includes the <code>manila</code> user account and the NFS and Samba server software packages. I uploaded this image into Glance under the name <code>manila-service-image-master</code>.</p>

<p>Next I&rsquo;ve chosen the size of the machine used for the service instance with <code>service_instance_flavor_id = 103</code>.</p>

<p>The service instance is connected to two networks. The first network is called a <em>service network</em> and is created by Manila before booting up the service instance. Manila uses this network for the SSH access to the service instance. The second network is a <em>share network</em>. The NFS server managed by Manila is accessible on this network. In our case, because we have configured <code>connect_share_server_to_tenant_network = True</code>, the share network will directly map to one of our tenant networks.</p>

<p>Finally, we have to generate a public/private key pair and tell Manila about it using the options <code>path_to_public_key</code> and <code>path_to_private_key</code>. Manila will upload this keypair into Nova under the name <code>manila-service</code>. When creating the service instance, Nova injects the public key into the instance and so allows Manila the SSH access.</p>

<p>In order to make our generic backend available to the Manila users, we&rsquo;re going to define a <code>generic</code> share type next.</p>

<h2>Defining a share type</h2>

<p>The <em>share type</em> has a similar purpose as the <em>volume type</em> in Cinder. It defines the backend used for the share creation. If there are multiple share backends available, an OpenStack administrator can define a separate share type for each of them. When creating a new share, the user can choose which share type to allocate the storage from.</p>

<p>To create a <code>generic</code> share type that maps to our <code>generic</code> backend you can run the following commands as an OpenStack administrator:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>manila <span class="nb">type</span>-create generic True
</span><span class='line'>manila <span class="nb">type</span>-key generic <span class="nb">set </span><span class="nv">share_backend_name</span><span class="o">=</span>generic_backend
</span></code></pre></td></tr></table></div></figure></p>

<h2>Creating a share and mounting it</h2>

<p>Finally, we&rsquo;re done with all the configuration and can start enjoying our share service. All the following commands are run as an ordinary tenant user.</p>

<p>At first, we&rsquo;d like to create a share network and map it to one of our tenant networks:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>manila share-network-create <span class="p">&amp;</span>ndash<span class="p">;</span>neutron-net-id 4f179a8c-7068-4f0b-9be4-9cb11451b401 <span class="p">&amp;</span>ndash<span class="p">;</span>neutron-subnet-id c7d753b0-039b-4f8c-9e0f-012651ff4ada <span class="p">&amp;</span>ndash<span class="p">;</span>name management
</span></code></pre></td></tr></table></div></figure></p>

<p>Now we can create our first NFS share called <code>myshare</code> with the size 1 GB:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>manila create <span class="p">&amp;</span>ndash<span class="p">;</span>name myshare <span class="p">&amp;</span>ndash<span class="p">;</span>share-network management NFS 1
</span></code></pre></td></tr></table></div></figure></p>

<p>Creating the first share on a given tenant network takes longer as Manila has to spin up a new service instance in the background.</p>

<p>Eventually, the status of the share turns into <code>available</code> which means that the share is ready. The <code>manila show myshare</code> command will display the location from where we can mount the share. In our case, it is <code>10.13.243.173:/shares/share-b87367aa-3ef3-4282-a6b5-e45cab991b6c</code>. Before we can mount the share we have to allow access to it by modifying the access list:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>manila access-allow <span class="p">&amp;</span>ndash<span class="p">;</span>access_level rw myshare ip 10.13.244.12
</span></code></pre></td></tr></table></div></figure></p>

<p>The above command provides an instance having the IP address 10.13.244.12 with a read-write access to the share. Note that the IP addresses 10.13.243.173 and 10.13.244.12 belong to the same network. Finally, we can SSH into the instance and mount the share with:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo mount -t nfs 10.13.243.173:/shares/share-b87367aa-3ef3-4282-a6b5-e45cab991b6c /mnt
</span></code></pre></td></tr></table></div></figure></p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[TripleO Installer - the Good, the Bad and the Ugly]]></title>
    <link href="http://alesnosek.com/blog/2016/03/27/tripleo-installer-the-good/"/>
    <updated>2016-03-27T19:39:15-07:00</updated>
    <id>http://alesnosek.com/blog/2016/03/27/tripleo-installer-the-good</id>
    <content type="html"><![CDATA[<p><a href="https://wiki.openstack.org/wiki/TripleO">TripleO</a> is an OpenStack deployment and management tool I&rsquo;ve been using since the Kilo release of OpenStack. It does its job pretty well, however not everything is perfect. My experience presented in this article applies more or less to the Red Hat&rsquo;s OpenStack director too, as the Red Hat OpenStack director is a downstream version of TripleO.</p>

<!-- more -->


<h2>The good things about TripleO</h2>

<h3>TripleO is a great idea</h3>

<p>TripleO, aka OpenStack-on-OpenStack, installs OpenStack cluster using OpenStack. At first, a minimum one-node OpenStack installation is created which is in turn used to provision a much bigger workload OpenStack cluster. I find this TripleO idea amazing. If OpenStack is the best way to manage your infrastructure, then why use something else to install it? As an administrator I would prefer to provision my OpenStack nodes with Ironic before introducing yet another tool like <a href="http://cobbler.github.io/">Cobbler</a> to do the same job. Needless to say that as the Ironic and Heat components improve, so improves the OpenStack installation experience.</p>

<p>One could argue that using the OpenStack to form an installer comes with a ton of complexity when installing the installer itself. In my experience, however, the installation of the undercloud OpenStack using the provided Puppet scripts doesn&rsquo;t impose any problem.</p>

<h3>TripleO has a vibrant community</h3>

<p>TripleO is used to continuously deploy and test the OpenStack cloud during its development. The RDO project adopted TripleO as their OpenStack installation tool. Red Hat derives their OpenStack director installer from the RDO project. A large community of TripleO users is a great plus.</p>

<h2>The bad things about TripleO</h2>

<h3>Configuration flexibility</h3>

<p>TripleO installer consists of a bunch of Heat templates to orchestrate the overcloud image provisioning and a number of Puppet and shell scripts for the following configuration of the overcloud nodes. These templates and scripts are heavily developed from release to release as the new TripleO features come in. To avoid the upgrade headaches, you should not modify the TripleO templates and scripts directly. Instead, TripleO provides extension points (via extra config) where you can put your customizations. This didn&rsquo;t work for me. My goal was to deploy an Ironic service in the overcloud OpenStack. For that to work, I needed to provision an additional undercloud network including a VIP for the load balancer. This was not possible without patching the Heat templates and Puppet scripts. I dread the day when I&rsquo;ll have to port these patches to the next TripleO release.</p>

<p>Furthermore, the current way to modify OpenStack configuration properties is less straight forward. To configure a property, I have to first grep through the Puppet scripts to find out whether the desired property is managed by Puppet or not. Afterwards, I grep through the TripleO Heat templates to find out whether TripleO provides a direct template parameter to set the Puppet variable or not. Afterwards, I can either pass the parameter to the TripleO template or I set the Puppet variable in the extra config section or I&rsquo;m on my own.</p>

<p><blockquote><p>I&rsquo;d like to be able to easily modify any property in any configuration file on any OpenStack node.</p></blockquote></p>

<p>OpenStack comes with tons of configuration properties and I think it would be great to have a more straight forward way to configure them.</p>

<h3>Deployment control</h3>

<p>TripleO uses Heat to deploy and configure the overcloud OpenStack. Heat orchestrates the infrastracture based on the description provided by the user in the Heat templates. In the Heat templates, we tell Heat what our deployment should look like, but we have no control over the steps Heat will take to get to the desired state. I find this lack of control rather problematic.</p>

<p><blockquote><p>A fine-grained deployment control would be desirable.</p></blockquote></p>

<p>Let&rsquo;s say I have an overcloud consisting of 100 nodes. After changing the configuration in my Heat templates, I can only re-run the entire Heat configuration process and hope that I won&rsquo;t end up with a broken cloud. Instead, I&rsquo;d like to apply the configuration changes to a couple of nodes to make sure that everything works before I continue with the rest of the cloud. The ability to apply only part of the configuration would be useful as well.</p>

<h2>The ugly experience with TripleO</h2>

<p>I&rsquo;d like to share one scary experience I had with the TripleO installer. While using TripleO for a couple of months, I have to say that this was the only serious problem I&rsquo;ve encountered.</p>

<p>One day I uploaded an updated node image into the undercloud OpenStack. I was about to create new nodes in the overcloud cluster and wanted to have them provisioned with this new image. After starting the Heat stack update, it occurred to me that the processing took longer than usual. Well, after I SSHed into the overcloud nodes I realized why. Heat simply wiped out the entire disk content of the existing nodes and replaced it with the fresh disk image. Wow, my entire workload cloud was gone!</p>

<p>I learned that when you update the disk image in the undercloud, Heat will find out what nodes have to be updated and will simply replace their disk content with the new image. If you are orchestrating cloud deployments where your machines are cattle, this is what you want, however:</p>

<p><blockquote><p>The overcloud baremetal nodes are pets and should not be handled as cattle.</p></blockquote></p>

<p>To protect the overcloud nodes from deletion, I run the following command for each node against the undercloud Nova database:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">UPDATE</span> <span class="n">instances</span> <span class="k">SET</span> <span class="n">disable_terminate</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">WHERE</span> <span class="n">uuid</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">lsquo</span><span class="p">;</span><span class="o">&lt;</span><span class="n">uuid</span> <span class="k">of</span> <span class="n">the</span> <span class="n">overcloud</span> <span class="n">instance</span><span class="o">&gt;&amp;</span><span class="n">rsquo</span><span class="p">;;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>So far, I haven&rsquo;t found a better way how to do it. This effectively prevents deleting the node whether by issuing a <code>nova delete</code> command or by Heat when updating the stack.</p>

<h2>Conclusion and suggestions</h2>

<p>TripleO installer is a great tool to deploy an OpenStack cloud. It&rsquo;s backed by a large user community and doesn&rsquo;t invent any new tools to install OpenStack.</p>

<p>On the other hand, I&rsquo;m somewhat sceptical about Heat being the right tool to do software configuration. Funneling the configuration options through the Heat templates down to the Puppet scripts seems cumbersome to me.</p>

<p>I&rsquo;d like to suggest the following approach: let Heat do the node provisioning, network configuration and perhaps a minimum node setup using cloud-init. At the end of the deployment, Heat would provide the information about the deployment in the format understandable to the configuration management tools like Puppet, Chef or Ansible. The configuration management tool then merges the facts provided by Heat with the tons of OpenStack configuration settings provided by the user. The following OpenStack installation, configuration, and later orchestration would solely be done by the configuration management tool more suitable for this job. Heat would not be involved at all in this stage.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Bootstrapping a Galera Cluster on RHEL7]]></title>
    <link href="http://alesnosek.com/blog/2016/01/31/bootstrapping-a-galera-cluster-on-rhel7/"/>
    <updated>2016-01-31T15:24:55-08:00</updated>
    <id>http://alesnosek.com/blog/2016/01/31/bootstrapping-a-galera-cluster-on-rhel7</id>
    <content type="html"><![CDATA[<p>The MariaDB Galera packages provided by the RDO project in their OpenStack repositories don&rsquo;t seem to include a command or script to bootstrap the cluster. Let&rsquo;s look at an alternative way to bring the cluster up.</p>

<!-- more -->


<p>RHEL7 comes with the init system <code>systemd</code>. Unfortunately, systemd doesn&rsquo;t provide a way to pass command-line arguments to the unit files. Hence, doing something like this won&rsquo;t work:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@rhel1 ~<span class="o">]</span><span class="nv">$ </span>systemctl start mariadb <span class="p">&amp;</span>ndash<span class="p">;</span>wsrep_new_cluster
</span><span class='line'>systemctl: unrecognized option <span class="p">&amp;</span>lsquo<span class="p">;&amp;</span>ndash<span class="p">;</span>wsrep_new_cluster<span class="p">&amp;</span>rsquo<span class="p">;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Instead of passing command-line arguments, systemd allows for creating <a href="http://0pointer.de/blog/projects/instances.html">multiple instances</a> of the same service where each instance can obtain it&rsquo;s own set of environment variables. The Percona XtraDB Cluster includes the standard and the bootstrap service instance definitions in the RPM package <code>Percona-XtraDB-Cluster-server</code>. To boostrap the Percona cluster, the first node can be started with the following command:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@percona1 ~<span class="o">]</span><span class="nv">$ </span>systemctl start &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;&amp;#x6d;&amp;#97;&amp;#105;&amp;#108;&amp;#x74;&amp;#x6f;&amp;#x3a;&amp;#109;&amp;#121;&amp;#115;&amp;#113;&amp;#x6c;&amp;#64;&amp;#98;&amp;#x6f;&amp;#x6f;&amp;#116;&amp;#x73;&amp;#x74;&amp;#114;&amp;#x61;&amp;#112;&amp;#x2e;&amp;#115;&amp;#101;&amp;#x72;&amp;#x76;&amp;#105;&amp;#x63;&amp;#101;&quot;</span>&gt;<span class="p">&amp;</span><span class="c">#x6d;&amp;#121;&amp;#115;&amp;#x71;&amp;#108;&amp;#x40;&amp;#98;&amp;#x6f;&amp;#x6f;&amp;#x74;&amp;#115;&amp;#x74;&amp;#114;&amp;#97;&amp;#112;&amp;#x2e;&amp;#x73;&amp;#101;&amp;#114;&amp;#118;&amp;#105;&amp;#x63;&amp;#101;&lt;/a&gt;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>At the moment, this boostrap service definition is missing in the RDO OpenStack packages. Before a similar <code>mysql@.service</code> script is available in RDO you can start the MariaDB Galera cluster as follows:</p>

<ul>
<li><p>On the first node, start the MariaDB with the <code>--wsrep-new-cluster</code> to create a new cluster:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@rhel1 ~<span class="o">]</span><span class="nv">$ </span>/usr/bin/mysqld_safe <span class="p">&amp;</span>ndash<span class="p">;</span>wsrep-new-cluster
</span></code></pre></td></tr></table></div></figure>
Let the command run in the foreground.</p></li>
<li><p>On the remaining cluster nodes start the mariadb service as usual:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@rhel2 ~<span class="o">]</span><span class="nv">$ </span>systemctl start mariadb
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>After the cluster has been fully formed, stop the mariadb on the first node by sending it a SIGQUIT (press CTRL + \ on the console).</p></li>
<li><p>On the first node, start the mariadb service via systemd:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@rhel1 ~<span class="o">]</span><span class="nv">$ </span>systemctl start mariadb
</span></code></pre></td></tr></table></div></figure></p></li>
</ul>


<p>That&rsquo;s it. You can check the status of each of the cluster nodes by running the following command:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@rhel1 ~<span class="o">]</span><span class="nv">$ </span>mysql -e <span class="p">&amp;</span>ldquo<span class="p">;</span>SHOW GLOBAL STATUS LIKE <span class="p">&amp;</span>lsquo<span class="p">;</span>wsrep%<span class="p">&amp;</span>rsquo<span class="p">;;&amp;</span>rdquo<span class="p">;</span>
</span></code></pre></td></tr></table></div></figure></p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Monitoring OpenStack Cluster with Icinga]]></title>
    <link href="http://alesnosek.com/blog/2015/11/30/monitoring-openstack-cluster-with-icinga/"/>
    <updated>2015-11-30T21:13:33-08:00</updated>
    <id>http://alesnosek.com/blog/2015/11/30/monitoring-openstack-cluster-with-icinga</id>
    <content type="html"><![CDATA[<p>If you don&rsquo;t monitor it, it&rsquo;s not in production! To get an OpenStack cloud ready for production, monitoring is a must. Let&rsquo;s take a look at two projects providing Nagios/Icinga plugins for checking the health of OpenStack services.</p>

<!-- more -->


<p>First, a few words about <a href="https://www.icinga.org/" title="Icinga">Icinga</a>. I started using Icinga 2 only recently and I&rsquo;m very pleased with this flexible and well-documented software. I&rsquo;ve listened to a German presentation about Icinga where they said that Icinga was not that widely spread in the US as it was the case in Europe. Dear Icinga team, you have one more happy user in the US now. Your software just works and your web GUI is beautiful.</p>

<p>I found two very useful projects for monitoring the OpenStack APIs both hosted on GitHub:</p>

<ul>
<li><a href="https://github.com/cirrax/openstack-nagios-plugins">OpenStack Nagios Plugins</a></li>
<li><a href="https://github.com/openstack/monitoring-for-openstack">Monitoring for OpenStack</a></li>
</ul>


<h2>OpenStack Nagios Plugins</h2>

<p><a href="https://github.com/cirrax/openstack-nagios-plugins">OpenStack Nagios Plugins</a> provides a collection of checks for the OpenStack services Nova, Neutron, Cinder, Keystone and Ceilometer. Available plugins worked right away with my OpenStack Liberty cluster. The Nova Hypervisor check monitors the &ldquo;virtual&rdquo; CPU and memory usage across your compute nodes. The name virtual CPU is a little misleading here. In reality, the number of physical cores is monitored as the Nova API actually reports the number of physical cores. I stick to the OpenStack default settings that overcommit the CPUs by factor of 16 and the memory by factor of 1.5. To accommodate this fact, I changed the warning and critical ranges for the check_nova-hypervisors plugin as follows:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>check_nova-hypervisors <span class="p">&amp;</span>ndash<span class="p">;</span>warn_memory_percent 0:135 <span class="p">&amp;</span>ndash<span class="p">;</span>critical_memory_percent 0:142 <span class="p">&amp;</span>ndash<span class="p">;</span>warn_vcpus_percent 0:1440 <span class="p">&amp;</span>ndash<span class="p">;</span>critical_vcpus_percent 0:1520
</span></code></pre></td></tr></table></div></figure></p>

<h2>Monitoring for OpenStack</h2>

<p>Plugins coming with the <a href="(https://github.com/openstack/monitoring-for-openstack">Monitoring for OpenStack</a> project provide deeper checks of OpenStack functionality. I liked the following ones the best:</p>

<ul>
<li><code>check_nova_instance</code>: Creates an instance on your cloud and deletes it again as soon as it is active. It&rsquo;s recommended to use a small disk image like cirros for this check.</li>
<li><code>cinder_volume</code>: Allocates a volume of size 1GB and deletes it again.</li>
<li><code>neutron_floating_ip</code>: Tries to allocate a floating IP. You have to configure the network where to allocate the IP from.</li>
<li><code>glance_upload</code>: Uploads 1MB of data as an image into Glance.</li>
<li><code>check_horizon_login</code>: Given a user name and a password the plugin will log into the Horizon dashboard.</li>
</ul>


<p>Some of the plugins didn&rsquo;t work for me due to incompatibilities with the Liberty client APIs. If you encounter the same problem you can try out my fixed version of the plugins on GitHub <a href="https://github.com/noseka1/monitoring-for-openstack">here</a>.</p>

<h2>Icinga 2 Screenshot</h2>

<p>And this is how the OpenStack APIs service group looks in Icinga Web 2. Happy monitoring!</p>

<p><img class="center" src="/images/posts/osmon.png"></p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Assigning Roles to Nodes Directly in RDO]]></title>
    <link href="http://alesnosek.com/blog/2015/11/09/assigning-roles-to-nodes-directly-in-rdo/"/>
    <updated>2015-11-09T20:49:40-08:00</updated>
    <id>http://alesnosek.com/blog/2015/11/09/assigning-roles-to-nodes-directly-in-rdo</id>
    <content type="html"><![CDATA[<p>RDO Manager defines multiple roles that nodes can play in OpenStack deployment. For large-sized installations, RDO features automatic assignment of roles to nodes. This assignment is based on the facts that RDO obtained about each node during the introspection. However, for smaller deployments, you might prefer to assign the roles to the available nodes by hand. It was not straight forward for me to find out about this manual option even when it is described in the <a href="http://docs.openstack.org/developer/tripleo-docs/advanced_deployment/profile_matching.html#optional-manually-add-the-profiles-to-the-nodes" title="TripleO documentation">TripleO documentation</a>. Let&rsquo;s review the required configuration steps in this blogpost.</p>

<!-- more -->


<p>The relationship between roles and nodes is organized via flavors. A flavor is a set of properties that the target node must match in order to be eligible for deployment of a specific role. The manual assignment of a role to a node is a three-step process:</p>

<ol>
<li>Define a flavor with a property <code>capabilities:profile</code> set to the role name</li>
<li>Add the same profile to the capabilities list of the target node</li>
<li>Tell RDO what flavor to use for a specific role when beginning the deployment</li>
</ol>


<p>The creation of flavors with the associated <code>capabilities:profile</code> property looks like this:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>openstack flavor create <span class="p">&amp;</span>ndash<span class="p">;</span>id auto <span class="p">&amp;</span>ndash<span class="p">;</span>ram <span class="m">4096</span> <span class="p">&amp;</span>ndash<span class="p">;</span>disk <span class="m">40</span> <span class="p">&amp;</span>ndash<span class="p">;</span>vcpus <span class="m">1</span> ceph
</span><span class='line'>openstack flavor create <span class="p">&amp;</span>ndash<span class="p">;</span>id auto <span class="p">&amp;</span>ndash<span class="p">;</span>ram <span class="m">4096</span> <span class="p">&amp;</span>ndash<span class="p">;</span>disk <span class="m">40</span> <span class="p">&amp;</span>ndash<span class="p">;</span>vcpus <span class="m">1</span> cinder
</span><span class='line'>openstack flavor create <span class="p">&amp;</span>ndash<span class="p">;</span>id auto <span class="p">&amp;</span>ndash<span class="p">;</span>ram <span class="m">4096</span> <span class="p">&amp;</span>ndash<span class="p">;</span>disk <span class="m">40</span> <span class="p">&amp;</span>ndash<span class="p">;</span>vcpus <span class="m">1</span> compute
</span><span class='line'>openstack flavor create <span class="p">&amp;</span>ndash<span class="p">;</span>id auto <span class="p">&amp;</span>ndash<span class="p">;</span>ram <span class="m">4096</span> <span class="p">&amp;</span>ndash<span class="p">;</span>disk <span class="m">40</span> <span class="p">&amp;</span>ndash<span class="p">;</span>vcpus <span class="m">1</span> controller
</span><span class='line'>openstack flavor create <span class="p">&amp;</span>ndash<span class="p">;</span>id auto <span class="p">&amp;</span>ndash<span class="p">;</span>ram <span class="m">4096</span> <span class="p">&amp;</span>ndash<span class="p">;</span>disk <span class="m">40</span> <span class="p">&amp;</span>ndash<span class="p">;</span>vcpus <span class="m">1</span> swift&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;openstack flavor <span class="nb">set</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>cpu_arch<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>x86_64<span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>capabilities:boot_option<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span><span class="nb">local</span><span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>capabilities:profile<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>ceph<span class="p">&amp;</span>rdquo<span class="p">;</span> ceph
</span><span class='line'>openstack flavor <span class="nb">set</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>cpu_arch<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>x86_64<span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>capabilities:boot_option<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span><span class="nb">local</span><span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>capabilities:profile<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>cinder<span class="p">&amp;</span>rdquo<span class="p">;</span> cinder
</span><span class='line'>openstack flavor <span class="nb">set</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>cpu_arch<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>x86_64<span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>capabilities:boot_option<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span><span class="nb">local</span><span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>capabilities:profile<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>compute<span class="p">&amp;</span>rdquo<span class="p">;</span> compute
</span><span class='line'>openstack flavor <span class="nb">set</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>cpu_arch<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>x86_64<span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>capabilities:boot_option<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span><span class="nb">local</span><span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>capabilities:profile<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>controller<span class="p">&amp;</span>rdquo<span class="p">;</span> controller
</span><span class='line'>openstack flavor <span class="nb">set</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>cpu_arch<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>x86_64<span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>capabilities:boot_option<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span><span class="nb">local</span><span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>capabilities:profile<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>swift<span class="p">&amp;</span>rdquo<span class="p">;</span> swift
</span></code></pre></td></tr></table></div></figure></p>

<p>Now we need to add the profiles to the capabilities list of the respective nodes:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>ironic node-update &lt;node1 UUID here&gt; replace properties/capabilities<span class="o">=</span><span class="p">&amp;</span>lsquo<span class="p">;</span>profile:ceph,boot_option:local<span class="p">&amp;</span>rsquo<span class="p">;</span>
</span><span class='line'>ironic node-update &lt;node2 UUID here&gt; replace properties/capabilities<span class="o">=</span><span class="p">&amp;</span>lsquo<span class="p">;</span>profile:cinder,boot_option:local<span class="p">&amp;</span>rsquo<span class="p">;</span>
</span><span class='line'>ironic node-update &lt;node3 UUID here&gt; replace properties/capabilities<span class="o">=</span><span class="p">&amp;</span>lsquo<span class="p">;</span>profile:compute,boot_option:local<span class="p">&amp;</span>rsquo<span class="p">;</span>
</span><span class='line'>ironic node-update &lt;node4 UUID here&gt; replace properties/capabilities<span class="o">=</span><span class="p">&amp;</span>lsquo<span class="p">;</span>profile:controller,boot_option:local<span class="p">&amp;</span>rsquo<span class="p">;</span>
</span><span class='line'>ironic node-update &lt;node5 UUID here&gt; replace properties/capabilities<span class="o">=</span><span class="p">&amp;</span>lsquo<span class="p">;</span>profile:swift,boot_option:local<span class="p">&amp;</span>rsquo<span class="p">;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>When deploying the OpenStack cloud, we need to tell the RDO manager what flavor to use for each specific role:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>openstack overcloud deploy <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>templates /usr/share/openstack-tripleo-heat-templates <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>ceph-storage-scale <span class="m">1</span> <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>block-storage-scale <span class="m">1</span> <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>compute-scale <span class="m">1</span> <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>control-scale <span class="m">1</span> <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>swift-storage-scale <span class="m">1</span> <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>ceph-storage-flavor ceph <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>block-storage-flavor cinder <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>compute-flavor compute <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>control-flavor controller <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>swift-storage-flavor swift
</span></code></pre></td></tr></table></div></figure></p>

<p>And that&rsquo;s all for today. Hope you&rsquo;re enjoying the full control over your OpenStack cloud deployment.</p>
]]></content>
  </entry>

</feed>
