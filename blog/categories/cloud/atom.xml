<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: cloud | Ales Nosek - The Software Practitioner]]></title>
  <link href="http://alesnosek.com/blog/categories/cloud/atom.xml" rel="self"/>
  <link href="http://alesnosek.com/"/>
  <updated>2016-06-26T19:58:29-07:00</updated>
  <id>http://alesnosek.com/</id>
  <author>
    <name><![CDATA[Ales Nosek]]></name>

  </author>
  <generator uri="http://octopress.org/">Octopress</generator>


  <entry>
    <title type="html"><![CDATA[Deploying Kubernetes on OpenStack using Heat]]></title>
    <link href="http://alesnosek.com/blog/2016/06/26/deploying-kubernetes-on-openstack-using-heat/"/>
    <updated>2016-06-26T08:28:11-07:00</updated>
    <id>http://alesnosek.com/blog/2016/06/26/deploying-kubernetes-on-openstack-using-heat</id>
    <content type="html"><![CDATA[<p>Want to install Kubernetes on top of OpenStack? There are <a href="http://kubernetes.io/docs/getting-started-guides/">many ways</a> how to install a Kubernetes cluster. The upcoming Kubernetes 1.3 release comes with yet another method called <a href="http://kubernetes.io/docs/getting-started-guides/openstack-heat/">OpenStack Heat</a>. In this article, we&rsquo;re going to explore this deployment method when creating a minimum Kubernetes cluster on top of OpenStack.</p>

<!-- more -->


<p>In this tutorial, there are three OpenStack virtual machines involved. The first machine called <em>Kubernetes installer</em> machine is created manually and is used for compiling Kubernetes from source and running the Kubernetes installer. Other two OpenStack machines, <em>Kubernetes master</em> and <em>Kubernetes node</em>, are created during the installation process.</p>

<p>The Kubernetes installer machine and both the Kubernetes machines run on the CentOS-7-x86_64-GenericCloud-1605 image. You can download this image from the <a href="http://cloud.centos.org/centos/7/images/">CentOS image repository</a>. After I uploaded the CentOS 7 image into OpenStack, it has been assigned ID <code>17e4e783-321c-48c1-9308-6f99d67c5fa6</code> for me.</p>

<h2>Building Kubernetes from source</h2>

<p>First off, let&rsquo;s spin up a Kubernetes installer machine in OpenStack. I recommend using the <code>m1.large</code> flavor that comes with 8 GB of RAM. The compilation of Kubernetes is rather memory intensive.</p>

<p>To ensure consistent and reproducible builds, a Docker container is created at the beginning of the build process and the build proceeds within the container. So, let&rsquo;s setup Docker a little quick on our build machine:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo yum install docker
</span></code></pre></td></tr></table></div></figure></p>

<p>Configure the Docker service to start on boot and start it:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo systemctl <span class="nb">enable </span>docker
</span><span class='line'>sudo systemctl start docker
</span></code></pre></td></tr></table></div></figure></p>

<p>The Kubernetes build scripts expect that the <code>docker</code> command can successfully contact the Docker daemon. In the default CentOS configuration, the <code>sudo docker</code> is required in order to connect to the <code>/var/run/docker.sock</code> socket which is owned by the user root. To overcome the permission problem, let&rsquo;s create a wrapper script that will invoke the <code>docker</code> command using <code>sudo</code>:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>mkdir bin
</span><span class='line'><span class="nb">echo</span> -e <span class="p">&amp;</span>lsquo<span class="p">;</span><span class="c">#!/bin/bash\nexec sudo /usr/bin/docker &amp;ldquo;$@&amp;rdquo;&amp;rsquo; &gt; bin/docker</span>
</span><span class='line'>chmod <span class="m">755</span> bin/docker
</span><span class='line'><span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span>~/bin:<span class="nv">$PATH</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>You can test your changes with the <code>docker info</code> command which should work now.</p>

<p>Kubernetes is written in the Go language and its source code is stored in a Git repository. So, let&rsquo;s install the Go language environment and Git:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo yum install golang git
</span></code></pre></td></tr></table></div></figure></p>

<p>Next we&rsquo;ll clone the Kubernetes Git repository and start the build. The <code>quick-release</code> make target creates a build for the amd64 architecture only and doesn&rsquo;t run any tests.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>git clone &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://github.com/kubernetes/kubernetes.git&quot;</span>&gt;https://github.com/kubernetes/kubernetes.git&lt;/a&gt;
</span><span class='line'><span class="nb">cd </span>kubernetes
</span><span class='line'>make quick-release
</span></code></pre></td></tr></table></div></figure></p>

<p>After about 15 minutes when the build was successful, you&rsquo;ll find the distribution tarballs <code>kubernetes.tar.gz</code> and <code>kubernetes-salt.tar.gz</code> in the <code>_output/release-tars</code> directory.</p>

<h2>Setting up the OpenStack CLI tools</h2>

<p>The Kubernetes installer uses the OpenStack CLI tools to talk to the OpenStack in order to create a Kubernetes cluster. Before you can install the OpenStack CLI tools on CentOS 7, you have to enable the OpenStack Mitaka RPM repository:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo yum install centos-release-openstack-mitaka
</span></code></pre></td></tr></table></div></figure></p>

<p>Install the OpenStack CLI tools that are used by the Kubernetes installer when creating a cluster with:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo yum install python-openstackclient python-swiftclient python-glanceclient python-novaclient python-heatclient
</span></code></pre></td></tr></table></div></figure></p>

<p>Next, you have to obtain your OpenStack <code>openrc.sh</code> file and source it into your environment:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>. openrc.sh
</span></code></pre></td></tr></table></div></figure></p>

<p>You should be able to talk to the OpenStack now. For example, check if you can list the available OpenStack networks with:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>openstack network list
</span></code></pre></td></tr></table></div></figure></p>

<h2>Configuring the Kubernetes installer</h2>

<p>In this section, we&rsquo;re going to more or less follow the instructions found in the chapter <a href="http://kubernetes.io/docs/getting-started-guides/openstack-heat/">OpenStack Heat</a> of the Kubernetes documentation.</p>

<p>When deploying the Kubernetes cluster, the installer executes the following steps that you can find in <code>cluster/openstack-heat/util.sh</code>:</p>

<ul>
<li>Upload the distribution tarballs <code>kubernetes.tar.gz</code> and <code>kubernetes-salt.tar.gz</code> into the <code>kubernetes</code> container in Swift</li>
<li>Upload the virtual machine image for the Kubernetes VMs into Glance</li>
<li>Add the user&rsquo;s keypair into Nova</li>
<li>Run a Heat script in order to create the Kubernetes VMs and put them on a newly created private network. Create a router connecting the private network with an external network.</li>
<li>At the first boot, the Kubernetes VMs download the distribution tarballs from Swift and install the Kubernetes software using Salt</li>
</ul>


<p>Let&rsquo;s create an <code>openstack-heat.sh</code> file with the configuration values for the Kubernetes installer:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nb">export </span><span class="nv">KUBERNETES_PROVIDER</span><span class="o">=</span>openstack-heat
</span><span class='line'><span class="nb">export </span><span class="nv">STACK_NAME</span><span class="o">=</span>kubernetes
</span><span class='line'><span class="nb">export </span><span class="nv">KUBERNETES_KEYPAIR_NAME</span><span class="o">=</span>mykeypair
</span><span class='line'><span class="nb">export </span><span class="nv">NUMBER_OF_MINIONS</span><span class="o">=</span>1
</span><span class='line'><span class="nb">export </span><span class="nv">MAX_NUMBER_OF_MINIONS</span><span class="o">=</span>1
</span><span class='line'><span class="nb">export </span><span class="nv">EXTERNAL_NETWORK</span><span class="o">=</span>gateway
</span><span class='line'><span class="nb">export </span><span class="nv">CREATE_IMAGE</span><span class="o">=</span><span class="nb">false</span>
</span><span class='line'><span class="nb">export </span><span class="nv">DOWNLOAD_IMAGE</span><span class="o">=</span><span class="nb">false</span>
</span><span class='line'><span class="nb">export </span><span class="nv">IMAGE_ID</span><span class="o">=</span>17e4e783-321c-48c1-9308-6f99d67c5fa6
</span><span class='line'><span class="nb">export </span><span class="nv">DNS_SERVER</span><span class="o">=</span>10.0.0.10
</span><span class='line'><span class="nb">export </span><span class="nv">SWIFT_SERVER_URL</span><span class="o">=</span>&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://openstack.localdomain:13808/swift/v1&quot;</span>&gt;https://openstack.localdomain:13808/swift/v1&lt;/a&gt;
</span></code></pre></td></tr></table></div></figure></p>

<p>The above configuration will create exactly one Kubernetes master and one Kubernetes node. It will inject the keypair called <code>mykeypair</code> into both of them. Note that you have to ensure that the keypair <code>mykeypair</code> exists in Nova before proceeding. You probably want to change the name of the external network to a network available in your OpenStack. We&rsquo;re going to use the same CentOS 7 image for both of our Kubernetes VMs. This CentOS image has already been uploaded into OpenStack and in my case it was assigned ID <code>17e4e783-321c-48c1-9308-6f99d67c5fa6</code>. You also want to change the IP address of the DNS server to something that suits your environment. The Swift server URL is the public endpoint of your Swift server that you can obtain from the output of the command <code>openstack catalog show object-store</code>.</p>

<p>When your configuration is ready, you can source it into your environment:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>. openstack-heat.sh
</span></code></pre></td></tr></table></div></figure></p>

<p>Next, in my environment I had a problem where the IP range of the private network created by Kubernetes collided with the existing corporate network in my company. I had to directly edit the file <code>cluster/openstack-heat/kubernetes-heat/kubecluster.yaml</code> to change the <code>10.0.0.0/24</code> CIDR to something like <code>10.123.0.0/24</code>. If you don&rsquo;t have this problem you can safely use the default settings.</p>

<p>The Kubernetes cluster can leverage the underlying OpenStack cloud to attach existing Cinder volumes to the Kubernetes pods and to create external loadbalancers. For this to work, Kubernetes has to know how to connect to OpenStack APIs. With regard to the external loadbalancers, we also need to tell Kubernetes what Neutron subnet the loadbalancer&rsquo;s VIP should be placed on.</p>

<p>The OpenStack configuration can be found in the <em>cloud-config</em> script <code>cluster/openstack-heat/kubernetes-heat/fragments/configure-salt.yaml</code>. You can see that this script will create a configuration file <code>/srv/kubernetes/openstack.conf</code> on the Kubernetes machine which contains the OpenStack settings. In my case, I changed the original block:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[Global]
</span><span class='line'>auth-url=$OS_AUTH_URL
</span><span class='line'>username=$OS_USERNAME
</span><span class='line'>password=$OS_PASSWORD
</span><span class='line'>region=$OS_REGION_NAME
</span><span class='line'>tenant-id=$OS_TENANT_ID</span></code></pre></td></tr></table></div></figure></p>

<p>to read:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[Global]
</span><span class='line'>auth-url=$OS_AUTH_URL
</span><span class='line'>username=$OS_USERNAME
</span><span class='line'>password=$OS_PASSWORD
</span><span class='line'>region=$OS_REGION_NAME
</span><span class='line'>tenant-id=$OS_TENANT_ID
</span><span class='line'>domain-name=MyDomain # Keystone V3 domain
</span><span class='line'>[LoadBalancer]
</span><span class='line'>lb-version=v1
</span><span class='line'>subnet-id=73f8eb91-90cf-42f4-85d0-dcff44077313</span></code></pre></td></tr></table></div></figure></p>

<p>Besides adding the <code>LoadBalancer</code> section, I also appended the <code>domain-name</code> option to the end of the <code>Global</code> section, as in my OpenStack environment I want to authenticate against a non-default Keystone V3 domain.</p>

<h2>Installing the Kubernetes cluster</h2>

<p>After you&rsquo;ve sourced both the <code>openrc.sh</code> and <code>openstack-heat.sh</code> environment settings, you can kick off the installation of the Kubernetes cluster with:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>./cluster/kube-up.sh
</span></code></pre></td></tr></table></div></figure></p>

<p>After about 25 minutes, you should have a Kubernetes cluster up and running. You can check the status of the Kubernetes pods with the command:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>./cluster/kubectl.sh get pods <span class="p">&amp;</span>ndash<span class="p">;</span>namespace kube-system
</span></code></pre></td></tr></table></div></figure></p>

<p>All pods should be running. The network topology of the Kubernetes cluster as displayed by Horizon:</p>

<p><img class="center" src="/images/posts/kube.png"></p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Test-Driving OpenStack Manila]]></title>
    <link href="http://alesnosek.com/blog/2016/05/22/test-driving-openstack-manila/"/>
    <updated>2016-05-22T16:59:30-07:00</updated>
    <id>http://alesnosek.com/blog/2016/05/22/test-driving-openstack-manila</id>
    <content type="html"><![CDATA[<p>Do you need to provision an NFS share for your Hadoop cluster? And what about creating a CIFS share to make your files accesible to the Windows clients? Manila is a provisioning and management service for shared file systems within OpenStack. Let&rsquo;s test-drive it in this blogpost.</p>

<!-- more -->


<p>In this introductory article, we&rsquo;re going to allocate a volume in Cinder and provide that volume as an NFS share to our Nova instances. For this, I&rsquo;m using the OpenStack Mitaka installed via TripleO on RHEL7. The Manila version included in the Mitaka release is version 2.0.</p>

<p>After installing Manila, the following Manila services are running on the controller nodes:</p>

<ul>
<li><em>openstack-manila-api</em> exposes REST APIs that the Manila client talks to.</li>
<li><em>openstack-manila-scheduler</em> makes provisioning decisions when creating a new share.</li>
<li><em>openstack-manila-share</em> comes with a host of drivers to talk to the storage systems.</li>
</ul>


<h2>Configuring the generic share driver</h2>

<p>In order for Manila to allocate shares on Cinder volumes, we&rsquo;ll have to configure Manila to use the <em>generic</em> share driver. For that we&rsquo;ll add a new Manila backend <code>generic_backend</code> into <code>/etc/manila/manila.conf</code>:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='ini'><span class='line'><span class="k">[DEFAULT]</span>
</span><span class='line'><span class="na">enabled_share_backends</span> <span class="o">=</span> <span class="s">generic_backend</span>
</span><span class='line'><span class="na">default_share_type</span> <span class="o">=</span> <span class="s">generic</span>
</span><span class='line'><span class="k">[generic_backend]</span>
</span><span class='line'><span class="na">share_driver</span> <span class="o">=</span> <span class="s">manila.share.drivers.generic.GenericShareDriver</span>
</span><span class='line'><span class="na">share_backend_name</span> <span class="o">=</span> <span class="s">generic_backend</span>
</span><span class='line'><span class="na">service_instance_name_template</span> <span class="o">=</span> <span class="s">manila_service_instance_%s</span>
</span><span class='line'><span class="na">service_image_name</span> <span class="o">=</span> <span class="s">manila-service-image-master</span>
</span><span class='line'><span class="na">driver_handles_share_servers</span> <span class="o">=</span> <span class="s">True</span>
</span><span class='line'><span class="na">service_instance_flavor_id</span> <span class="o">=</span> <span class="s">103</span>
</span><span class='line'><span class="na">connect_share_server_to_tenant_network</span> <span class="o">=</span> <span class="s">True</span>
</span><span class='line'><span class="na">service_instance_user</span> <span class="o">=</span> <span class="s">manila</span>
</span><span class='line'><span class="na">path_to_public_key</span> <span class="o">=</span> <span class="s">/etc/manila/id_rsa.pub</span>
</span><span class='line'><span class="na">path_to_private_key</span> <span class="o">=</span> <span class="s">/etc/manila/id_rsa</span>
</span><span class='line'><span class="na">manila_service_keypair_name</span> <span class="o">=</span> <span class="s">manila-service</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Before explaining the configuration settings, I&rsquo;ll briefly describe how the <em>generic</em> driver actually works. Behind the scenes, the generic driver creates a so called <em>service instance</em>. The service instance is a Nova instance owned by the Manila service. It&rsquo;s not even visible to the tenant users. Manila allocates a Cinder volume and asks Nova to attach that volume to the service instance. Afterwards, Manila connects to the service instance using SSH in order to create the filesytem on the attached Cinder volume and mount it and export that as a NFS/CIFS share to the tenant instances.</p>

<p>The service instance can be created by the OpenStack administrator or we can configure Manila to create the service instance by itself (option <code>driver_handles_share_servers = True</code>).</p>

<p>The service instance will be created from the image that we have to upload into Glance beforehand. I downloaded an existing Manila service image from <a href="http://tarballs.openstack.org/manila-image-elements/images/manila-service-image-master.qcow2">here</a>. This image is based on Ubuntu 14.04.4 LTS and includes the <code>manila</code> user account and the NFS and Samba server software packages. I uploaded this image into Glance under the name <code>manila-service-image-master</code>.</p>

<p>Next I&rsquo;ve chosen the size of the machine used for the service instance with <code>service_instance_flavor_id = 103</code>.</p>

<p>The service instance is connected to two networks. The first network is called a <em>service network</em> and is created by Manila before booting up the service instance. Manila uses this network for the SSH access to the service instance. The second network is a <em>share network</em>. The NFS server managed by Manila is accessible on this network. In our case, because we have configured <code>connect_share_server_to_tenant_network = True</code>, the share network will directly map to one of our tenant networks.</p>

<p>Finally, we have to generate a public/private key pair and tell Manila about it using the options <code>path_to_public_key</code> and <code>path_to_private_key</code>. Manila will upload this keypair into Nova under the name <code>manila-service</code>. When creating the service instance, Nova injects the public key into the instance and so allows Manila the SSH access.</p>

<p>In order to make our generic backend available to the Manila users, we&rsquo;re going to define a <code>generic</code> share type next.</p>

<h2>Defining a share type</h2>

<p>The <em>share type</em> has a similar purpose as the <em>volume type</em> in Cinder. It defines the backend used for the share creation. If there are multiple share backends available, an OpenStack administrator can define a separate share type for each of them. When creating a new share, the user can choose which share type to allocate the storage from.</p>

<p>To create a <code>generic</code> share type that maps to our <code>generic</code> backend you can run the following commands as an OpenStack administrator:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>manila <span class="nb">type</span>-create generic True
</span><span class='line'>manila <span class="nb">type</span>-key generic <span class="nb">set </span><span class="nv">share_backend_name</span><span class="o">=</span>generic_backend
</span></code></pre></td></tr></table></div></figure></p>

<h2>Creating a share and mounting it</h2>

<p>Finally, we&rsquo;re done with all the configuration and can start enjoying our share service. All the following commands are run as an ordinary tenant user.</p>

<p>At first, we&rsquo;d like to create a share network and map it to one of our tenant networks:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>manila share-network-create <span class="p">&amp;</span>ndash<span class="p">;</span>neutron-net-id 4f179a8c-7068-4f0b-9be4-9cb11451b401 <span class="p">&amp;</span>ndash<span class="p">;</span>neutron-subnet-id c7d753b0-039b-4f8c-9e0f-012651ff4ada <span class="p">&amp;</span>ndash<span class="p">;</span>name management
</span></code></pre></td></tr></table></div></figure></p>

<p>Now we can create our first NFS share called <code>myshare</code> with the size 1 GB:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>manila create <span class="p">&amp;</span>ndash<span class="p">;</span>name myshare <span class="p">&amp;</span>ndash<span class="p">;</span>share-network management NFS 1
</span></code></pre></td></tr></table></div></figure></p>

<p>Creating the first share on a given tenant network takes longer as Manila has to spin up a new service instance in the background.</p>

<p>Eventually, the status of the share turns into <code>available</code> which means that the share is ready. The <code>manila show myshare</code> command will display the location from where we can mount the share. In our case, it is <code>10.13.243.173:/shares/share-b87367aa-3ef3-4282-a6b5-e45cab991b6c</code>. Before we can mount the share we have to allow access to it by modifying the access list:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>manila access-allow <span class="p">&amp;</span>ndash<span class="p">;</span>access_level rw myshare ip 10.13.244.12
</span></code></pre></td></tr></table></div></figure></p>

<p>The above command provides an instance having the IP address 10.13.244.12 with a read-write access to the share. Note that the IP addresses 10.13.243.173 and 10.13.244.12 belong to the same network. Finally, we can SSH into the instance and mount the share with:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo mount -t nfs 10.13.243.173:/shares/share-b87367aa-3ef3-4282-a6b5-e45cab991b6c /mnt
</span></code></pre></td></tr></table></div></figure></p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[TripleO Installer - the Good, the Bad and the Ugly]]></title>
    <link href="http://alesnosek.com/blog/2016/03/27/tripleo-installer-the-good/"/>
    <updated>2016-03-27T19:39:15-07:00</updated>
    <id>http://alesnosek.com/blog/2016/03/27/tripleo-installer-the-good</id>
    <content type="html"><![CDATA[<p><a href="https://wiki.openstack.org/wiki/TripleO">TripleO</a> is an OpenStack deployment and management tool I&rsquo;ve been using since the Kilo release of OpenStack. It does its job pretty well, however not everything is perfect. My experience presented in this article applies more or less to the Red Hat&rsquo;s OpenStack director too, as the Red Hat OpenStack director is a downstream version of TripleO.</p>

<!-- more -->


<h2>The good things about TripleO</h2>

<h3>TripleO is a great idea</h3>

<p>TripleO, aka OpenStack-on-OpenStack, installs OpenStack cluster using OpenStack. At first, a minimum one-node OpenStack installation is created which is in turn used to provision a much bigger workload OpenStack cluster. I find this TripleO idea amazing. If OpenStack is the best way to manage your infrastructure, then why use something else to install it? As an administrator I would prefer to provision my OpenStack nodes with Ironic before introducing yet another tool like <a href="http://cobbler.github.io/">Cobbler</a> to do the same job. Needless to say that as the Ironic and Heat components improve, so improves the OpenStack installation experience.</p>

<p>One could argue that using the OpenStack to form an installer comes with a ton of complexity when installing the installer itself. In my experience, however, the installation of the undercloud OpenStack using the provided Puppet scripts doesn&rsquo;t impose any problem.</p>

<h3>TripleO has a vibrant community</h3>

<p>TripleO is used to continuously deploy and test the OpenStack cloud during its development. The RDO project adopted TripleO as their OpenStack installation tool. Red Hat derives their OpenStack director installer from the RDO project. A large community of TripleO users is a great plus.</p>

<h2>The bad things about TripleO</h2>

<h3>Configuration flexibility</h3>

<p>TripleO installer consists of a bunch of Heat templates to orchestrate the overcloud image provisioning and a number of Puppet and shell scripts for the following configuration of the overcloud nodes. These templates and scripts are heavily developed from release to release as the new TripleO features come in. To avoid the upgrade headaches, you should not modify the TripleO templates and scripts directly. Instead, TripleO provides extension points (via extra config) where you can put your customizations. This didn&rsquo;t work for me. My goal was to deploy an Ironic service in the overcloud OpenStack. For that to work, I needed to provision an additional undercloud network including a VIP for the load balancer. This was not possible without patching the Heat templates and Puppet scripts. I dread the day when I&rsquo;ll have to port these patches to the next TripleO release.</p>

<p>Furthermore, the current way to modify OpenStack configuration properties is less straight forward. To configure a property, I have to first grep through the Puppet scripts to find out whether the desired property is managed by Puppet or not. Afterwards, I grep through the TripleO Heat templates to find out whether TripleO provides a direct template parameter to set the Puppet variable or not. Afterwards, I can either pass the parameter to the TripleO template or I set the Puppet variable in the extra config section or I&rsquo;m on my own.</p>

<p><blockquote><p>I&rsquo;d like to be able to easily modify any property in any configuration file on any OpenStack node.</p></blockquote></p>

<p>OpenStack comes with tons of configuration properties and I think it would be great to have a more straight forward way to configure them.</p>

<h3>Deployment control</h3>

<p>TripleO uses Heat to deploy and configure the overcloud OpenStack. Heat orchestrates the infrastracture based on the description provided by the user in the Heat templates. In the Heat templates, we tell Heat what our deployment should look like, but we have no control over the steps Heat will take to get to the desired state. I find this lack of control rather problematic.</p>

<p><blockquote><p>A fine-grained deployment control would be desirable.</p></blockquote></p>

<p>Let&rsquo;s say I have an overcloud consisting of 100 nodes. After changing the configuration in my Heat templates, I can only re-run the entire Heat configuration process and hope that I won&rsquo;t end up with a broken cloud. Instead, I&rsquo;d like to apply the configuration changes to a couple of nodes to make sure that everything works before I continue with the rest of the cloud. The ability to apply only part of the configuration would be useful as well.</p>

<h2>The ugly experience with TripleO</h2>

<p>I&rsquo;d like to share one scary experience I had with the TripleO installer. While using TripleO for a couple of months, I have to say that this was the only serious problem I&rsquo;ve encountered.</p>

<p>One day I uploaded an updated node image into the undercloud OpenStack. I was about to create new nodes in the overcloud cluster and wanted to have them provisioned with this new image. After starting the Heat stack update, it occurred to me that the processing took longer than usual. Well, after I SSHed into the overcloud nodes I realized why. Heat simply wiped out the entire disk content of the existing nodes and replaced it with the fresh disk image. Wow, my entire workload cloud was gone!</p>

<p>I learned that when you update the disk image in the undercloud, Heat will find out what nodes have to be updated and will simply replace their disk content with the new image. If you are orchestrating cloud deployments where your machines are cattle, this is what you want, however:</p>

<p><blockquote><p>The overcloud baremetal nodes are pets and should not be handled as cattle.</p></blockquote></p>

<p>To protect the overcloud nodes from deletion, I run the following command for each node against the undercloud Nova database:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sql'><span class='line'><span class="k">UPDATE</span> <span class="n">instances</span> <span class="k">SET</span> <span class="n">disable_terminate</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">WHERE</span> <span class="n">uuid</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">lsquo</span><span class="p">;</span><span class="o">&lt;</span><span class="n">uuid</span> <span class="k">of</span> <span class="n">the</span> <span class="n">overcloud</span> <span class="n">instance</span><span class="o">&gt;&amp;</span><span class="n">rsquo</span><span class="p">;;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>So far, I haven&rsquo;t found a better way how to do it. This effectively prevents deleting the node whether by issuing a <code>nova delete</code> command or by Heat when updating the stack.</p>

<h2>Conclusion and suggestions</h2>

<p>TripleO installer is a great tool to deploy an OpenStack cloud. It&rsquo;s backed by a large user community and doesn&rsquo;t invent any new tools to install OpenStack.</p>

<p>On the other hand, I&rsquo;m somewhat sceptical about Heat being the right tool to do software configuration. Funneling the configuration options through the Heat templates down to the Puppet scripts seems cumbersome to me.</p>

<p>I&rsquo;d like to suggest the following approach: let Heat do the node provisioning, network configuration and perhaps a minimum node setup using cloud-init. At the end of the deployment, Heat would provide the information about the deployment in the format understandable to the configuration management tools like Puppet, Chef or Ansible. The configuration management tool then merges the facts provided by Heat with the tons of OpenStack configuration settings provided by the user. The following OpenStack installation, configuration, and later orchestration would solely be done by the configuration management tool more suitable for this job. Heat would not be involved at all in this stage.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Bootstrapping a Galera Cluster on RHEL7]]></title>
    <link href="http://alesnosek.com/blog/2016/01/31/bootstrapping-a-galera-cluster-on-rhel7/"/>
    <updated>2016-01-31T15:24:55-08:00</updated>
    <id>http://alesnosek.com/blog/2016/01/31/bootstrapping-a-galera-cluster-on-rhel7</id>
    <content type="html"><![CDATA[<p>The MariaDB Galera packages provided by the RDO project in their OpenStack repositories don&rsquo;t seem to include a command or script to bootstrap the cluster. Let&rsquo;s look at an alternative way to bring the cluster up.</p>

<!-- more -->


<p>RHEL7 comes with the init system <code>systemd</code>. Unfortunately, systemd doesn&rsquo;t provide a way to pass command-line arguments to the unit files. Hence, doing something like this won&rsquo;t work:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@rhel1 ~<span class="o">]</span><span class="nv">$ </span>systemctl start mariadb <span class="p">&amp;</span>ndash<span class="p">;</span>wsrep_new_cluster
</span><span class='line'>systemctl: unrecognized option <span class="p">&amp;</span>lsquo<span class="p">;&amp;</span>ndash<span class="p">;</span>wsrep_new_cluster<span class="p">&amp;</span>rsquo<span class="p">;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Instead of passing command-line arguments, systemd allows for creating <a href="http://0pointer.de/blog/projects/instances.html">multiple instances</a> of the same service where each instance can obtain it&rsquo;s own set of environment variables. The Percona XtraDB Cluster includes the standard and the bootstrap service instance definitions in the RPM package <code>Percona-XtraDB-Cluster-server</code>. To boostrap the Percona cluster, the first node can be started with the following command:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@percona1 ~<span class="o">]</span><span class="nv">$ </span>systemctl start &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;&amp;#109;&amp;#97;&amp;#x69;&amp;#x6c;&amp;#x74;&amp;#x6f;&amp;#x3a;&amp;#x6d;&amp;#x79;&amp;#x73;&amp;#113;&amp;#108;&amp;#64;&amp;#x62;&amp;#x6f;&amp;#111;&amp;#116;&amp;#x73;&amp;#x74;&amp;#x72;&amp;#x61;&amp;#x70;&amp;#46;&amp;#115;&amp;#101;&amp;#x72;&amp;#118;&amp;#x69;&amp;#99;&amp;#101;&quot;</span>&gt;<span class="p">&amp;</span><span class="c">#x6d;&amp;#121;&amp;#115;&amp;#113;&amp;#x6c;&amp;#x40;&amp;#x62;&amp;#111;&amp;#111;&amp;#116;&amp;#115;&amp;#x74;&amp;#114;&amp;#97;&amp;#112;&amp;#46;&amp;#x73;&amp;#101;&amp;#x72;&amp;#118;&amp;#105;&amp;#99;&amp;#x65;&lt;/a&gt;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>At the moment, this boostrap service definition is missing in the RDO OpenStack packages. Before a similar <code>mysql@.service</code> script is available in RDO you can start the MariaDB Galera cluster as follows:</p>

<ul>
<li><p>On the first node, start the MariaDB with the <code>--wsrep-new-cluster</code> to create a new cluster:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@rhel1 ~<span class="o">]</span><span class="nv">$ </span>/usr/bin/mysqld_safe <span class="p">&amp;</span>ndash<span class="p">;</span>wsrep-new-cluster
</span></code></pre></td></tr></table></div></figure>
Let the command run in the foreground.</p></li>
<li><p>On the remaining cluster nodes start the mariadb service as usual:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@rhel2 ~<span class="o">]</span><span class="nv">$ </span>systemctl start mariadb
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>After the cluster has been fully formed, stop the mariadb on the first node by sending it a SIGQUIT (press CTRL + \ on the console).</p></li>
<li><p>On the first node, start the mariadb service via systemd:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@rhel1 ~<span class="o">]</span><span class="nv">$ </span>systemctl start mariadb
</span></code></pre></td></tr></table></div></figure></p></li>
</ul>


<p>That&rsquo;s it. You can check the status of each of the cluster nodes by running the following command:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@rhel1 ~<span class="o">]</span><span class="nv">$ </span>mysql -e <span class="p">&amp;</span>ldquo<span class="p">;</span>SHOW GLOBAL STATUS LIKE <span class="p">&amp;</span>lsquo<span class="p">;</span>wsrep%<span class="p">&amp;</span>rsquo<span class="p">;;&amp;</span>rdquo<span class="p">;</span>
</span></code></pre></td></tr></table></div></figure></p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Monitoring OpenStack Cluster with Icinga]]></title>
    <link href="http://alesnosek.com/blog/2015/11/30/monitoring-openstack-cluster-with-icinga/"/>
    <updated>2015-11-30T21:13:33-08:00</updated>
    <id>http://alesnosek.com/blog/2015/11/30/monitoring-openstack-cluster-with-icinga</id>
    <content type="html"><![CDATA[<p>If you don&rsquo;t monitor it, it&rsquo;s not in production! To get an OpenStack cloud ready for production, monitoring is a must. Let&rsquo;s take a look at two projects providing Nagios/Icinga plugins for checking the health of OpenStack services.</p>

<!-- more -->


<p>First, a few words about <a href="https://www.icinga.org/" title="Icinga">Icinga</a>. I started using Icinga 2 only recently and I&rsquo;m very pleased with this flexible and well-documented software. I&rsquo;ve listened to a German presentation about Icinga where they said that Icinga was not that widely spread in the US as it was the case in Europe. Dear Icinga team, you have one more happy user in the US now. Your software just works and your web GUI is beautiful.</p>

<p>I found two very useful projects for monitoring the OpenStack APIs both hosted on GitHub:</p>

<ul>
<li><a href="https://github.com/cirrax/openstack-nagios-plugins">OpenStack Nagios Plugins</a></li>
<li><a href="https://github.com/openstack/monitoring-for-openstack">Monitoring for OpenStack</a></li>
</ul>


<h2>OpenStack Nagios Plugins</h2>

<p><a href="https://github.com/cirrax/openstack-nagios-plugins">OpenStack Nagios Plugins</a> provides a collection of checks for the OpenStack services Nova, Neutron, Cinder, Keystone and Ceilometer. Available plugins worked right away with my OpenStack Liberty cluster. The Nova Hypervisor check monitors the &ldquo;virtual&rdquo; CPU and memory usage across your compute nodes. The name virtual CPU is a little misleading here. In reality, the number of physical cores is monitored as the Nova API actually reports the number of physical cores. I stick to the OpenStack default settings that overcommit the CPUs by factor of 16 and the memory by factor of 1.5. To accommodate this fact, I changed the warning and critical ranges for the check_nova-hypervisors plugin as follows:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>check_nova-hypervisors <span class="p">&amp;</span>ndash<span class="p">;</span>warn_memory_percent 0:135 <span class="p">&amp;</span>ndash<span class="p">;</span>critical_memory_percent 0:142 <span class="p">&amp;</span>ndash<span class="p">;</span>warn_vcpus_percent 0:1440 <span class="p">&amp;</span>ndash<span class="p">;</span>critical_vcpus_percent 0:1520
</span></code></pre></td></tr></table></div></figure></p>

<h2>Monitoring for OpenStack</h2>

<p>Plugins coming with the <a href="(https://github.com/openstack/monitoring-for-openstack">Monitoring for OpenStack</a> project provide deeper checks of OpenStack functionality. I liked the following ones the best:</p>

<ul>
<li><code>check_nova_instance</code>: Creates an instance on your cloud and deletes it again as soon as it is active. It&rsquo;s recommended to use a small disk image like cirros for this check.</li>
<li><code>cinder_volume</code>: Allocates a volume of size 1GB and deletes it again.</li>
<li><code>neutron_floating_ip</code>: Tries to allocate a floating IP. You have to configure the network where to allocate the IP from.</li>
<li><code>glance_upload</code>: Uploads 1MB of data as an image into Glance.</li>
<li><code>check_horizon_login</code>: Given a user name and a password the plugin will log into the Horizon dashboard.</li>
</ul>


<p>Some of the plugins didn&rsquo;t work for me due to incompatibilities with the Liberty client APIs. If you encounter the same problem you can try out my fixed version of the plugins on GitHub <a href="https://github.com/noseka1/monitoring-for-openstack">here</a>.</p>

<h2>Icinga 2 Screenshot</h2>

<p>And this is how the OpenStack APIs service group looks in Icinga Web 2. Happy monitoring!</p>

<p><img class="center" src="/images/posts/osmon.png"></p>
]]></content>
  </entry>

</feed>
