<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: development | Ales Nosek - The Software Practitioner]]></title>
  <link href="http://alesnosek.com/blog/categories/development/atom.xml" rel="self"/>
  <link href="http://alesnosek.com/"/>
  <updated>2018-05-05T16:24:18-07:00</updated>
  <id>http://alesnosek.com/</id>
  <author>
    <name><![CDATA[Ales Nosek]]></name>

  </author>
  <generator uri="http://octopress.org/">Octopress</generator>


  <entry>
    <title type="html"><![CDATA[Designing a Common Build System]]></title>
    <link href="http://alesnosek.com/blog/2018/05/03/designing-a-common-build-system/"/>
    <updated>2018-05-03T21:57:40-07:00</updated>
    <id>http://alesnosek.com/blog/2018/05/03/designing-a-common-build-system</id>
    <content type="html"><![CDATA[<p>Code reuse belongs to the basic tenets of software development. Moreover, one should have the same principle in mind when maintaining build scripts. If you are copy-pasting Makefiles and pom.xml files from project to project, stop now and read this article! We are going to discuss how to design a common build system.</p>

<!-- more -->


<p>A good software practitioner avoids having huge chunks of Makefiles, pom.xml files, build.xml files or shell scripts copy-pasted all over the code base. Based on experience, copy-pasted build scripts lead to inconsistencies, build issues and are overall driving the maintenance cost high.</p>

<p>In 2010, we invested heavily in the improvements of our build infrastructure. We introduced a continuous integration server Hudson (remember the project that was later renamed to Jenkins?), added source code analysis tool Sonar (nowadays called SonarQube), embraced RPM packaging and created a set of highly reusable build scripts which we called a common build system. In the next section, I&rsquo;m going to discuss a high-level design and ideas behind the common build system.</p>

<h2>High-level overview</h2>

<p>Our build system supports Java, C++ and C development. The core of the build system comprises of:</p>

<ul>
<li><a href="https://ant.apache.org/">Apache Ant</a> An old-timer between build tools. In current times, writing build scripts in XML is not sexy anymore, however, we like Ant for its simplicity and power. If you cannot express the required functionality using Ant tasks, you can always defer to using an embedded JavaScript. In our Ant scripts, you could find the <code>&lt;script language="javascript"&gt; ... &lt;/script&gt;</code> element that embeds JavaScript in several places.</li>
<li><a href="http://ant.apache.org/ivy/">Apache Ivy</a> Is a very flexible dependency manager that integrates with Apache Ant. While Ivy is predominantly used to manage Java jar files, we use it to manage C/C++ dependencies, too. For that, we zip up the C++ header files and push it along with the C++ shared libraries into the artifact repository.</li>
<li><a href="https://github.com/tumbarumba/AntScriptLibrary">Ant Script Library</a> Writing Ant build scripts from scratch is time consuming. To avoid spending this effort, we embraced Ant Script Library (ASL) which is a collection of re-usable Ant scripts providing a number of pre-defined targets.</li>
<li><a href="https://github.com/dmoulding/boilermake">Boilermake</a> Boilermake is a reusable GNU Make compatible Makefile. It uses a non-recursive strategy which avoids the many well-known pitfalls of recursive make, see also <a href="http://aegis.sourceforge.net/auug97.pdf">Recursive Make Considered Harmful</a>. We leverage Boilermake to compile C/C++ source code. An Ant <code>build.xml</code> wrapper script calls Boilermake when building a C/C++ module.</li>
</ul>


<p>The following diagram illustrates the organization of our code base:</p>

<p><img src="/images/posts/common_build_system.svg" width="600" height="600"></p>

<p>Our code base is divided up into modules. A module contains either Java or C/C++ source code required to build a library or executable. The <code>build-common</code> module is special in that it doesn&rsquo;t contain any source code to compile. Instead, it acts as a container in which we store all our reusable build scripts. The build scripts of other modules import the definitions from the <code>build-common</code> module. Due to high code reuse, the build scripts of individual modules are rather concise. The Ant statement to import the common definitions from the <code>build-common</code> module looks as follows:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>&lt;import <span class="nv">file</span><span class="o">=</span><span class="s2">&quot;../../Build/build-common/module.xml&quot;</span> /&gt;
</span></code></pre></td></tr></table></div></figure></p>

<p>Modules are organized into Git repositories according to the functionality they implement. For instance, modules of a specific product reside in its own Git repository. Furthermore, the <code>Platform</code> repository groups together modules that implement common libraries shared across several products.</p>

<p>All artifacts exported by the individual modules (jars, shared libraries, header files) are shared between the modules via the artifact repository. If additional information needs to be shared between modules, modules can publish Java properties files into the artifact repository which other modules can fetch and import. It was important to us to avoid any direct references between modules on the file system with the exception of the reference to the <code>build-common</code> module. These direct references between modules would be less obvious than passing artifacts and extra information via the artifact repository. We like to keep a good track of the dependencies between our software modules.</p>

<h2>Module directory structure</h2>

<p>Apache Ant does not propose any particular directory structure. However, it is easier to work with modules which have a common structure. Ant Script Library embraces the <a href="https://maven.apache.org/guides/introduction/introduction-to-the-standard-directory-layout.html">Standard Directory Layout</a> from Maven project and so we derive our directory structure from this standard. Here is our module directory structure in greater detail:</p>

<table>
<thead>
<tr>
<th style="text-align:left;"> Directory          </th>
<th style="text-align:left;"> Purpose                          </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> src/main/java      </td>
<td style="text-align:left;"> Java application/library sources </td>
</tr>
<tr>
<td style="text-align:left;"> src/main/c++       </td>
<td style="text-align:left;"> C++ application/library sources  </td>
</tr>
<tr>
<td style="text-align:left;"> src/main/c         </td>
<td style="text-align:left;"> C application/library sources    </td>
</tr>
<tr>
<td style="text-align:left;"> src/main/resources </td>
<td style="text-align:left;"> Application/library resources    </td>
</tr>
<tr>
<td style="text-align:left;"> src/main/scripts   </td>
<td style="text-align:left;"> Application/library scripts      </td>
</tr>
<tr>
<td style="text-align:left;"> src/test/java      </td>
<td style="text-align:left;"> Java test sources                </td>
</tr>
<tr>
<td style="text-align:left;"> src/test/c++       </td>
<td style="text-align:left;"> C++ test sources                 </td>
</tr>
<tr>
<td style="text-align:left;"> src/test/c         </td>
<td style="text-align:left;"> C test sources                   </td>
</tr>
<tr>
<td style="text-align:left;"> build.xml          </td>
<td style="text-align:left;"> Ant build file                   </td>
</tr>
<tr>
<td style="text-align:left;"> ivy.xml            </td>
<td style="text-align:left;"> Ivy module descriptor            </td>
</tr>
<tr>
<td style="text-align:left;"> README.md          </td>
<td style="text-align:left;"> Module&rsquo;s README file             </td>
</tr>
<tr>
<td style="text-align:left;"> target             </td>
<td style="text-align:left;"> Build output directory           </td>
</tr>
</tbody>
</table>


<h2>Common build targets</h2>

<p>We maintain a set of common Ant build targets which every module must implement:</p>

<table>
<thead>
<tr>
<th style="text-align:left;"> Target name               </th>
<th style="text-align:left;"> Description                                                                </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;"> default                   </td>
<td style="text-align:left;"> Build artifacts, publish artifacts                                         </td>
</tr>
<tr>
<td style="text-align:left;"> distribute                </td>
<td style="text-align:left;"> Build artifacts, publish artifacts and create a distribution package       </td>
</tr>
<tr>
<td style="text-align:left;"> all                       </td>
<td style="text-align:left;"> Build and test artifacts, publish artifacts, create a distribution package </td>
</tr>
<tr>
<td style="text-align:left;"> clean                     </td>
<td style="text-align:left;"> Delete files generated during the build                                    </td>
</tr>
<tr>
<td style="text-align:left;"> clean-dist                </td>
<td style="text-align:left;"> Delete files generated during the RPM package build                        </td>
</tr>
<tr>
<td style="text-align:left;"> clean-all                 </td>
<td style="text-align:left;"> Delete all generated files                                                 </td>
</tr>
<tr>
<td style="text-align:left;"> ci-default                </td>
<td style="text-align:left;"> Called by CI server during the build job execution                         </td>
</tr>
<tr>
<td style="text-align:left;"> report-sonar              </td>
<td style="text-align:left;"> Do statical analysis and send reports to Sonar server                      </td>
</tr>
</tbody>
</table>


<p>These build targets constitute a well-known interface which allows developers to clean and build any module using the same Ant command. Furthermore, each module defines the <code>ci-default</code> target which is called by the Jenkins CI server when building the module. This allows us to have Jenkins build any of our modules by issuing these two commands:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>ant clean-all
</span><span class='line'>ant ci-default
</span></code></pre></td></tr></table></div></figure></p>

<p>This two-command interface establishes a contract between modules and Jenkins and allows us to keep the Jenkins job definition fairly static. On the other hand, developers have full power to define what should happen during the build by implementing the module&rsquo;s  <code>ci-default</code> target.</p>

<p>Target <code>report-sonar</code> runs the SonarQube source code analysis tool, and sends the collected data to the SonarQube server. As the source code analysis takes some time to complete, we don&rsquo;t run it on every push to the Git repository. Instead, we scheduled a nightly Jenkins job that analyzes all the modules and uploads the collected data at once.</p>

<h2>Final remarks</h2>

<p>It has been several years since we created the common build system and we have been improving it ever since. We added many features to support our development process. We have already discussed some of them. Here is a summary of the most important capabilities we implemented so far:</p>

<ul>
<li>Support for building Java and C/C++ code</li>
<li>Support for multiple target platforms (RHEL5, RHEL6, RHEL7, Solaris 10)</li>
<li>Packaging software as RPM, Solaris pkg, IzPack and Docker image</li>
<li>Import of test data into Oracle database before running the unit tests</li>
<li>Management of build jobs in Jenkins</li>
<li>Source code analysis using SonarQube</li>
</ul>


<p>In our company, the relentless improvement process never stops. The Ant + Ivy tools we leverage at the core of our build system are past their prime. So, what&rsquo;s next? I&rsquo;m very excited about our current progress in gradually replacing Ant + Ivy with the more modern Gradle build tool.</p>

<p>I hope you enjoyed the tour through the design of the common build system. I would be interested to know how you promote code reuse of the build scripts in your company. It would be great to hear about your approach. Please, feel free to leave your comments in the comment section below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[AWS Lambda Adapter for Vert.x Web Applications]]></title>
    <link href="http://alesnosek.com/blog/2017/10/18/aws-lambda-adapter-for-vert-dot-x-web-applications/"/>
    <updated>2017-10-18T23:09:33-07:00</updated>
    <id>http://alesnosek.com/blog/2017/10/18/aws-lambda-adapter-for-vert-dot-x-web-applications</id>
    <content type="html"><![CDATA[<p><a href="http://vertx.io/">Vert.x</a> is an awesome tool-kit for developing reactive microservices. In this article, I&rsquo;m going to present an adapter that allows you to run your Vert.x web service as a Lambda function on AWS.</p>

<!-- more -->


<p><img class="right" src="/images/posts/vertx_logo.png" width="130" height="130"></p>

<p>If you are creating web services using Vert.x, you are leveraging the HTTP server built into the core of the Vert.x tool-kit. This HTTP server is based on the <a href="https://netty.io/">Netty</a> framework. When your web service comes up, the HTTP server opens a network port and starts listening for the incoming client connections. After a client connects, the HTTP server reads the HTTP request and processes this request by invoking callbacks that you registered. HTTP response generated by your implementation is returned back to the client. The web service remains running and processing requests until you shut it down.</p>

<p><img class="right" src="/images/posts/aws_lambda_logo.png" width="100" height="100"></p>

<p>In contrast to the constantly running web service, an AWS Lambda function is instantiated to serve a single HTTP request. After the HTTP request has been processed, the Lambda function is torn down.</p>

<p>In our company, we&rsquo;re looking at deploying our web services on-premise and in the cloud. We realized that when deploying into AWS it would be more cost effective if we could run some of our web services as Lambda functions. The question was, how to allow a Vert.x web service to alternatively run as a Lambda function? And how to accomplish this with a minimum development and maintenance effort?</p>

<p>After browsing through the Vert.x source code, it occurred to me that it would be possible to write a simple adapter that would convert HTTP requests and responses between the Lambda API and the Vert.x API. I then get on with the job and implemented such an adapter. And because software practitioners love open source, you can find this adapter along with the sample application on GitHub: <a href="https://github.com/noseka1/vertx-aws-lambda">vertx-aws-lambda</a>.</p>

<p>As always, I&rsquo;d love to hear your feedback. What do you think about this project? Feel free to leave your comments below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Evaluating Application Metrics Solutions - Prometheus?]]></title>
    <link href="http://alesnosek.com/blog/2017/09/10/evaluating-application-metrics-solutions-prometheus/"/>
    <updated>2017-09-10T16:17:49-07:00</updated>
    <id>http://alesnosek.com/blog/2017/09/10/evaluating-application-metrics-solutions-prometheus</id>
    <content type="html"><![CDATA[<p>In our <a href="https://en.wikipedia.org/wiki/Service-oriented_architecture">SOA-based</a> application, the problem of application metrics hasn&rsquo;t been solved yet. We would like to have our application services expose metrics that could be used for monitoring, auto-scaling and analytics. In this blog post, I would like to present to you one of the proposals to solve the application metrics which suggests leveraging <a href="https://prometheus.io/">Prometheus</a>.</p>

<!-- more -->


<p>Our application consists of multiple services that are deployed on multiple machines. Currently, our applicaton is deployed on-premise or as a managed offering on the virtual machines in AWS. We&rsquo;re also working on containerizing the application services to achieve higher density and better manageability when deploying into the AWS cloud. Some of our services are so light-weight that we&rsquo;re going to turn them into Lambda functions in the future to further reduce the operational costs. Thus, a solution for application metrics should be able to work in the serverless environment, too.</p>

<p>As of now, we deploy <a href="https://www.icinga.com/">Icinga</a> along with our application to provide system-level monitoring. Icinga collects the information about the nodes and checks that our services are still running. However, we don&rsquo;t collect any application-level metrics that would allow us to better assess the performance of our system. For example, we would like to know how many requests are processed per second, average request latency, request error rate, what are the depths of the internal queues and so forth. Application metrics would be a welcome input to the auto-scaling decisions and we would like to feed them into our analytics engine as well.</p>

<h2>Getting to know Prometheus</h2>

<p><img class="right" src="/images/posts/prometheus_logo.png" width="130" height="130"></p>

<p>Before jumping in and implementing our own solution for metrics collection and perhaps reinventing the wheel we started shopping around. It seemed to us, that Prometheus monitoring solution was gaining a lot of momentum in recent times. So, we took a closer look at Prometheus and this is what we found:</p>

<ol>
<li>Prometheus is an open-source monitoring solution hosted by <a href="https://www.cncf.io/">CNCF</a> - a foundation that hosts Kubernetes as well. Many companies use and contribute to Prometheus.</li>
<li>The <a href="https://prometheus.io/docs/introduction/overview/">architecture</a> of Prometheus is easy to understand and is modular. While Prometheus provides modules for metrics collection, alerts and Web UI, we would not have to use all of them.</li>
<li>Prometheus is a pull-based monitoring system. Each monitored target has to expose Prometheus formatted metrics. By default, targets make the metrics endpoint available at <a href="http://target/metrics.">http://target/metrics.</a> Prometheus periodically scrapes the metrics exposed by the targets.</li>
<li>Our services would need to expose the application metrics in the <a href="https://prometheus.io/docs/instrumenting/exposition_formats/">Prometheus format</a>. There are actually two formats available: a simple text format and protobufs. There are instrumentation <a href="https://prometheus.io/docs/instrumenting/clientlibs/">libraries</a> for Java, C++ and other languages, to gather the metrics and expose them in the Prometheus format.</li>
<li>The text-based Prometheus metrics format is so simple that it could be collected by other monitoring systems like Nagios or Icinga. Exposing metrics in the Prometheus format doesn&rsquo;t really mandate using Prometheus server for monitoring.</li>
<li>There&rsquo;s a Prometheus <a href="https://github.com/prometheus/jmx_exporter">jmx_exporter</a> library to convert the JMX MBeans data into Prometheus format. This would come in handy for gathering Tomcat metrics, for example.</li>
<li><a href="http://metrics.dropwizard.io/">Dropwizard metrics</a> is a popular Java instrumentation library. For instance, Vert.x toolkit can report its <a href="http://vertx.io/docs/vertx-dropwizard-metrics/java/">internal metrics</a> using the Dropwizard metrics library and there are other frameworks that supports it. Prometheus comes with a <a href="https://github.com/prometheus/client_java/tree/master/simpleclient_dropwizard">simpleclient_dropwizard</a> library that can make Dropwizard metrics available to Prometheus monitoring.</li>
<li>To prevent unauthorized access, the metric targets would need to be protected using TLS in combination with client certs, bearer token or HTTP basic authentication.</li>
<li>Prometheus pulls the metrics from the monitored targets. In addition, Prometheus comes with a <a href="https://prometheus.io/docs/practices/pushing/">Pushgateway</a> where clients can push their metrics to. However, as noted in the Prometheus documentation: <em>Usually, the only valid use case for the Pushgateway is for capturing the outcome of a service-level batch job</em>. Hence, Pushgateway would not work for aggregating metrics pushed by the Lambda functions.</li>
<li>In addition to application-level metrics, system-level metrics can be collected by Prometheus as well thanks to the <a href="https://github.com/prometheus/node_exporter">node_exporter</a>.</li>
<li>Prometheus is a great fit for dynamic environments like clouds and container clusters due to its discovery capabilities. In AWS, operator attaches tags to VMs and based on that Prometheus can discover them and start monitoring them automatically. The same principle works for container clusters like Kubernetes, too. One has to add annotations to pods and Prometheus will discover them automatically.</li>
<li>Prometheus makes the collected metrics available for querying via an <a href="https://prometheus.io/docs/querying/api/">HTTP API</a>. We could retrieve the metrics using this API in order to feed them into our analytics engine.</li>
<li>There is a great <a href="https://prometheus.io/docs/practices/naming/">guide</a> that would help us when designing our custom metrics.</li>
<li>Prometheus is written in Go and comes in a form of statically-linked binaries. This makes the installation of Prometheus a breeze.</li>
</ol>


<h2>Instrumenting Java applications</h2>

<p>In order to gather application metrics and to make them available to the Prometheus monitoring system, we would need to instrument our application services using Prometheus libraries. To get a clear idea, we created a proof-of-concept Java application instrumented using Prometheus. You can find it on <a href="https://github.com/noseka1/prometheus-poc">GitHub</a>.</p>

<p>Alternatively, we are thinking about leveraging Dropwizard metrics library for instrumentation. The Dropwizard metrics library is rather popular and is not connected with any particular monitoring solution. We would still be able to expose the Dropwizard metrics to Prometheus using a wrapper <a href="https://github.com/prometheus/client_java/tree/master/simpleclient_dropwizard">simpleclient_dropwizard</a>.</p>

<h2>Monitoring AWS Lambda functions</h2>

<p>AWS Lambda functions are extremely short-lived processes. Prometheus won&rsquo;t be able to pull the application metrics from them. Instead, Lambdas will have to push their metrics to Prometheus. At the first glance, we thought that the Prometheus Pushgateway could help here, however, reading the Pushgateway&rsquo;s documentation more carefully we found that <em>the Pushgateway is explicitly not an aggregator or distributed counter but rather a metrics cache</em>. And that&rsquo;s a problem, as we would like to count how many Lambda instances are being invoked per second and so on.</p>

<p>At the moment, we can see two approaches how to make the monitoring of Lambda functions work with Prometheus. Either, push the application metrics from the Lambda functions using a StatsD client. Prometheus&#8217; <a href="https://github.com/prometheus/statsd_exporter">statsd_exporter</a> would play a role of a StastD server and make the metrics available to Prometheus. Or, the second approach would be to create our own metrics aggregator that would receive the metrics from Lambda functions in the Prometheus format, aggregate them and expose them to the Prometheus server.</p>

<h2>Alternatives</h2>

<p>Besides using Prometheus, we were also thinking about other solutions for application metrics. As we already deploy Icinga for the system-level monitoring, it would make sense to use it for application metrics, too. We really like Icinga, it&rsquo;s a great monitoring software. Unfortunately, Icinga is based on the node and services model where a statically configured set of nodes are running services on them. This doesn&rsquo;t really fit with the modern containerized deployments where containers are dynamically scheduled on the cluster nodes and are also scaled up and down. Also, Prometheus server supports all sorts of metric queries and aggregations. Icinga is lacking this feature altogether. That&rsquo;s why we&rsquo;re leaning towards replacing Icinga with Prometheus for system-level as well as application-level monitoring.</p>

<p><a href="http://www.hawkular.org/">Hawkular</a> seems to be another modern monitoring project we would like to take a closer look at. In contrast to Prometheus project which is developed by many parties, it seems that Hawkular project is mostly driven by Red Hat.</p>

<h2>Conclusion</h2>

<p>Prometheus is a modern monitoring system. It was the first system we evaluated as we were trying to find a good solution for application metrics. In addition to application-level metrics, we could use Prometheus to collect system-level metrics as well. This would make Prometheus a single monitoring solution for our application. The only bigger issue for us is the absence of the AWS Lambda monitoring story.</p>

<p>If you have an application that you deliver on-premise as well as in the cloud, how did you solve the application metrics collection and monitoring? Is Prometheus a good way to go? Please, leave your comments below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Centralized Configuration Management, Our Approach]]></title>
    <link href="http://alesnosek.com/blog/2017/08/20/centralized-configuration-management/"/>
    <updated>2017-08-20T13:09:28-07:00</updated>
    <id>http://alesnosek.com/blog/2017/08/20/centralized-configuration-management</id>
    <content type="html"><![CDATA[<p>Are you migrating your existing application to the cloud? Are you missing a solution for centralized configuration management? Read on to learn, how we implemented centralized configuration management on top of our existing application and got it ready for the cloud.</p>

<!-- more -->


<p>Our existing application is a <a href="https://en.wikipedia.org/wiki/Service-oriented_architecture">SOA-based</a> application, i.e an application consisting of multiple services that communicate with each other and that are deployed across multiple nodes. Services are configured using configuration files located on the local filesystem. Currently, the operator has to edit multiple configuration files across multiple nodes by hand. In addition to that, for the sake of performance and redundancy, there are multiple instances of each service deployed behind a load balancer. This increases the configuration burden even further as the operator has to keep the configuration files consistent across several instances of the same service.</p>

<p>With the growing number of services and the need to deploy our application into dynamic cloud environments, a centralized configuration management became a necessity.</p>

<h2>Looking for a solution</h2>

<p>It would be possible to leverage the standard DevOps tools like Puppet, Chef or Ansible to manage the configuration files on each of the deployed nodes. For bare metal deployments or when deploying on virtual machines in the cloud, these tools could do a decent job. However, on our way to the cloud, weâ€™re looking at containerizing all of our services. Furthermore, down the road we would also like to leverage serverless architecture as well. For updating a handful of configuration files inside of a Docker container, Puppet, Chef or Ansible just seem too heavy. Needless to say that these tools would not be usable when considering serverless architecture.</p>

<p>When searching for a solution, we came across the <a href="https://github.com/kelseyhightower/confd">confd</a> and <a href="https://github.com/hashicorp/consul-template">consul-template</a> projects. Both tools are based on the same principle. First, the values of the configuration options are persisted in a backend store. While consul-template can store values in Consul only, confd supports a host of different backends like Consul, etcd, Redis or DynamoDB. Second, a set of templates on the filesystem is populated with the values from the backend store and hence forming valid configuration files. These configuration files are then consumed by the application. We drew a great deal of inspiration from this approach.</p>

<h2>Our approach</h2>

<p>Our centralized configuration management consists of two components: <em>CCS</em> (Centralized Configuration Store) which is a Consul cluster holding the configuration data, and <em>CCT</em> (Centralized Configuration Tool) which is a command-line client. CCT implements two functions. First, it allows the operator to query and modify the configuration values persisted in CCS. Second, it syncs up the configuration files on the local filesystem with their state in CCS. The following diagram depicts the components involved in the centralized configuration management:</p>

<p><img class="left" src="/images/posts/centralized_configuration_management.png"></p>

<p>In addition to the configuration values, the CCS component also stores all the additional data that is needed to completely recreate a given configuration file on the local filesystem. For instance, in the case of an ini file, CCS stores the absolute file path, file owner, file group, file mode, sections of the ini file, ini options with their values and all comment lines. Each ini option is also assigned a type or a set of allowed values and any configuration changes made by the operator are checked against the type information before they are accepted.</p>

<p>Apart from the ini file format, Java properties and XML files are also supported. The configuration management verifies that the XML file either conforms to a specific XML schema or is well-formed, before it is accepted. Lastly, all other configuration files that are not parsed by the configuration management are marked as &ldquo;unmanaged&rdquo; and the entire content of such a file is stored under a single key in the key-value store in Consul.</p>

<p>Next, let&rsquo;s review an example scenario where an operator wants to modify a configuration of a specific service. The individual steps are depicted in the diagram above:</p>

<ol>
<li><p>Using the CCT command-line client, the operator obtains a list of configuration files for a specific service managed by CCS. The operator uses CCT to edit the selected configuration file. CCT fetches all the data from CCS that are required to recreate the configuration file and presents it to the operator for editing (for example by opening the file in the operator&rsquo;s favorite editor).</p></li>
<li><p>After the operator made changes to the configuration file, the CCT parses the file to find out which values have been modified. The modified values are checked for corectness by CCT before they are saved in CCS.</p></li>
<li><p>Upon request, CCT fetches the configuration data from CCS in order to use it in the next step.</p></li>
<li><p>CCT syncs up the configuration files on the local filesystem with the data fetched from CCS.</p></li>
<li><p>The new configuration takes effect after the respective service has been restarted by the operator.</p></li>
</ol>


<p>Overall, CCS is a single source of truth for the application configuration. This is in contrast with the confd or consul-template approach where the configuration values are stored in the backend while the templates are stored on the filesystem. When a new release of the application is deployed, having all configuration data in one place makes the upgrade of the configuration data easier.</p>

<h2>Future directions</h2>

<p>It was important to us to introduce the centralized configuration management into our existing application without breaking the existing operational workflows. For example, operators should be able to edit the configuration files as they did in the previous versions of our application. Also, as the configuration files are written to the filesystem, the existing services continue to work without any modification from the previous versions. Hence the centralized configuration management can be deployed as a truly optional component on top of the existing application.</p>

<p>In the future, when some of our services will be deployed as (Lambda) functions in the serverless environment, those services will need to fetch their configuration by directly contacting CCS. However, nothing will change from the operator&rsquo;s standpoint. The operator will continue editing configuration files even when those won&rsquo;t exist on any filesystem anymore.</p>

<p>Do you use confd or consul-template to configure your application? Or did you build your own centralized configuration management? I would like to hear your comments. Feel free to use the comment section below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Git - Getting the Timing Right]]></title>
    <link href="http://alesnosek.com/blog/2017/01/02/git-getting-the-timing-right/"/>
    <updated>2017-01-02T16:50:48-08:00</updated>
    <id>http://alesnosek.com/blog/2017/01/02/git-getting-the-timing-right</id>
    <content type="html"><![CDATA[<p>Do you work on a development team that is distributed across several time zones? Got confused by the dates that Git shows in the commit logs? In this blog post we&rsquo;re going to review some basics about how Git deals with time.</p>

<!-- more -->


<h2>Pay attention to the commit timestamps</h2>

<p>Let&rsquo;s assume that two developers work in the same Git repository. The first developer Prasad is located in Bangalore, India. His colleague Joe is located in San Diego, US. The Git log they created looks as follows:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>git log
</span><span class='line'>commit 0f7a87b545d23870aa3dfe2665e242fd1a807445
</span><span class='line'>Author: Joe Smith &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;&amp;#x6d;&amp;#x61;&amp;#105;&amp;#108;&amp;#x74;&amp;#111;&amp;#x3a;&amp;#x6a;&amp;#x73;&amp;#109;&amp;#x69;&amp;#116;&amp;#104;&amp;#64;&amp;#115;&amp;#x61;&amp;#x6e;&amp;#x64;&amp;#x69;&amp;#101;&amp;#x67;&amp;#111;&amp;#46;&amp;#x75;&amp;#x73;&quot;</span>&gt;<span class="p">&amp;</span><span class="c">#106;&amp;#115;&amp;#109;&amp;#x69;&amp;#x74;&amp;#104;&amp;#64;&amp;#115;&amp;#97;&amp;#110;&amp;#100;&amp;#105;&amp;#101;&amp;#x67;&amp;#x6f;&amp;#46;&amp;#117;&amp;#115;&lt;/a&gt;</span>
</span><span class='line'>Date:   Tue Dec <span class="m">6</span> 11:41:44 <span class="m">2016</span> -0800&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;Commit 3
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;commit 62d95dfbc185ad60cd3ce2a4a3d02a12c1fb7dea
</span><span class='line'>Author: Prasad Gupta &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;&amp;#x6d;&amp;#x61;&amp;#105;&amp;#108;&amp;#116;&amp;#x6f;&amp;#x3a;&amp;#x70;&amp;#x67;&amp;#x75;&amp;#x70;&amp;#116;&amp;#97;&amp;#x40;&amp;#98;&amp;#97;&amp;#110;&amp;#x67;&amp;#x61;&amp;#108;&amp;#111;&amp;#x72;&amp;#x65;&amp;#46;&amp;#105;&amp;#x6e;&quot;</span>&gt;<span class="p">&amp;</span><span class="c">#x70;&amp;#x67;&amp;#x75;&amp;#x70;&amp;#x74;&amp;#97;&amp;#64;&amp;#x62;&amp;#x61;&amp;#x6e;&amp;#x67;&amp;#97;&amp;#108;&amp;#111;&amp;#114;&amp;#x65;&amp;#46;&amp;#105;&amp;#110;&lt;/a&gt;</span>
</span><span class='line'>Date:   Tue Dec <span class="m">6</span> 21:45:51 <span class="m">2016</span> +0530&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;Commit 2
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;commit 106db71e39e3b25fb3fa3df4c55cc8f063e78ff5
</span><span class='line'>Author: Prasad Gupta &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;&amp;#x6d;&amp;#x61;&amp;#105;&amp;#108;&amp;#116;&amp;#111;&amp;#58;&amp;#x70;&amp;#x67;&amp;#117;&amp;#x70;&amp;#116;&amp;#x61;&amp;#x40;&amp;#98;&amp;#x61;&amp;#110;&amp;#103;&amp;#x61;&amp;#108;&amp;#x6f;&amp;#114;&amp;#x65;&amp;#46;&amp;#105;&amp;#x6e;&quot;</span>&gt;<span class="p">&amp;</span><span class="c">#112;&amp;#x67;&amp;#x75;&amp;#112;&amp;#x74;&amp;#97;&amp;#x40;&amp;#x62;&amp;#x61;&amp;#x6e;&amp;#103;&amp;#x61;&amp;#x6c;&amp;#x6f;&amp;#114;&amp;#101;&amp;#46;&amp;#x69;&amp;#x6e;&lt;/a&gt;</span>
</span><span class='line'>Date:   Tue Dec <span class="m">6</span> 21:45:00 <span class="m">2016</span> +0530&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;Commit 1
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;
</span></code></pre></td></tr></table></div></figure></p>

<p>Git orders the commits based on the commit timestamps with the most recent commit shown on top. In the sample logs above you can see that all three commits were made on the same day Tuesday, December 6 2016. What might look a little bit odd is the order of the commits in the logs. <code>Commit 3</code> made at 11:41 should have actually appeared below the commits <code>Commit 1</code> and <code>Commit 2</code> made at 21:45, right? Wrong!</p>

<p>In the commit logs, Git displays the timestamp in the format <em>localtime + timezone offset</em>. When reading the timestamps it&rsquo;s important to take the timezone offset into account. Using the timezone offset you can convert the timestamp of the <code>Commit 2</code> and <code>Commit 3</code> into the UTC time in order to compare them. Because the <code>Commit 2</code> was made at 21:45 +0530 (= 16:15 UTC) and <code>Commit 3</code> was made at 11:41 -0800 (= 19:41 UTC) the <code>Commit 3</code> was created after the commits <code>Commit 1</code> and <code>Commit 2</code> and the chronological order displayed by Git is indeed correct.</p>

<h2>Check the time settings on the developer machines</h2>

<p>The timestamp recorded in the Git commit is based solely on the current time on the machine where the commit was created. Even if you have a corporate Git server where you push all your commits to you have to know that the Git server doesn&rsquo;t modify the timestamps in any way. You have to encourage your developers to have the time on their machines set correctly. This includes the correct local time as well as the time zone. On the Linux machines equipped with <a href="https://www.freedesktop.org/wiki/Software/systemd/">systemd</a> these time settings can be changed using the <code>timedatectl</code> command. Use <code>date</code> command to validate your settings:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>date
</span><span class='line'>Mon Jan  <span class="m">2</span> 20:33:04 PST 2017
</span></code></pre></td></tr></table></div></figure>
The output shows my correct local time and the correct time zone (PST) as I&rsquo;m located on the west coast of the US.</p>

<h2>Author date and commit date</h2>

<p>In the aforementioned example with the <code>git log</code> command I simplified the situation a little bit. There are actually two different timestamps recorded by Git for each commit: the <em>author date</em> and the <em>commit date</em>. When the commit is created both the timestamps are set to the current time of the machine where the commit was made. The author date denotes the time when the commit was originally made and it never changes. The commit date is updated every time the commit is being modified for example when rebasing or cherry-picking.</p>

<p>By default <code>git log</code> orders the logs according to the commit date, however, the author date is actually displayed in the output. This can easily lead to confusion when there are commits present for which the commit date and the author date actually differ. The parameter <code>--author-date-order</code> can be used to order the commits based on the author timestamp:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>git log <span class="p">&amp;</span>ndash<span class="p">;</span>author-date-order
</span></code></pre></td></tr></table></div></figure></p>
]]></content>
  </entry>

</feed>
