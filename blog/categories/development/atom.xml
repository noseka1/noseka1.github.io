<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: development | Ales Nosek - The Software Practitioner]]></title>
  <link href="http://alesnosek.com/blog/categories/development/atom.xml" rel="self"/>
  <link href="http://alesnosek.com/"/>
  <updated>2018-03-08T20:19:23-08:00</updated>
  <id>http://alesnosek.com/</id>
  <author>
    <name><![CDATA[Ales Nosek]]></name>

  </author>
  <generator uri="http://octopress.org/">Octopress</generator>


  <entry>
    <title type="html"><![CDATA[AWS Lambda Adapter for Vert.x Web Applications]]></title>
    <link href="http://alesnosek.com/blog/2017/10/18/aws-lambda-adapter-for-vert-dot-x-web-applications/"/>
    <updated>2017-10-18T23:09:33-07:00</updated>
    <id>http://alesnosek.com/blog/2017/10/18/aws-lambda-adapter-for-vert-dot-x-web-applications</id>
    <content type="html"><![CDATA[<p><a href="http://vertx.io/">Vert.x</a> is an awesome tool-kit for developing reactive microservices. In this article, I&rsquo;m going to present an adapter that allows you to run your Vert.x web service as a Lambda function on AWS.</p>

<!-- more -->


<p><img class="right" src="/images/posts/vertx_logo.png" width="130" height="130"></p>

<p>If you are creating web services using Vert.x, you are leveraging the HTTP server built into the core of the Vert.x tool-kit. This HTTP server is based on the <a href="https://netty.io/">Netty</a> framework. When your web service comes up, the HTTP server opens a network port and starts listening for the incoming client connections. After a client connects, the HTTP server reads the HTTP request and processes this request by invoking callbacks that you registered. HTTP response generated by your implementation is returned back to the client. The web service remains running and processing requests until you shut it down.</p>

<p><img class="right" src="/images/posts/aws_lambda_logo.png" width="100" height="100"></p>

<p>In contrast to the constantly running web service, an AWS Lambda function is instantiated to serve a single HTTP request. After the HTTP request has been processed, the Lambda function is torn down.</p>

<p>In our company, we&rsquo;re looking at deploying our web services on-premise and in the cloud. We realized that when deploying into AWS it would be more cost effective if we could run some of our web services as Lambda functions. The question was, how to allow a Vert.x web service to alternatively run as a Lambda function? And how to accomplish this with a minimum development and maintenance effort?</p>

<p>After browsing through the Vert.x source code, it occurred to me that it would be possible to write a simple adapter that would convert HTTP requests and responses between the Lambda API and the Vert.x API. I then get on with the job and implemented such an adapter. And because software practitioners love open source, you can find this adapter along with the sample application on GitHub: <a href="https://github.com/noseka1/vertx-aws-lambda">vertx-aws-lambda</a>.</p>

<p>As always, I&rsquo;d love to hear your feedback. What do you think about this project? Feel free to leave your comments below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Evaluating Application Metrics Solutions - Prometheus?]]></title>
    <link href="http://alesnosek.com/blog/2017/09/10/evaluating-application-metrics-solutions-prometheus/"/>
    <updated>2017-09-10T16:17:49-07:00</updated>
    <id>http://alesnosek.com/blog/2017/09/10/evaluating-application-metrics-solutions-prometheus</id>
    <content type="html"><![CDATA[<p>In our <a href="https://en.wikipedia.org/wiki/Service-oriented_architecture">SOA-based</a> application, the problem of application metrics hasn&rsquo;t been solved yet. We would like to have our application services expose metrics that could be used for monitoring, auto-scaling and analytics. In this blog post, I would like to present to you one of the proposals to solve the application metrics which suggests leveraging <a href="https://prometheus.io/">Prometheus</a>.</p>

<!-- more -->


<p>Our application consists of multiple services that are deployed on multiple machines. Currently, our applicaton is deployed on-premise or as a managed offering on the virtual machines in AWS. We&rsquo;re also working on containerizing the application services to achieve higher density and better manageability when deploying into the AWS cloud. Some of our services are so light-weight that we&rsquo;re going to turn them into Lambda functions in the future to further reduce the operational costs. Thus, a solution for application metrics should be able to work in the serverless environment, too.</p>

<p>As of now, we deploy <a href="https://www.icinga.com/">Icinga</a> along with our application to provide system-level monitoring. Icinga collects the information about the nodes and checks that our services are still running. However, we don&rsquo;t collect any application-level metrics that would allow us to better assess the performance of our system. For example, we would like to know how many requests are processed per second, average request latency, request error rate, what are the depths of the internal queues and so forth. Application metrics would be a welcome input to the auto-scaling decisions and we would like to feed them into our analytics engine as well.</p>

<h2>Getting to know Prometheus</h2>

<p><img class="right" src="/images/posts/prometheus_logo.png" width="130" height="130"></p>

<p>Before jumping in and implementing our own solution for metrics collection and perhaps reinventing the wheel we started shopping around. It seemed to us, that Prometheus monitoring solution was gaining a lot of momentum in recent times. So, we took a closer look at Prometheus and this is what we found:</p>

<ol>
<li>Prometheus is an open-source monitoring solution hosted by <a href="https://www.cncf.io/">CNCF</a> - a foundation that hosts Kubernetes as well. Many companies use and contribute to Prometheus.</li>
<li>The <a href="https://prometheus.io/docs/introduction/overview/">architecture</a> of Prometheus is easy to understand and is modular. While Prometheus provides modules for metrics collection, alerts and Web UI, we would not have to use all of them.</li>
<li>Prometheus is a pull-based monitoring system. Each monitored target has to expose Prometheus formatted metrics. By default, targets make the metrics endpoint available at <a href="http://target/metrics.">http://target/metrics.</a> Prometheus periodically scrapes the metrics exposed by the targets.</li>
<li>Our services would need to expose the application metrics in the <a href="https://prometheus.io/docs/instrumenting/exposition_formats/">Prometheus format</a>. There are actually two formats available: a simple text format and protobufs. There are instrumentation <a href="https://prometheus.io/docs/instrumenting/clientlibs/">libraries</a> for Java, C++ and other languages, to gather the metrics and expose them in the Prometheus format.</li>
<li>The text-based Prometheus metrics format is so simple that it could be collected by other monitoring systems like Nagios or Icinga. Exposing metrics in the Prometheus format doesn&rsquo;t really mandate using Prometheus server for monitoring.</li>
<li>There&rsquo;s a Prometheus <a href="https://github.com/prometheus/jmx_exporter">jmx_exporter</a> library to convert the JMX MBeans data into Prometheus format. This would come in handy for gathering Tomcat metrics, for example.</li>
<li><a href="http://metrics.dropwizard.io/">Dropwizard metrics</a> is a popular Java instrumentation library. For instance, Vert.x toolkit can report its <a href="http://vertx.io/docs/vertx-dropwizard-metrics/java/">internal metrics</a> using the Dropwizard metrics library and there are other frameworks that supports it. Prometheus comes with a <a href="https://github.com/prometheus/client_java/tree/master/simpleclient_dropwizard">simpleclient_dropwizard</a> library that can make Dropwizard metrics available to Prometheus monitoring.</li>
<li>To prevent unauthorized access, the metric targets would need to be protected using TLS in combination with client certs, bearer token or HTTP basic authentication.</li>
<li>Prometheus pulls the metrics from the monitored targets. In addition, Prometheus comes with a <a href="https://prometheus.io/docs/practices/pushing/">Pushgateway</a> where clients can push their metrics to. However, as noted in the Prometheus documentation: <em>Usually, the only valid use case for the Pushgateway is for capturing the outcome of a service-level batch job</em>. Hence, Pushgateway would not work for aggregating metrics pushed by the Lambda functions.</li>
<li>In addition to application-level metrics, system-level metrics can be collected by Prometheus as well thanks to the <a href="https://github.com/prometheus/node_exporter">node_exporter</a>.</li>
<li>Prometheus is a great fit for dynamic environments like clouds and container clusters due to its discovery capabilities. In AWS, operator attaches tags to VMs and based on that Prometheus can discover them and start monitoring them automatically. The same principle works for container clusters like Kubernetes, too. One has to add annotations to pods and Prometheus will discover them automatically.</li>
<li>Prometheus makes the collected metrics available for querying via an <a href="https://prometheus.io/docs/querying/api/">HTTP API</a>. We could retrieve the metrics using this API in order to feed them into our analytics engine.</li>
<li>There is a great <a href="https://prometheus.io/docs/practices/naming/">guide</a> that would help us when designing our custom metrics.</li>
<li>Prometheus is written in Go and comes in a form of statically-linked binaries. This makes the installation of Prometheus a breeze.</li>
</ol>


<h2>Instrumenting Java applications</h2>

<p>In order to gather application metrics and to make them available to the Prometheus monitoring system, we would need to instrument our application services using Prometheus libraries. To get a clear idea, we created a proof-of-concept Java application instrumented using Prometheus. You can find it on <a href="https://github.com/noseka1/prometheus-poc">GitHub</a>.</p>

<p>Alternatively, we are thinking about leveraging Dropwizard metrics library for instrumentation. The Dropwizard metrics library is rather popular and is not connected with any particular monitoring solution. We would still be able to expose the Dropwizard metrics to Prometheus using a wrapper <a href="https://github.com/prometheus/client_java/tree/master/simpleclient_dropwizard">simpleclient_dropwizard</a>.</p>

<h2>Monitoring AWS Lambda functions</h2>

<p>AWS Lambda functions are extremely short-lived processes. Prometheus won&rsquo;t be able to pull the application metrics from them. Instead, Lambdas will have to push their metrics to Prometheus. At the first glance, we thought that the Prometheus Pushgateway could help here, however, reading the Pushgateway&rsquo;s documentation more carefully we found that <em>the Pushgateway is explicitly not an aggregator or distributed counter but rather a metrics cache</em>. And that&rsquo;s a problem, as we would like to count how many Lambda instances are being invoked per second and so on.</p>

<p>At the moment, we can see two approaches how to make the monitoring of Lambda functions work with Prometheus. Either, push the application metrics from the Lambda functions using a StatsD client. Prometheus&#8217; <a href="https://github.com/prometheus/statsd_exporter">statsd_exporter</a> would play a role of a StastD server and make the metrics available to Prometheus. Or, the second approach would be to create our own metrics aggregator that would receive the metrics from Lambda functions in the Prometheus format, aggregate them and expose them to the Prometheus server.</p>

<h2>Alternatives</h2>

<p>Besides using Prometheus, we were also thinking about other solutions for application metrics. As we already deploy Icinga for the system-level monitoring, it would make sense to use it for application metrics, too. We really like Icinga, it&rsquo;s a great monitoring software. Unfortunately, Icinga is based on the node and services model where a statically configured set of nodes are running services on them. This doesn&rsquo;t really fit with the modern containerized deployments where containers are dynamically scheduled on the cluster nodes and are also scaled up and down. Also, Prometheus server supports all sorts of metric queries and aggregations. Icinga is lacking this feature altogether. That&rsquo;s why we&rsquo;re leaning towards replacing Icinga with Prometheus for system-level as well as application-level monitoring.</p>

<p><a href="http://www.hawkular.org/">Hawkular</a> seems to be another modern monitoring project we would like to take a closer look at. In contrast to Prometheus project which is developed by many parties, it seems that Hawkular project is mostly driven by Red Hat.</p>

<h2>Conclusion</h2>

<p>Prometheus is a modern monitoring system. It was the first system we evaluated as we were trying to find a good solution for application metrics. In addition to application-level metrics, we could use Prometheus to collect system-level metrics as well. This would make Prometheus a single monitoring solution for our application. The only bigger issue for us is the absence of the AWS Lambda monitoring story.</p>

<p>If you have an application that you deliver on-premise as well as in the cloud, how did you solve the application metrics collection and monitoring? Is Prometheus a good way to go? Please, leave your comments below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Centralized Configuration Management, Our Approach]]></title>
    <link href="http://alesnosek.com/blog/2017/08/20/centralized-configuration-management/"/>
    <updated>2017-08-20T13:09:28-07:00</updated>
    <id>http://alesnosek.com/blog/2017/08/20/centralized-configuration-management</id>
    <content type="html"><![CDATA[<p>Are you migrating your existing application to the cloud? Are you missing a solution for centralized configuration management? Read on to learn, how we implemented centralized configuration management on top of our existing application and got it ready for the cloud.</p>

<!-- more -->


<p>Our existing application is a <a href="https://en.wikipedia.org/wiki/Service-oriented_architecture">SOA-based</a> application, i.e an application consisting of multiple services that communicate with each other and that are deployed across multiple nodes. Services are configured using configuration files located on the local filesystem. Currently, the operator has to edit multiple configuration files across multiple nodes by hand. In addition to that, for the sake of performance and redundancy, there are multiple instances of each service deployed behind a load balancer. This increases the configuration burden even further as the operator has to keep the configuration files consistent across several instances of the same service.</p>

<p>With the growing number of services and the need to deploy our application into dynamic cloud environments, a centralized configuration management became a necessity.</p>

<h2>Looking for a solution</h2>

<p>It would be possible to leverage the standard DevOps tools like Puppet, Chef or Ansible to manage the configuration files on each of the deployed nodes. For bare metal deployments or when deploying on virtual machines in the cloud, these tools could do a decent job. However, on our way to the cloud, we’re looking at containerizing all of our services. Furthermore, down the road we would also like to leverage serverless architecture as well. For updating a handful of configuration files inside of a Docker container, Puppet, Chef or Ansible just seem too heavy. Needless to say that these tools would not be usable when considering serverless architecture.</p>

<p>When searching for a solution, we came across the <a href="https://github.com/kelseyhightower/confd">confd</a> and <a href="https://github.com/hashicorp/consul-template">consul-template</a> projects. Both tools are based on the same principle. First, the values of the configuration options are persisted in a backend store. While consul-template can store values in Consul only, confd supports a host of different backends like Consul, etcd, Redis or DynamoDB. Second, a set of templates on the filesystem is populated with the values from the backend store and hence forming valid configuration files. These configuration files are then consumed by the application. We drew a great deal of inspiration from this approach.</p>

<h2>Our approach</h2>

<p>Our centralized configuration management consists of two components: <em>CCS</em> (Centralized Configuration Store) which is a Consul cluster holding the configuration data, and <em>CCT</em> (Centralized Configuration Tool) which is a command-line client. CCT implements two functions. First, it allows the operator to query and modify the configuration values persisted in CCS. Second, it syncs up the configuration files on the local filesystem with their state in CCS. The following diagram depicts the components involved in the centralized configuration management:</p>

<p><img class="left" src="/images/posts/centralized_configuration_management.png"></p>

<p>In addition to the configuration values, the CCS component also stores all the additional data that is needed to completely recreate a given configuration file on the local filesystem. For instance, in the case of an ini file, CCS stores the absolute file path, file owner, file group, file mode, sections of the ini file, ini options with their values and all comment lines. Each ini option is also assigned a type or a set of allowed values and any configuration changes made by the operator are checked against the type information before they are accepted.</p>

<p>Apart from the ini file format, Java properties and XML files are also supported. The configuration management verifies that the XML file either conforms to a specific XML schema or is well-formed, before it is accepted. Lastly, all other configuration files that are not parsed by the configuration management are marked as &ldquo;unmanaged&rdquo; and the entire content of such a file is stored under a single key in the key-value store in Consul.</p>

<p>Next, let&rsquo;s review an example scenario where an operator wants to modify a configuration of a specific service. The individual steps are depicted in the diagram above:</p>

<ol>
<li><p>Using the CCT command-line client, the operator obtains a list of configuration files for a specific service managed by CCS. The operator uses CCT to edit the selected configuration file. CCT fetches all the data from CCS that are required to recreate the configuration file and presents it to the operator for editing (for example by opening the file in the operator&rsquo;s favorite editor).</p></li>
<li><p>After the operator made changes to the configuration file, the CCT parses the file to find out which values have been modified. The modified values are checked for corectness by CCT before they are saved in CCS.</p></li>
<li><p>Upon request, CCT fetches the configuration data from CCS in order to use it in the next step.</p></li>
<li><p>CCT syncs up the configuration files on the local filesystem with the data fetched from CCS.</p></li>
<li><p>The new configuration takes effect after the respective service has been restarted by the operator.</p></li>
</ol>


<p>Overall, CCS is a single source of truth for the application configuration. This is in contrast with the confd or consul-template approach where the configuration values are stored in the backend while the templates are stored on the filesystem. When a new release of the application is deployed, having all configuration data in one place makes the upgrade of the configuration data easier.</p>

<h2>Future directions</h2>

<p>It was important to us to introduce the centralized configuration management into our existing application without breaking the existing operational workflows. For example, operators should be able to edit the configuration files as they did in the previous versions of our application. Also, as the configuration files are written to the filesystem, the existing services continue to work without any modification from the previous versions. Hence the centralized configuration management can be deployed as a truly optional component on top of the existing application.</p>

<p>In the future, when some of our services will be deployed as (Lambda) functions in the serverless environment, those services will need to fetch their configuration by directly contacting CCS. However, nothing will change from the operator&rsquo;s standpoint. The operator will continue editing configuration files even when those won&rsquo;t exist on any filesystem anymore.</p>

<p>Do you use confd or consul-template to configure your application? Or did you build your own centralized configuration management? I would like to hear your comments. Feel free to use the comment section below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Git - Getting the Timing Right]]></title>
    <link href="http://alesnosek.com/blog/2017/01/02/git-getting-the-timing-right/"/>
    <updated>2017-01-02T16:50:48-08:00</updated>
    <id>http://alesnosek.com/blog/2017/01/02/git-getting-the-timing-right</id>
    <content type="html"><![CDATA[<p>Do you work on a development team that is distributed across several time zones? Got confused by the dates that Git shows in the commit logs? In this blog post we&rsquo;re going to review some basics about how Git deals with time.</p>

<!-- more -->


<h2>Pay attention to the commit timestamps</h2>

<p>Let&rsquo;s assume that two developers work in the same Git repository. The first developer Prasad is located in Bangalore, India. His colleague Joe is located in San Diego, US. The Git log they created looks as follows:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>git log
</span><span class='line'>commit 0f7a87b545d23870aa3dfe2665e242fd1a807445
</span><span class='line'>Author: Joe Smith &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;&amp;#x6d;&amp;#x61;&amp;#105;&amp;#x6c;&amp;#x74;&amp;#111;&amp;#x3a;&amp;#106;&amp;#115;&amp;#x6d;&amp;#105;&amp;#x74;&amp;#x68;&amp;#64;&amp;#115;&amp;#x61;&amp;#x6e;&amp;#x64;&amp;#x69;&amp;#101;&amp;#103;&amp;#x6f;&amp;#x2e;&amp;#117;&amp;#x73;&quot;</span>&gt;<span class="p">&amp;</span><span class="c">#106;&amp;#115;&amp;#109;&amp;#x69;&amp;#116;&amp;#104;&amp;#64;&amp;#115;&amp;#97;&amp;#x6e;&amp;#x64;&amp;#105;&amp;#101;&amp;#103;&amp;#x6f;&amp;#x2e;&amp;#117;&amp;#115;&lt;/a&gt;</span>
</span><span class='line'>Date:   Tue Dec <span class="m">6</span> 11:41:44 <span class="m">2016</span> -0800&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;Commit 3
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;commit 62d95dfbc185ad60cd3ce2a4a3d02a12c1fb7dea
</span><span class='line'>Author: Prasad Gupta &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;&amp;#109;&amp;#97;&amp;#105;&amp;#108;&amp;#116;&amp;#x6f;&amp;#x3a;&amp;#112;&amp;#x67;&amp;#117;&amp;#x70;&amp;#x74;&amp;#97;&amp;#64;&amp;#x62;&amp;#97;&amp;#x6e;&amp;#103;&amp;#97;&amp;#x6c;&amp;#111;&amp;#114;&amp;#101;&amp;#x2e;&amp;#x69;&amp;#110;&quot;</span>&gt;<span class="p">&amp;</span><span class="c">#x70;&amp;#103;&amp;#117;&amp;#x70;&amp;#116;&amp;#97;&amp;#64;&amp;#98;&amp;#97;&amp;#110;&amp;#103;&amp;#x61;&amp;#108;&amp;#x6f;&amp;#114;&amp;#x65;&amp;#x2e;&amp;#105;&amp;#x6e;&lt;/a&gt;</span>
</span><span class='line'>Date:   Tue Dec <span class="m">6</span> 21:45:51 <span class="m">2016</span> +0530&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;Commit 2
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;commit 106db71e39e3b25fb3fa3df4c55cc8f063e78ff5
</span><span class='line'>Author: Prasad Gupta &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;&amp;#109;&amp;#97;&amp;#105;&amp;#108;&amp;#x74;&amp;#x6f;&amp;#58;&amp;#x70;&amp;#x67;&amp;#117;&amp;#112;&amp;#116;&amp;#x61;&amp;#64;&amp;#98;&amp;#x61;&amp;#110;&amp;#103;&amp;#x61;&amp;#x6c;&amp;#x6f;&amp;#x72;&amp;#x65;&amp;#46;&amp;#x69;&amp;#110;&quot;</span>&gt;<span class="p">&amp;</span><span class="c">#112;&amp;#103;&amp;#x75;&amp;#112;&amp;#116;&amp;#97;&amp;#x40;&amp;#98;&amp;#97;&amp;#110;&amp;#103;&amp;#x61;&amp;#x6c;&amp;#x6f;&amp;#x72;&amp;#101;&amp;#x2e;&amp;#105;&amp;#110;&lt;/a&gt;</span>
</span><span class='line'>Date:   Tue Dec <span class="m">6</span> 21:45:00 <span class="m">2016</span> +0530&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;Commit 1
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;
</span></code></pre></td></tr></table></div></figure></p>

<p>Git orders the commits based on the commit timestamps with the most recent commit shown on top. In the sample logs above you can see that all three commits were made on the same day Tuesday, December 6 2016. What might look a little bit odd is the order of the commits in the logs. <code>Commit 3</code> made at 11:41 should have actually appeared below the commits <code>Commit 1</code> and <code>Commit 2</code> made at 21:45, right? Wrong!</p>

<p>In the commit logs, Git displays the timestamp in the format <em>localtime + timezone offset</em>. When reading the timestamps it&rsquo;s important to take the timezone offset into account. Using the timezone offset you can convert the timestamp of the <code>Commit 2</code> and <code>Commit 3</code> into the UTC time in order to compare them. Because the <code>Commit 2</code> was made at 21:45 +0530 (= 16:15 UTC) and <code>Commit 3</code> was made at 11:41 -0800 (= 19:41 UTC) the <code>Commit 3</code> was created after the commits <code>Commit 1</code> and <code>Commit 2</code> and the chronological order displayed by Git is indeed correct.</p>

<h2>Check the time settings on the developer machines</h2>

<p>The timestamp recorded in the Git commit is based solely on the current time on the machine where the commit was created. Even if you have a corporate Git server where you push all your commits to you have to know that the Git server doesn&rsquo;t modify the timestamps in any way. You have to encourage your developers to have the time on their machines set correctly. This includes the correct local time as well as the time zone. On the Linux machines equipped with <a href="https://www.freedesktop.org/wiki/Software/systemd/">systemd</a> these time settings can be changed using the <code>timedatectl</code> command. Use <code>date</code> command to validate your settings:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>date
</span><span class='line'>Mon Jan  <span class="m">2</span> 20:33:04 PST 2017
</span></code></pre></td></tr></table></div></figure>
The output shows my correct local time and the correct time zone (PST) as I&rsquo;m located on the west coast of the US.</p>

<h2>Author date and commit date</h2>

<p>In the aforementioned example with the <code>git log</code> command I simplified the situation a little bit. There are actually two different timestamps recorded by Git for each commit: the <em>author date</em> and the <em>commit date</em>. When the commit is created both the timestamps are set to the current time of the machine where the commit was made. The author date denotes the time when the commit was originally made and it never changes. The commit date is updated every time the commit is being modified for example when rebasing or cherry-picking.</p>

<p>By default <code>git log</code> orders the logs according to the commit date, however, the author date is actually displayed in the output. This can easily lead to confusion when there are commits present for which the commit date and the author date actually differ. The parameter <code>--author-date-order</code> can be used to order the commits based on the author timestamp:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>git log <span class="p">&amp;</span>ndash<span class="p">;</span>author-date-order
</span></code></pre></td></tr></table></div></figure></p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Browsing Docker Source using Eclipse]]></title>
    <link href="http://alesnosek.com/blog/2015/07/19/browsing-docker-source-using-eclipse/"/>
    <updated>2015-07-19T20:59:12-07:00</updated>
    <id>http://alesnosek.com/blog/2015/07/19/browsing-docker-source-using-eclipse</id>
    <content type="html"><![CDATA[<p>Do you like Docker technology and want to learn more about it? There&rsquo;s no better way to learn than reading the source code. In this article, we&rsquo;ll install the Go programming language, download the latest Docker source code and navigate through it in Eclipse.</p>

<!-- more -->


<h2>Installing the Go programming language</h2>

<p>The Go programming language is relatively young and sees a lot of development. Within the past two years there was a major release available every half a year. To keep up with the latest state of art it&rsquo;s better to install Go packages directly from the project&rsquo;s <a href="https://golang.org/dl/" title="Go Downloads">download site</a> instead of trying to make use of the packages coming with your Linux distribution. Currently, Docker requires Go version 1.4 or later. Before you start the installation, make sure that there are no Go packages installed on your system. The following command will uninstall all Go packages from your Debian-based Linux:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo apt-get purge golang*
</span></code></pre></td></tr></table></div></figure></p>

<p>After you&rsquo;ve downloaded the Go binary distribution tarball your can install it with:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo tar -C /usr/local -xzf go1.4.2.linux-amd64.tar.gz
</span></code></pre></td></tr></table></div></figure></p>

<p>This will extract the archive into the <code>/usr/local/go</code> directory. The Go distributions assume they will be installed in <code>/usr/local/go</code>. If you install Go into a different location you&rsquo;ll have to set the <code>GOROOT</code> environment variable. For more information on the Go installation refer to the Go&rsquo;s <a href="http://golang.org/doc/install" title="Getting Started">Getting Started</a> page.</p>

<p>The <code>/usr/local/go/bin</code> includes the Go tool (the <code>go</code> command). We&rsquo;ll use this tool to download and compile Docker. You want to have the Go tool available on your path:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$PATH</span>:/usr/local/go/bin
</span></code></pre></td></tr></table></div></figure></p>

<h2>Creating a Go workspace</h2>

<p>The Go <em>workspace</em> is a directory with three subdirectories:</p>

<ul>
<li><code>src</code> is where the source code resides</li>
<li><code>pkg</code> is where the libraries are stored</li>
<li><code>bin</code> is where the executables reside</li>
</ul>


<p>Typically, Go programmers keep <em>all</em> their source code and dependencies (libraries) in a single workspace. It means that all your Go projects are located in a single workspace.</p>

<p>The workspace can be created at an arbitrary location. In order for Go tool to find the available workspaces, you must list them in the <code>GOPATH</code> environment variable. We&rsquo;ll define a single workspace under the current user&rsquo;s home directory:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nb">export </span><span class="nv">GOPATH</span><span class="o">=</span><span class="nv">$HOME</span>/go
</span><span class='line'><span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$PATH</span>:<span class="nv">$GOPATH</span>/bin
</span></code></pre></td></tr></table></div></figure></p>

<p>At the same time, we&rsquo;ve included the workspace&rsquo;s <code>bin</code> directory into our path variable. Whenever we build an executable it&rsquo;ll be instantly available for us to use. Note that the workspace directory doesn&rsquo;t exist yet. Go tool will automatically create it when we download the Docker source code.</p>

<p>Furher information on the Go code organization and workspaces can be found <a href="http://golang.org/doc/code.html" title="How to Write Go Code">here</a>.</p>

<h2>Building Docker from source</h2>

<p>So far, we&rsquo;ve defined the location of our Go workspace. Now we&rsquo;re going to download the latest Docker code and build it. The Go tool can directly download the Docker Git repository and save it into our workspace:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>go get -d github.com/docker/docker
</span></code></pre></td></tr></table></div></figure></p>

<p>Now we can change our directory to the cloned Git repository and start the build:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nb">cd</span> <span class="nv">$HOME</span>/go/src/github.com/docker/docker
</span><span class='line'><span class="nv">GOPATH</span><span class="o">=</span><span class="nv">$HOME</span>/go:<span class="nv">$HOME</span>/go/src/github.com/docker/docker/vendor ./hack/make.sh dynbinary
</span></code></pre></td></tr></table></div></figure></p>

<p>Docker comes with a set of dependencies directly checked into the Docker Git repository. You can find them in the <code>vendor</code> directory. This directory is actually a Go workspace which we only needed to include in our <code>GOPATH</code> when we triggered the build. If everything went fine, you can find some generated source files in the <code>autogen</code> directory and the freshly built Docker executables in the <code>bundles</code> directory.</p>

<h2>Creating an Eclipse project</h2>

<p>In this section, we&rsquo;re going to install a Go language plug-in into Eclipse IDE and create a Docker project. The <a href="https://github.com/GoClipse/goclipse" title="GoClipse">GoClipse</a> plug-in brings Go language support into Eclipse. The minimum installation requirements for this plug-in are: Eclipse 4.5 (Mars) running on Java 8 or later. You can follow the installation instructions available <a href="https://github.com/GoClipse/goclipse/blob/latest/documentation/Installation.md" title="GoClipse installation">here</a> to get the plug-in installed.</p>

<p>After the successful plug-in installation and restart of Eclipse select <code>File -&gt; New -&gt; Project ...</code>. Choose <code>Go Project</code> in the dialog box. You&rsquo;ll be presented with a window similar to:</p>

<p><img class="center" src="/images/posts/eclipse_go1.png"></p>

<p>Instead of using the default location, let Eclipse create the project in your Go workspace. After your Go project has been created, go to <code>Window -&gt; Preferences</code> and find the tab with the Go configuration. You want to set the location of your Go language installation to the standard <code>/usr/local/go</code> directory. Make sure you set the <code>GOOS</code> and <code>GOARCH</code> fields, too. You&rsquo;ll also have to add the path to the Docker&rsquo;s <code>vendor</code> directory into the <code>GOPATH</code> field.</p>

<p><img class="center" src="/images/posts/eclipse_go2.png"></p>

<h2>Code navigation and auto-completion</h2>

<p>In this final section, we&rsquo;re going to make navigation and auto-completion in Eclipse work. The <code>Open Definition</code> navigation in Eclipse (keyboard shorcut F3) requires the Go <code>oracle</code> tool to be installed. Whenever you open the definition of the entity under the cursor, Eclipse will call the <code>oracle</code> tool in order to obtain the information about the navigation target.</p>

<p>The <code>oracle</code> tool is part of a bigger Go toolset located <a href="https://github.com/golang/tools" title="Golang tools">here</a>. You can easily install the <code>oracle</code> tool using the <code>go</code> command. Type this in your console:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>go get golang.org/x/tools/cmd/oracle
</span></code></pre></td></tr></table></div></figure></p>

<p>You can run this command to confirm that the <code>oracle</code> tool was installed successfully:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>oracle <span class="p">&amp;</span>ndash<span class="p">;</span><span class="nb">help</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>The source code navigation in Eclipse using the F3 keyboard shortcut should start working. Let&rsquo;s focus on the code auto-completion (Ctrl+Space) next. In order to make the auto-completion work, we need to install an auto-completion daemon for the Go programming language <a href="https://github.com/nsf/gocode" title="gocode">gocode</a>. The installation with the <code>go</code> tool is pretty simple:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>go get github.com/nsf/gocode
</span></code></pre></td></tr></table></div></figure></p>

<p>To confirm that the <code>gocode</code> tool was installed successfully type:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>gocode <span class="p">&amp;</span>ndash<span class="p">;</span><span class="nb">help</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Eclipse provides a configuration dialog for Go tools under <code>Window -&gt; Preferences</code>. There are even buttons to click and install the <code>oracle</code> and <code>gocode</code> tools from within Eclipse. We did this installation on the command-line.</p>

<p><img class="center" src="/images/posts/eclipse_go3.png"></p>

<p>Now that you&rsquo;re all set I wish you happy browsing through the Docker source code!</p>
]]></content>
  </entry>

</feed>
