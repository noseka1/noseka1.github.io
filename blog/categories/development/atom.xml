<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: development | Ales Nosek - The Software Practitioner]]></title>
  <link href="https://alesnosek.com/blog/categories/development/atom.xml" rel="self"/>
  <link href="https://alesnosek.com/"/>
  <updated>2022-09-17T10:52:24-07:00</updated>
  <id>https://alesnosek.com/</id>
  <author>
    <name><![CDATA[Ales Nosek]]></name>

  </author>
  <generator uri="http://octopress.org/">Octopress</generator>


  <entry>
    <title type="html"><![CDATA[Deep Dive into CodeReady Containers Deployment on Linux]]></title>
    <link href="https://alesnosek.com/blog/2021/02/28/deep-dive-into-codeready-containers-deployment-on-linux/"/>
    <updated>2021-02-28T16:38:46-08:00</updated>
    <id>https://alesnosek.com/blog/2021/02/28/deep-dive-into-codeready-containers-deployment-on-linux</id>
    <content type="html"><![CDATA[<p>Are you considering <a href="https://developers.redhat.com/products/codeready-containers/overview">Red Hat CodeReady Containers</a> (CRC) for your local OpenShift development? Are you going to install CRC on Linux? In this blog, I am going to walk you through the deployment of CRC on Linux. We will take a closer look at how CRC works and review some of the configuration options. Fasten your seatbelts, the tour begins.</p>

<!-- more -->


<p>This blog uses CRC version 1.21.0, which is based on OpenShift Container Platform (OCP) version 4.6.9. I am installing CRC on Debian 10 GNU/Linux but any modern Linux distribution like Fedora or Ubuntu will do. As of version 1.21.0, CRC can be deployed on a Linux host that meets the following requirements:</p>

<ul>
<li><em>KVM</em> and <em>libvirt</em> installed</li>
<li>Networking configuration is managed by the <em>NetworkManager</em></li>
<li>User installing CRC has <em>sudo</em> access to the Linux host</li>
</ul>


<p>In addition to the Linux host, you will also need to download the CRC distribution tarball and a pull secret. The pull secret is a JSON file that includes the authentication information for accessing protected container image registries that are hosted by Red Hat. If you are not a Red Hat customer, you can join the <a href="https://developers.redhat.com/">Red Hat Developer Program</a> and download the pull secret at no cost. Through the Developer Program you can also obtain the CRC distribution tarball. Alternatively, you can download the CRC distribution tarball directly from <a href="https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/crc/latest/">here</a>.</p>

<p>CRC comes with excellent documentation which is being expanded with every new release. You can find it <a href="https://code-ready.github.io/crc/">here</a>.</p>

<h2>CodeReady Containers deployment overview</h2>

<p>From a high-level perspective, CRC creates a virtual machine that runs a single-node OpenShift cluster. This node plays the role of a master and a worker node at the same time. No OpenShift installation takes place while deploying CRC. Instead, CRC launches OpenShift from a pre-installed virtual machine image. The diagram below depicts the CRC deployment:</p>

<p><img class="center" src="/images/posts/codeready_containers_deployment_diagram.png"></p>

<p>On the Linux host, CRC makes use of libvirt to create a network, storage pool, and a CRC virtual machine. The virtual machine&rsquo;s data is persisted on the libvirt volume which ensures that the objects created by the user in OpenShift will survive CRC restarts.</p>

<p>The OpenShift instance comes with a fixed set of <em>PersistentVolume</em>s. Users can attach these volumes to the application pods by creating <em>PersistentVolumeClaim</em>s (PVC). All of the volume access modes are supported: <em>ReadWriteOnce</em>, <em>ReadWriteMany</em>, and <em>ReadOnlyMany</em>. The content of the persistent volumes is saved on the host and can be found in the <code>/var/mnt/pv-data</code> directory.</p>

<p>How about the integrated image registry? The image registry in OpenShift is backed by a PersistentVolume and is exposed via a public route <code>default-route-openshift-image-registry.apps-crc.testing</code>. Users can use this route to push their container images to the registry before launching them on OpenShift.</p>

<p>The last thing to note in the diagram is the DNS configuration. Why is this needed? CRC configures the DNS resolution on the Linux host so that the connections to the endpoints <code>api.crc.testing</code> and <code>*.apps-crc.testing</code> are routed to the OpenShift instance. NetworkManager, which is a requirement for CRC to work, is used to achieve this DNS configuration. CRC instructs the NetworkManager to spin up a dnsmasq instance, which forwards the DNS queries for the OpenShift endpoints to the second dnsmasq instance which was deployed inside the virtual machine. This second dnsmasq instance actually resolves the queries.</p>

<h2>Configuring CodeReady Containers</h2>

<p>In this section, we are going to review several CRC configuration options that you may want to customize. By default, the CRC virtual machine is provisioned with 4 vCPUs, 8790 MiB RAM, and 31 GiB disk space. Depending on your use case, you may want to increase these values. On my desktop, I like to bump up the number of vCPUs to 10. Using the <code>crc</code> command, it can be accomplished this way:</p>

<pre><code>$ ./crc config set cpus 10
</code></pre>

<p>The default memory size of 8790 MiB seems rather small to me. Most of the memory is used by the OpenShift components. Out of the  8790 MiB of memory, only about 2 GiB remain available for your applications. As I have plenty of memory available on my desktop computer, I increase the memory size to some 46 GiB:</p>

<pre><code>$ ./crc config set memory 47104
</code></pre>

<p>Regarding the disk space, the CRC virtual machine comes with a 31 GiB disk by default. About half of this capacity is not available as it is used by the Red Hat CoreOS operating system and OpenShift components. Around 16 GiB remains available to cover the storage needs of the integrated image registry and any other volumes (PVCs) that you create for your applications. The default 31 GiB disk provides a good amount of space, however, I like to increase the disk size to 120 GiB just to match the default disk size of OCP clusters. Note that CRC allows you to increase the disk size any time after the installation as well. To increase the disk size to 120 GiB, issue the command:</p>

<pre><code>$ ./crc config set disk-size 120
</code></pre>

<p>CRC requires an OpenShift pull secret. I like to point the CRC to my OpenShift pull secret which I store in a protected location in my home directory. This pull secret is required by OpenShift regardless of which method is used to deploy OpenShift. The same pull secret used for CRC can be used for installing OpenShift via the OpenShift installer. After I downloaded the pull secret from the Red Hat site and saved it at <code>~/.mysecrets/ocp/pull-secret.json</code>, I then need to tell CRC where to find it:</p>

<pre><code>$ ./crc config set pull-secret-file ~/.mysecrets/ocp/pull-secret.json
</code></pre>

<p>One final configuration option I like to set is <code>consent-telemetry</code>. Setting this option upfront prevents CRC from asking interactively: &ldquo;Would you like to contribute anonymous usage statistics [y/N]&rdquo;:</p>

<pre><code>$ ./crc config set consent-telemetry yes
</code></pre>

<p>CRC saves the above settings in <code>~/.crc/crc.json</code>. If you prefer, you can also edit this JSON file directly. The full list of available configuration options can be obtained by typing:</p>

<pre><code>$ ./crc config --help
</code></pre>

<p>With the CRC configuration squared away, we are ready to prepare the Linux host for launching an OpenShift cluster.</p>

<h2>Setting up CodeReady Containers</h2>

<p>In this section, we are going to prepare the Linux host for launching the CRC virtual machine. Note that this is a one-time setup. Go ahead and issue the following command to begin the setup process:</p>

<pre><code>$ ./crc setup
</code></pre>

<p>While the setup is running, let&rsquo;s talk about the CRC binary.  Did you notice that the CRC binary is quite large? The size of the CRC binary version 1.21.0 is around 2.6 GiB. Why is that? To answer this question, check out the binary structure that is depicted in the following diagram:</p>

<p><img class="center" src="/images/posts/codeready_containers_binary.png"></p>

<p>The CRC binary consists of four parts. The first part is the CRC executable.  The second part is the <code>admin-helper-linux</code> utility that is used for updating the <code>/etc/hosts</code> file. After that comes the <code>crc-driver-libvirt</code> daemon executable, which implements functions specific to the libvirt virtualization and abstracts the virtualization details away from the CRC core. Finally, the so-called CRC bundle (<code>crc_libvirt_4.6.9.crcbundle</code> in the diagram) is the last part of the CRC binary. This bundle contains a virtual machine image and accounts for the majority of the size of the CRC binary.</p>

<p>Now let&rsquo;s get back to the CRC setup. At the initial stage, the <code>./crc setup</code> command extracts all the components appended to the CRC executable and places them below the <code>~/.crc</code> directory. The embedded CRC bundle is a <em>tar.xz</em> archive whose contents is immediately decompressed into <code>~/.crc/cache</code>. This bundle contains the following files:</p>

<ul>
<li>The <code>crc-bundle-info.json</code> carries the bundle metadata. CRC refers to it throughout the deployment process.</li>
<li>The virtual machine image <code>crc.qcow2</code>  contains a pre-installed OpenShift node. This image will be used as a backing image for the CRC virtual machine&rsquo;s disk image.</li>
<li>The <code>id_rsa_crc</code> bootstrap key is used by CRC for SSHing into the virtual machine at its first start. After connecting to the virtual machine, CRC generates a new unique SSH key pair and adds it to the machine&rsquo;s <code>~core/.ssh/authorized_keys</code> file. The original bootstrap SSH key is removed from this file, and hence can no longer be used to access the virtual machine.</li>
<li>The <code>kubeadmin-password</code> file holds the password of the <em>kubeadmin</em> user on OpenShift.</li>
<li>The <code>kubeconfig</code> file allows logging into OpenShift as user <em>kube:admin</em>. It includes the user&rsquo;s private key that is needed for successful authentication.</li>
<li>The <code>oc</code> executable is an oc client whose version matches the version of the bundled OpenShift cluster.</li>
</ul>


<p>After the extraction of the CRC components is complete, the <code>~/.crc</code> directory looks like this:</p>

<pre><code>$ tree --noreport .crc
.crc
├── bin
│   ├── admin-helper-linux
│   ├── crc-driver-libvirt
│   └── oc
│       └── oc -&gt; /home/anosek/.crc/cache/crc_libvirt_4.6.9/oc
├── cache
│   ├── crc_libvirt_4.6.9
│   │   ├── crc-bundle-info.json
│   │   ├── crc.qcow2
│   │   ├── id_ecdsa_crc
│   │   ├── kubeadmin-password
│   │   ├── kubeconfig
│   │   └── oc
│   └── crc_libvirt_4.6.9.crcbundle
└── crc.json
</code></pre>

<p>The next notable step carried out by the CRC setup is configuring DNS on the Linux host. CRC configures DNS, so that the connections to the endpoints <code>api.crc.testing</code> and <code>*.apps-crc.testing</code> are routed to the OpenShift instance. It is known ahead of time that this OpenShift instance is going to expose its endpoints on a hard-coded IP address <code>192.168.130.11</code>. So, how does CRC ensure the proper DNS resolution of the OpenShift endpoints? To achieve that, CRC creates a <code>/etc/NetworkManager/conf.d/crc-nm-dnsmasq.conf</code> file with the following configuration:</p>

<pre><code>[main]
dns=dnsmasq
</code></pre>

<p>This configuration instructs the NetworkManager to first, spin up a dnsqmasq instance, and second, modify the <code>/etc/resolv.conf</code> on the machine to use this instance as a default DNS server. In the next step, CRC configures the dnsmasq server by creating a <code>/etc/NetworkManager/dnsmasq.d/crc.conf</code> configuration file with the following content:</p>

<pre><code>server=/apps-crc.testing/192.168.130.11
server=/crc.testing/192.168.130.11
</code></pre>

<p>This forwards DNS queries for the <code>crc.testing</code> and <code>apps-crc.testing</code> domains plus all their subdomains to the DNS server <code>192.168.130.11</code>. This DNS server will be deployed inside the CRC virtual machine and will be handling the resolution of the OpenShift endpoints.</p>

<p>Note that the dnsmasq forwarder as described above is only created if your host doesn&rsquo;t use <em>systemd-resolved</em> for DNS resolution. If your host uses systemd-resolved, then CRC will configure the forwarding in systemd-resolved instead of spinning up the additional dnsmasq forwarder.</p>

<p>The last step executed by the <code>./crc setup</code> command is creating a libvirt network. What needs to be done here? CRC creates a libvirt network called <code>crc</code> of type NAT. The only host on the network will get the IP address <code>192.168.130.11</code>:</p>

<pre><code>$ virsh net-dumpxml crc
&lt;network connections='1'&gt;
  &lt;name&gt;crc&lt;/name&gt;
  &lt;uuid&gt;49eee855-d342-46c3-9ed3-b8d1758814cd&lt;/uuid&gt;
  &lt;forward mode='nat'&gt;
    &lt;nat&gt;
      &lt;port start='1024' end='65535'/&gt;
    &lt;/nat&gt;
  &lt;/forward&gt;
  &lt;bridge name='crc' stp='on' delay='0'/&gt;
  &lt;mac address='52:54:00:fd:be:d0'/&gt;
  &lt;ip family='ipv4' address='192.168.130.1' prefix='24'&gt;
    &lt;dhcp&gt;
      &lt;host mac='52:fd:fc:07:21:82' ip='192.168.130.11'/&gt;
    &lt;/dhcp&gt;
  &lt;/ip&gt;
&lt;/network&gt;
</code></pre>

<p>This network will be hosting the CRC virtual machine. This virtual machine will be created with the MAC address <code>52:fd:fc:07:21:82</code>. The above configuration assigns the fixed IP address <code>192.168.130.11</code> to this virtual machine. Both the MAC address <code>52:fd:fc:07:21:82</code> and the IP address <code>192.168.130.11</code> are hard-coded in CRC.</p>

<p>Creating the libvirt network was the last setup step that I wanted to discuss. In the next section, we are going to create and launch the CRC virtual machine.</p>

<h2>Starting CodeReady Containers</h2>

<p>After the <code>./crc setup</code> completes, issue the following command to create and start the CRC virtual machine:</p>

<pre><code>$ ./crc start
</code></pre>

<p>Note that if the CRC machine already exists, the above command will just start it. In the remainder of this section, I am going to discuss some of the notable steps executed during the start process.</p>

<p>First, CRC runs the pre-flight checks that were also executed previously by <code>./crc setup</code> command. This is to ensure that the environment is still in a good shape.</p>

<p>Next, CRC creates a libvirt storage pool called <code>crc</code> in the directory <code>~/.crc/machines/crc</code>. In this storage pool, a new machine image is created which is called <code>crc.qcow2</code> and has the size of 120 GiB. We chose the 120 GiB size by configuring the <code>disk-size</code> parameter in the configuration section above. The image is thin-provisioned, i.e.  no disk capacity is allocated ahead of time. Instead, additional space allocation occurs on demand as data is written to the disk. Also, CRC supports resizing the image. If the user decides to change the <code>disk-size</code> parameter, CRC will resize the machine image accordingly before starting the virtual machine. The libvirt storage pool definition looks something like this:</p>

<pre><code>$ virsh pool-dumpxml crc
&lt;pool type='dir'&gt;
  &lt;name&gt;crc&lt;/name&gt;
  &lt;uuid&gt;ecfe5181-6476-43a8-9ff9-a133841df011&lt;/uuid&gt;
  &lt;capacity unit='bytes'&gt;1769532428288&lt;/capacity&gt;
  &lt;allocation unit='bytes'&gt;1119201026048&lt;/allocation&gt;
  &lt;available unit='bytes'&gt;650331402240&lt;/available&gt;
  &lt;source&gt;
  &lt;/source&gt;
  &lt;target&gt;
    &lt;path&gt;/home/anosek/.crc/machines/crc&lt;/path&gt;
    &lt;permissions&gt;
      &lt;mode&gt;0755&lt;/mode&gt;
      &lt;owner&gt;1000&lt;/owner&gt;
      &lt;group&gt;1000&lt;/group&gt;
    &lt;/permissions&gt;
  &lt;/target&gt;
&lt;/pool&gt;
</code></pre>

<p>After the virtual machine disk is ready, CRC will create a virtual machine and start it. The virtual machine is called <code>crc</code> and you can review its definition by issuing:</p>

<pre><code>$ virsh dumpxml crc
&lt;domain type='kvm' id='2'&gt;
  &lt;name&gt;crc&lt;/name&gt;
  &lt;uuid&gt;4269a53d-0fa7-465e-9190-ab33047244ee&lt;/uuid&gt;
  &lt;memory unit='KiB'&gt;46000128&lt;/memory&gt;
  &lt;currentMemory unit='KiB'&gt;46000128&lt;/currentMemory&gt;
  &lt;vcpu placement='static'&gt;10&lt;/vcpu&gt;
  &lt;resource&gt;
    &lt;partition&gt;/machine&lt;/partition&gt;
  &lt;/resource&gt;

...

  &lt;devices&gt;
    &lt;disk type='file' device='disk'&gt;
      &lt;driver name='qemu' type='qcow2' io='threads'/&gt;
      &lt;source file='/home/anosek/.crc/machines/crc/crc.qcow2' index='1'/&gt;
      &lt;backingStore type='file' index='2'&gt;
        &lt;format type='qcow2'/&gt;
        &lt;source file='/home/anosek/.crc/cache/crc_libvirt_4.6.9/crc.qcow2'/&gt;
        &lt;backingStore/&gt;
      &lt;/backingStore&gt;
...

&lt;/domain&gt;
</code></pre>

<p>The virtual machine&rsquo;s vCPU and memory are sized according to the <code>cpus</code> and <code>memory</code> configuration we made above. In the machine definition, you can see that the virtual machine runs off the image that was created at <code>~/.crc/machines/crc/crc.qcow2</code>. This image is actually an overlay image created on top of the <code>~/.crc/cache/crc_libvirt_4.6.9/crc.qcow2</code> backing image. The overlay image starts as an empty image and records changes made to the disk of the CRC virtual machine. All the changes made in the CRC virtual machine are persisted in the overlay image and will survive the virtual machine restarts. That means that your OpenShift configuration will persist across the <code>./crc stop</code> and <code>./crc start</code> commands.</p>

<p>Next, CRC creates a new SSH key pair that will be used for SSHing to the virtual machine as user <code>core</code>. The public key from this key pair is added to the <code>~core/.ssh/authorized_keys</code> file in the virtual machine to enable the access. CRC uses SSH to further configure the virtual machine during the start.</p>

<p>If the user changed the <code>disk-size</code> parameter, the file system needs to be enlarged to span the full disk size. This is achieved by executing the <code>xfs_growfs /</code> command inside the virtual machine. CRC comes with the default disk size of 31 GiB. At this point, the file system is expanded to the 120 GiB that we configured above.</p>

<p>Next, CRC starts a DNS server &mdash; a podman container running dnsmasq &mdash; inside the virtual machine. This DNS server resolves the domain names <code>*.apps-crc.testing</code>, <code>api.crc.testing</code>, and <code>api-int.crc.testing</code> to the IP address <code>192.168.130.11</code>, which is the address of the virtual machine itself. These domain names are assigned to the well-known OpenShift endpoints: ingress router, API server, and internal API, respectively. The DNS server is used both by the virtual machine as well as by the Linux host. The Linux host reaches the DNS server through the DNS forwarder as we described before.</p>

<p>During the start phase, CRC adds the following line to <code>/etc/hosts</code> on the Linux host:</p>

<pre><code>192.168.130.11 api.crc.testing console-openshift-console.apps-crc.testing default-route-openshift-image-registry.apps-crc.testing oauth-openshift.apps-crc.testing
</code></pre>

<p>I am not sure why this setting is being done. The resolution of the above domains is already handled by the dnsmasq server running inside the CRC virtual machine. Anyway, it is good to know that CRC adds this line to your <code>/etc/hosts</code>.</p>

<p>In the next step of the start sequence, CRC starts the kubelet service which in turn brings up OpenShift services. If the kubelet&rsquo;s client certificate expired while the CRC virtual machine was down, kubelet will issue a Certificate Signing Request (CSR) to obtain a new certificate. CRC checks for the presence of any new CSRs and automatically approves them.</p>

<p>As the last step, CRC adds the user&rsquo;s pull secret to the OpenShift cluster.</p>

<p>The configuration related to the particular CRC machine instance is stored in the <code>.crc/machines/crc</code> directory. You can check it out using:</p>

<pre><code>$ tree --noreport .crc/machines
.crc/machines
└── crc
    ├── config.json
    ├── crc.qcow2
    ├── id_ecdsa
    ├── id_ecdsa.pub
    └── kubeconfig
</code></pre>

<p>Congratulations, your OpenShift instance is now up and running!</p>

<h2>CodeReady Containers convenience commands</h2>

<p>In this section, I would like to mention a couple of commands that are useful when working with CRC. First, you can open OpenShift Web Console in the default browser by issuing:</p>

<pre><code>$ ./crc console
</code></pre>

<p>You can always display the OpenShift user credentials by running:</p>

<pre><code>$ ./crc console --credentials
To login as a regular user, run 'oc login -u developer -p developer https://api.crc.testing:6443'.
To login as an admin, run 'oc login -u kubeadmin -p HqC3I-wgtiB-q7qCf-KEsuK https://api.crc.testing:6443'
</code></pre>

<p>You can SSH into the CRC virtual machine as the user <code>core</code>:</p>

<pre><code>$ ssh -i ~/.crc/machines/crc/id_ecdsa core@api.crc.testing
</code></pre>

<p>The <code>core</code> user has full administrative privileges via sudo.</p>

<h2>Conclusion</h2>

<p>This blog covered the deployment of CodeReady Containers to a Linux host. We began by reviewing the prerequisites that are needed to deploy CRC. In the deployment overview section, we showed how CRC interacts with libvirt to spin up  the OpenShift virtual machine. We also discussed the DNS configuration made by CRC. Before deploying CRC, we customized the CRC configuration and provided the virtual machine with additional resources beyond the factory defaults. We discussed the CRC setup and start phases in great detail. Finally, I shared some of the convenience commands I like to use.</p>

<p><strong> Update 3/29/2021: </strong> I also have a video related to this topic, it can be found <a href="https://www.youtube.com/watch?v=JrjLo_fl280">here</a>.</p>

<p>Hope you enjoyed the CodeReady Containers tour presented in this blog.  If you have any questions or comments, please leave them in the comment section below. I look forward to hearing from you!</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[14 Best Practices for Developing Applications on OpenShift]]></title>
    <link href="https://alesnosek.com/blog/2020/11/24/14-best-practices-for-developing-applications-on-openshift/"/>
    <updated>2020-11-24T08:16:33-08:00</updated>
    <id>https://alesnosek.com/blog/2020/11/24/14-best-practices-for-developing-applications-on-openshift</id>
    <content type="html"><![CDATA[<p>Over the course of this year, I have been working with several development teams that started building applications on OpenShift. My goal was to provide the developers with guidance and best practices that would help them to successfully deploy their applications to production. If you are a developer that builds applications on top of OpenShift, this blog might be of interest to you.</p>

<p>The blog is published at <a href="https://www.openshift.com/blog/14-best-practices-for-developing-applications-on-openshift">openshift.com/blog</a>.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Local Development with OpenShift and Tilt]]></title>
    <link href="https://alesnosek.com/blog/2020/06/08/local-development-with-openshift-and-tilt/"/>
    <updated>2020-06-08T20:36:36-07:00</updated>
    <id>https://alesnosek.com/blog/2020/06/08/local-development-with-openshift-and-tilt</id>
    <content type="html"><![CDATA[<p>In this blog post, I am going to show you how to use Tilt to facilitate local OpenShift development. Tilt’s capabilities will be demonstrated in a practical example that uses buildah and CodeReady Containers. If you develop containerized applications on OpenShift, this blog post is for you.</p>

<!-- more -->


<h2>How does Tilt facilitate local development?</h2>

<p>The diagram below depicts a development workflow orchestrated by Tilt. After you execute the <code>tilt up</code> command on your development machine, Tilt will keep running while performing the following actions:</p>

<ol>
<li>Tilt watches for changes in the source code made by the developer on the local machine.</li>
<li>After a change has been detected, Tilt executes buildah to update the container image. After the build completes, the updated container image is pushed to the internal registry in CodeReady Containers.</li>
<li>Tilt watches Kubernetes manifests on the local machine and keeps them in sync with CodeReady Containers. Any changes made to the manifests are instantly applied to CodeReady Containers.</li>
<li>Tilt forwards local ports to the application pod running in CodeReady Containers. This allows the developer to conveniently access the application on localhost.</li>
</ol>


<p><img class="center" src="/images/posts/local_development_with_openshift_and_tilt_diagram.png"></p>

<p>Tilt helps the developer automate many of the manual steps made during the development of containerized applications. It speeds up the edit-compile-run loop considerably. Interested to trying it out? Follow me to the next section, where we will implement the workflow depicted in the above diagram.</p>

<h2>Using Tilt for developing a sample application</h2>

<p>In this section, we are going to use Tilt to orchestrate the development of a sample application. As I didn&rsquo;t want to reinvent the wheel by designing a custom application, I grabbed the Plain Old Static HTML example that comes with Tilt and can be found on <a href="https://github.com/tilt-dev/tilt-example-html/tree/faad605963b396b0863151802544fb01f6b414c6/0-base">GitHub</a>. This example is described in the Tilt&rsquo;s <a href="https://docs.tilt.dev/example_static_html.html">documentation</a> and you may already be familiar with it. It consists of a very simple shell script that serves a static HTML. In contrast to Tilt&rsquo;s example which leverages Docker and upstream Kubernetes, I will be using developer tools from the Red Hat&rsquo;s portfolio:</p>

<ul>
<li><a href="https://buildah.io/">buildah</a></li>
<li><a href="https://www.redhat.com/en/blog/introducing-red-hat-universal-base-image">UBI container images</a></li>
<li><a href="https://developers.redhat.com/products/codeready-containers">CodeReady Containers</a></li>
</ul>


<p>In order to be able to use the Red Hat developer tools, I had to modify the original sample code. The sample code used in this tutorial can be found on <a href="https://github.com/noseka1/local-development-with-openshift-and-tilt">GitHub</a>. I recommend that you go ahead and briefly review it.</p>

<p>The overall setup consists of a Fedora 32 development machine where I installed buildah and CoreReady Containers (CRC) version 1.10. I installed Tilt version 0.13.4 which is the latest release of Tilt available at the time of this writing. If you plan to use Tilt along with CodeReady Containers, I recommend grabbing this or any future versions of Tilt, as this version includes a <a href="https://github.com/windmilleng/tilt/commit/7e9487816ea32ed086318ce7373c67d9febb6f36">patch</a> that makes Tilt work with CodeReady Containers without the need for further configuration. The Tilt binary can be downloaded from <a href="https://github.com/tilt-dev/tilt/releases">GitHub</a>.</p>

<p>Having the required tools in place, let&rsquo;s start by logging in into CRC and creating a new project:</p>

<pre><code>$ oc login
$ oc new-project tilt-example
</code></pre>

<p>Let&rsquo;s now focus  on configuring buildah to be able the pull and push images from the container registries. As you can see in the <a href="https://github.com/noseka1/local-development-with-openshift-and-tilt/blob/master/Dockerfile">Dockerfile</a>, our application uses the <code>registry.redhat.io/ubi8/ubi</code> container image as the base image. In order for buildah to be able to pull this image from the registry, we need to log in to this registry using the Red Hat Customer Portal credentials:</p>

<pre><code>$ buildah login registry.redhat.io
</code></pre>

<p>Next, let&rsquo;s configure buildah to be able to push images into the CRC internal registry. The CRC registry endpoint uses a self-signed certificate. Buildah will refuse to communicate with the internal registry as the certificate is signed by an unknown authority. In order for buildah to be able to push images into the internal registry, you will need to add this registry to the list of insecure registries. On your development machine, edit <code>/etc/containers/registries.conf</code> and add the CRC internal registry to the list of insecure registries.</p>

<pre><code>[registries.insecure]
registries = [ 'default-route-openshift-image-registry.apps-crc.testing' ]
</code></pre>

<p>In order for buildah to be able to push images into the CRC registry, we need to log in to this registry. For that, use the <code>oc</code> command to grab a token used for authentication against the registry:
<code>
$ oc whoami --show-token
7geTDzA6Mqa-NeXweTXtOFUJtEHocVShKl5yxtxqeB0
</code>
Log in to the registry using the authentication token:
<code>
$ buildah login \
    --username unused \
    --password 7geTDzA6Mqa-NeXweTXtOFUJtEHocVShKl5yxtxqeB0 \
    default-route-openshift-image-registry.apps-crc.testing
Login Succeeded!
</code></p>

<p>After successfully logging into the CRC internal registry, the buildah configuration is now complete. Finally, we can turn our attention to Tilt. First, let&rsquo;s review the <code>Tiltfile</code> which describes how Tilt will orchestrate our development workflow:</p>

<pre><code># push the container image to the CRC internal registry, project tilt-example
default_registry(
  'default-route-openshift-image-registry.apps-crc.testing/tilt-example',
  host_from_cluster='image-registry.openshift-image-registry.svc:5000/tilt-example')

# use buildah to build and push the container image
custom_build(
  'example-html-image',
  'buildah build-using-dockerfile --tag $EXPECTED_REF . &amp;&amp; buildah push $EXPECTED_REF',
  ['.'],
  skips_local_docker=True)

# deploy Kubernetes resource
k8s_yaml('kubernetes.yaml')

# make the application available on localhost:8000
k8s_resource('example-html', port_forwards=8000)
</code></pre>

<p>I annotated the <code>Tiltfile</code> with comments that explain the meaning of individual Tilt instructions. Ready to give it a shot? Just clone the git repository and run Tilt:</p>

<pre><code>$ git clone https://github.com/noseka1/local-development-with-openshift-and-tilt
$ cd local-development-with-openshift-and-tilt/
$ tilt up
</code></pre>

<p>After Tilt comes up, it will call buildah to pull the base image, build the application, and push the resulting image to the CRC internal registry. It will also deploy the application on Kubernetes by applying the <code>kubernetes.yaml</code> manifest referenced in the <code>Tiltfile</code>. If everything worked well, and the application pod starts up, you will see the &ldquo;Serving files on port 8000&rdquo; log message in the bottom pane:</p>

<p><img class="center" src="/images/posts/local_development_with_openshift_and_tilt.png"></p>

<p>At this point, you should be able to reach the running application on <code>localhost:8000</code>:
<code>
$ curl localhost:8000
&lt;!doctype html&gt;
&lt;html&gt;
  &lt;body style="font-size: 30px; font-family: sans-serif; margin: 0;"&gt;
    &lt;div style="display: flex; flex-direction: column; width: 100vw; height: 100vh; align-items: center; justify-content: center;"&gt;
      &lt;img src="pets.png" style="max-width: 30vw; max-height: 30vh;"&gt;
      &lt;div&gt;Hello cats!&lt;/div&gt;
    &lt;/div&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></p>

<p>To experience how Tilt facilitates the local development, change the content of the <code>index.html</code> file. After you save your changes, Tilt will instantly re-run the loop and deploy the updated application.</p>

<h2>Conclusion</h2>

<p>In this blog, we described how Tilt can facilitate the local development of containerized applications. We demonstrated Tilt&rsquo;s capabilities in a practical example that showed Tilt working along with buildah and CodeReady Containers. In this introductory article, we were able to only scratch the surface. There is much more that Tilt has to offer, including live updates that can update the application without restarting the container, and which can drastically speed up the edit-compile-run loop. I encourage you to read through <a href="https://docs.tilt.dev/">Tilt&rsquo;s documentation</a> to learn more about this tool.</p>

<p>Do you use Tilt for development on OpenShift or Kubernetes? What&rsquo;s your opinion on Tilt? I would be happy to hear about your experiences. You can leave your comments in the comment section below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Open Policy Agent, Part III &mdash; Integrating with Your Application]]></title>
    <link href="https://alesnosek.com/blog/2019/12/03/open-policy-agent-part-iii-integrating-with-your-application/"/>
    <updated>2019-12-03T12:46:10-08:00</updated>
    <id>https://alesnosek.com/blog/2019/12/03/open-policy-agent-part-iii-integrating-with-your-application</id>
    <content type="html"><![CDATA[<p>In the <a href="/blog/2019/10/27/open-policy-agent-part-ii-developing-policies/">previous entry</a> to this series, we discussed developing policies with Open Policy Agent. In this final article in the series, we are going to focus on how you can integrate Open Policy Agent with your application.</p>

<!-- more -->


<h2>Integrating OPA with your application</h2>

<p>There are several options how you can integrate OPA with your application. If you happen to build your application using the Go language, you can link OPA as a library straight into your application. Otherwise, you will run OPA as a stand-alone service (daemon). If you deploy your application on Kubernetes, you can run OPA service as a side-car container along with your application services. This minimizes the communication latency between OPA and your application. It also avoids possible communication issues between OPA and your application due to network failures. If you are deploying on virtual machines, you can run one replica of the OPA service on each of your virtual machines to achieve the same benefits. In summary, deploying OPA as a side-car service or a host-local service is the recommended approach.</p>

<p><img class="center" src="/images/posts/open_policy_agent/opa_integration.png"></p>

<p>Another deployment option would be running multiple OPA services behind a load balancer. Your application and OPA services would in this case talk over the network and the communication would go through the load balancer. You would incur the cost of network latency. However, I am not sure how would the overall reliability of this approach compare to the side-car approach. OPA documentation doesn&rsquo;t really mention this option of deploying multiple OPA services behind a load balancer. However, if you are building your application as a set of Lambda functions, then this might be the way to go:</p>

<p><img class="center" src="/images/posts/open_policy_agent/opa_integration_lambda.png"></p>

<h2>Utilizing Open Policy Agent APIs</h2>

<p>Open Policy Agent comes with a whole set of APIs that you can use in order to utilize OPA to its full potential. I depicted the possible API integrations in the following diagram:</p>

<p><img class="center" src="/images/posts/open_policy_agent/opa_integration_full.png"></p>

<p>In the diagram above, the green box is your application invoking policy queries against OPA. The purple services are optional services that you can include in your architecture. These services have to be implemented by yourself and they must expose APIs that are specified by OPA. Two blue boxes depict the Prometheus monitoring server and Kubernetes. OPA can integrate with them right away. In the diagram, the direction of the arrows between services is significant. The arrows indicate which service from the pair initiates the TCP connection. In the following subsections, let&rsquo;s take a closer look at each of these OPA interfaces.</p>

<h3>Open Policy Agent REST API</h3>

<p>This is the main API provided by OPA that your application uses to manage data and policies, and to execute queries. It is well described in the OPA&rsquo;s <a href="https://www.openpolicyagent.org/docs/latest/rest-api/">REST API documentation</a>. I would like to highlight two things that I learned about the OPA REST APIs:</p>

<p>First, OPA allows you to set watches on policy queries in order for you to be notified whenever the result of the query evaluation changes. Watches utilize the HTTP long polling mechanism. For additional details, you can refer to the <a href="https://www.openpolicyagent.org/docs/latest/rest-api/#watches">Watches</a> section of the OPA&rsquo;s documentation.</p>

<p>Second, the communication between the client (i.e. your application) and the OPA service can be protected using TLS. OPA authenticates itself to the client by presenting a valid TLS certificate. Client can authenticate itself to OPA either by presenting a client TLS certificate or by presenting a security token. And guess how OPA handles API authorization? Of course, by evaluating a Rego policy that you supply. For further details on OPA&rsquo;s security settings, refer to the <a href="https://www.openpolicyagent.org/docs/latest/security/">Security</a> section of OPA&rsquo;s documentation.</p>

<h3>Optional APIs</h3>

<p>These APIs are specified by OPA  and you can opt to implement them in order to gain additional functionality and better integrate OPA into your system.</p>

<p>If you recall, in the first part of the series we imported data and policies into OPA by pushing it via the REST APIs. Pushing the data and policies into OPA is not always a feasible option. For example, if OPA is deployed as an ephemeral pod on Kubernetes, it would be difficult to ensure that the API call is made each time right after the pod has started.  How can you make OPA work in such scenarios? You can deploy a service which implements the <a href="https://www.openpolicyagent.org/docs/latest/bundles/#bundle-service-api">Bundle Service API</a> and configure OPA to periodically pull up-to-date policies and data from this service. In the simplest case, the service can be implemented as a static HTTP server that hosts the policy and data bundles. However, if your use case demands it, you can implement a service that generates policies and data for OPA on-the-fly. When pulling the policies and data, OPA checks the ETag to find out if a new version is available. The Bundle service allows you to ensure that your policies are consistent across many OPA instances and enables you to hot reload them at any time.</p>

<p>OPA can be configured using a set of static configuration files. In Kubernetes, you would likely manage these configuration files as ConfigMaps. However, OPA comes with its own mechanism to manage configuration files from a central place. You can implement the <a href="https://www.openpolicyagent.org/docs/latest/discovery/">Discovery Service API</a> to expose the configuration files as discovery bundles to OPA instances. On the startup, OPA will download its configuration from the Discovery Service.</p>

<p>OPA can send <a href="https://www.openpolicyagent.org/docs/latest/status/">status</a> updates to remote HTTP servers that implement a simple <a href="https://www.openpolicyagent.org/docs/latest/status/#status-service-api">Status Service API</a> . You will be notified whenever OPA downloads and actives a new bundle. Notifications are issued for both policy+data bundles and discovery bundles.</p>

<p>OPA can periodically report <a href="https://www.openpolicyagent.org/docs/latest/decision-logs/">decision logs</a> to remote HTTP servers that implement a <a href="https://www.openpolicyagent.org/docs/latest/decision-logs/#decision-log-service-api">Decision Log Service API</a>. The reported decision logs record all the policy decisions made by OPA. How could you make use of it? You could implement a simple Decision Log Service that would run co-located with the OPA service and that would store the decision logs into a durable storage like Kafka. And here you go, an awesome audit log was born!</p>

<h3>Health and Monitoring APIs</h3>

<p>OPA exposes a <a href="https://www.openpolicyagent.org/docs/latest/rest-api/#health-api">Health API</a> for you to periodically verify that the OPA service is operational. If you are deploying OPA on top of Kubernetes,  you can leverage the Health API to define the <a href="https://www.openpolicyagent.org/docs/latest/deployments/#readiness-and-liveness-probes">liveness and readiness probes</a>. OPA also exposes Prometheus metrics to <a href="https://www.openpolicyagent.org/docs/latest/monitoring/">monitor</a> the performance of OPA API calls. Both health and monitoring APIs can be secured the same way as the OPA REST API which we discussed before.</p>

<h2>Where to go from here?</h2>

<p>In addition to the written sources that you can find on the web, I would like to point you to a couple of excellent presentations about Open Policy Agent hosted on YouTube:</p>

<ul>
<li><a href="https://www.youtube.com/watch?v=CDDsjMOtJ-c">Intro: Open Policy Agent - Torin Sandall, Styra (2018)</a></li>
<li><a href="https://www.youtube.com/watch?v=n94_FNhuzy4">Deep Dive: Open Policy Agent - Torin Sandall &amp; Tim Hinrichs, Styra (2019)</a></li>
</ul>


<p>If you are evaluating Open Policy Agent from the security perspective, this <a href="https://github.com/open-policy-agent/opa/blob/master/SECURITY_AUDIT.pdf">audit report</a> might be of interest to you as well.</p>

<h2>Conclusion</h2>

<p>In this article, we discussed several ways for how you can integrate Open Policy Agent with your application. We also described the set of APIs defined by OPA that you can utilize to take full advantage of Open Policy Agent.</p>

<p>Open Policy Agent is a young and fast-moving project. Despite my rather short experience with OPA, I can already recommend that you consider using Open Policy Agent in your project before spending time on implementing your own domain specific language for writing policies or coding your policies in a general-purpose programming language. OPA will keep the policies consistent across your system, will allow you to hot reload the policies at any time and will make policy decisions with very low latency for you.</p>

<p>So far, each of the open source projects came up with its own way how to implement access control. System administrators have to learn how to grant access permissions in the Apache HTTP server, Kubernetes, and fill in your favorite open source project here. It would be great if OPA&rsquo;s Rego language would become an open standard for describing access control rules across the open source ecosystem. I hope that the future will show that this is possible.</p>

<p>Regardless of what other projects decide for themselves, we are planning to utilize Open Policy Agent in the SaaS project I am involved with as a consultant. As we are moving forward,  I intend to share the experience that we gain with OPA with you in some of the future blog posts.</p>

<p>I hope that you found this blog series about Open Policy Agent helpful. If you have any questions or comments, please leave them in the comment section below. I look forward to hearing from you.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Open Policy Agent, Part II &mdash; Developing Policies]]></title>
    <link href="https://alesnosek.com/blog/2019/10/27/open-policy-agent-part-ii-developing-policies/"/>
    <updated>2019-10-27T20:37:08-07:00</updated>
    <id>https://alesnosek.com/blog/2019/10/27/open-policy-agent-part-ii-developing-policies</id>
    <content type="html"><![CDATA[<p>In the <a href="/blog/2019/10/08/open-policy-agent-part-i-the-introduction/">previous part</a> of the series, we explored Open Policy Agent and implemented an ACL-based access control for our application. In this entry, I am going to share with you some of the discoveries that I made while evaluating Open Policy Agent in regards to policy design and development.</p>

<!-- more -->


<h2>Notes on Policy Design</h2>

<p>After evaluating policy rules, OPA returns a result of the policy decision to your application. This result is a JSON structure. Based on your requirements, this JSON structure can contain a single member holding a <em>true</em> or <em>false</em> (authorized/not authorized) value. However, you can create policies whose evaluation results in an arbitrarily complex JSON document. For example, OPA can return a list of nodes on which Kubernetes should schedule a workload.</p>

<p>In microservice applications, OAuth 2.0 is a rather popular authorization framework used to secure service’s APIs. It typically leverages JSON Web Tokens (JWT) to convey claims. OPA comes with built-in functions that can decode the token and validate its signature and expiration time. Furthermore, your policy rules can make decisions based on the claims included in the token. Just forward the token as an input to OPA and offload the entire token processing from your application!</p>

<p>OPA makes policy decisions based on the data stored in memory. In the case of large data sets, replicating all the data in memory can be impractical. While evaluating policy rules, is OPA able to reach out to an external data store to get additional data for decision making? For example, send a query to LDAP to grab additional attributes or look up data in an SQL database? Based on my research, I think there are two possible approaches for leveraging external data sources in OPA. First, there is a built-in <a href="https://www.openpolicyagent.org/docs/latest/language-reference/#http">HTTP</a> function that can fetch data from external HTTP services during policy evaluation. Second, you can leverage Partial Evaluation as described in this <a href="https://blog.openpolicyagent.org/write-policy-in-opa-enforce-policy-in-sql-d9d24db93bf4">blog post</a>. While partially evaluating policies, OPA doesn’t return a complete policy decision but instead it returns a set of conditions. It is left to you to translate this set of conditions into a query appropriate for your data store and execute the query in order to obtain the final policy decision. Note that regardless of which approach you choose, reaching out to external data stores will have negative impact on latency and reliability of your solution. Caching data in OPA’s memory is always a better option assuming that it suits your use case.</p>

<p>If you have raw data that would be difficult to write a policy against, you can pre-process that data into a form that better suits the policy writing before importing it into OPA. Moreover, if you have multiple sources of data, e.g. data from LDAP and Active Directory, you can merge them outside of OPA and load the merged form into OPA.</p>

<p>RBAC (Role-Based Access Control) and ABAC (Attribute-Based Access Control) are two frequently used policy models. Are you wondering if you can implement them using OPA? Of course you can! Follow these two links to find sample implementations of <a href="https://www.openpolicyagent.org/docs/latest/comparison-to-other-systems/#role-based-access-control-rbac">RBAC</a> and <a href="https://www.openpolicyagent.org/docs/latest/comparison-to-other-systems/#attribute-based-access-control-abac">ABAC</a>.</p>

<p>Hierarchical group permissions are commonly found in practice, e.g. parent group permissions are a superset of child group permissions. These models can be elegantly described using recursive rules. However, at the time of this writing, OPA doesn’t support <a href="https://github.com/open-policy-agent/opa/issues/947">recursion in policies</a>.</p>

<h2>Developing policies</h2>

<p>While learning the OPA’s Rego language, I appreciated the built-in interactive shell (REPL) that I could use to write and test my policies instantly. Just type <code>opa run</code> and you are good to go. Alternatively, you can go on-line and utilize the <a href="https://play.openpolicyagent.org/">Rego Playground</a>, too.</p>

<p>If you are dealing with complex policies, how do you ensure that you implemented your policies correctly? OPA <a href="https://www.openpolicyagent.org/docs/latest/how-do-i-test-policies/">allows</a> you to write test cases which you can run against your policies. You can use data mocking and calculate test coverage. See also the command <code>opa test</code>.</p>

<p>Is the evaluation of your policies too slow? OPA comes with a <a href="https://www.openpolicyagent.org/docs/latest/how-do-i-test-policies/#profiling">profiler</a> to report on time spent on evaluating policy expressions. See also the <code>opa eval</code> command.</p>

<p>OPA comes with a formatting tool <code>opa fmt</code> to format Rego policy files. You don’t need to fight battles with other developers about how the Rego files should be formatted!</p>

<p>OPA is a relatively new project, however, additional tooling and integrations with OPA are showing up quickly. If you like to use Visual Studio Code, there is a feature-rich <a href="https://marketplace.visualstudio.com/items?itemName=tsandall.opa">VS Code plugin</a> available for you. Rego syntax highlighting is available for several other editors like VIM, <a href="https://github.com/open-policy-agent/opa/tree/master/misc/syntax/atom">Atom</a>, and <a href="https://github.com/open-policy-agent/opa/tree/master/misc/syntax/textmate">TextMate</a>.</p>

<h2>Conclusion</h2>

<p>In this blog post, I shared with you several tips and approaches for how to design policies in Open Policy Agent. In the <a href="/blog/2019/12/03/open-policy-agent-part-iii-integrating-with-your-application/">final article</a> in the series we will focus on how you can integrate Open Policy Agent with your application.</p>

<p>If you have any comments or questions, please use the comment section below. I look forward to hearing from you.</p>
]]></content>
  </entry>

</feed>
