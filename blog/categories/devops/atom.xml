<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: devops | Ales Nosek - The Software Practitioner]]></title>
  <link href="http://alesnosek.com/blog/categories/devops/atom.xml" rel="self"/>
  <link href="http://alesnosek.com/"/>
  <updated>2016-11-03T22:44:17-07:00</updated>
  <id>http://alesnosek.com/</id>
  <author>
    <name><![CDATA[Ales Nosek]]></name>

  </author>
  <generator uri="http://octopress.org/">Octopress</generator>


  <entry>
    <title type="html"><![CDATA[5 Linux Commands You Didn't Know You Needed]]></title>
    <link href="http://alesnosek.com/blog/2016/11/01/5-linux-commands-you-didnt-know-you-needed/"/>
    <updated>2016-11-01T23:11:44-07:00</updated>
    <id>http://alesnosek.com/blog/2016/11/01/5-linux-commands-you-didnt-know-you-needed</id>
    <content type="html"><![CDATA[<p>When preparing for the RHCSA and RHCE exams, I found several useful commands I was not really aware of. In this blog post I&rsquo;ll share them with you.</p>

<!-- more -->


<h2>findmnt</h2>

<p>The <code>findmnt</code> command is part of the essential package <em>util-linux</em> and hence is available on pretty much all Linux systems. It can print all mounted filesystems in the tree-like format. I found the output of <code>findmnt</code> command more readable than the output provided by the more popular <code>mount</code> command. This is an example of how the filesystem mounts on a Ceph node look like:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>findmnt
</span><span class='line'>TARGET                           SOURCE     FSTYPE     OPTIONS
</span><span class='line'>/                                /dev/sda2  xfs        rw,relatime,seclabel,attr2,inode64,noquota
</span><span class='line'>├─/sys                           sysfs      sysfs      rw,nosuid,nodev,noexec,relatime,seclabel
</span><span class='line'>│ ├─/sys/kernel/security         securityfs securityfs rw,nosuid,nodev,noexec,relatime
</span><span class='line'>│ ├─/sys/fs/cgroup               tmpfs      tmpfs      ro,nosuid,nodev,noexec,seclabel,mode<span class="o">=</span>755
</span><span class='line'>│ │ ├─/sys/fs/cgroup/systemd     cgroup     cgroup     rw,nosuid,nodev,noexec,relatime,xattr,release_agent<span class="o">=</span>/usr/lib/systemd/systemd-cgroups-agent,name<span class="o">=</span>systemd
</span><span class='line'>│ │ ├─/sys/fs/cgroup/cpu,cpuacct cgroup     cgroup     rw,nosuid,nodev,noexec,relatime,cpuacct,cpu
</span><span class='line'>│ │ ├─/sys/fs/cgroup/perf_event  cgroup     cgroup     rw,nosuid,nodev,noexec,relatime,perf_event
</span><span class='line'>│ │ ├─/sys/fs/cgroup/devices     cgroup     cgroup     rw,nosuid,nodev,noexec,relatime,devices
</span><span class='line'>│ │ ├─/sys/fs/cgroup/blkio       cgroup     cgroup     rw,nosuid,nodev,noexec,relatime,blkio
</span><span class='line'>│ │ ├─/sys/fs/cgroup/cpuset      cgroup     cgroup     rw,nosuid,nodev,noexec,relatime,cpuset
</span><span class='line'>│ │ ├─/sys/fs/cgroup/hugetlb     cgroup     cgroup     rw,nosuid,nodev,noexec,relatime,hugetlb
</span><span class='line'>│ │ ├─/sys/fs/cgroup/net_cls     cgroup     cgroup     rw,nosuid,nodev,noexec,relatime,net_cls
</span><span class='line'>│ │ ├─/sys/fs/cgroup/memory      cgroup     cgroup     rw,nosuid,nodev,noexec,relatime,memory
</span><span class='line'>│ │ └─/sys/fs/cgroup/freezer     cgroup     cgroup     rw,nosuid,nodev,noexec,relatime,freezer
</span><span class='line'>│ ├─/sys/fs/pstore               pstore     pstore     rw,nosuid,nodev,noexec,relatime
</span><span class='line'>│ ├─/sys/fs/selinux              selinuxfs  selinuxfs  rw,relatime
</span><span class='line'>│ ├─/sys/kernel/debug            debugfs    debugfs    rw,relatime
</span><span class='line'>│ └─/sys/kernel/config           configfs   configfs   rw,relatime
</span><span class='line'>├─/proc                          proc       proc       rw,nosuid,nodev,noexec,relatime
</span><span class='line'>│ ├─/proc/sys/fs/binfmt_misc     systemd-1  autofs     rw,relatime,fd<span class="o">=</span>26,pgrp<span class="o">=</span>1,timeout<span class="o">=</span>300,minproto<span class="o">=</span>5,maxproto<span class="o">=</span>5,direct
</span><span class='line'>│ └─/proc/fs/nfsd                nfsd       nfsd       rw,relatime
</span><span class='line'>├─/dev                           devtmpfs   devtmpfs   rw,nosuid,seclabel,size<span class="o">=</span>16307108k,nr_inodes<span class="o">=</span>4076777,mode<span class="o">=</span>755
</span><span class='line'>│ ├─/dev/shm                     tmpfs      tmpfs      rw,nosuid,nodev,seclabel
</span><span class='line'>│ ├─/dev/pts                     devpts     devpts     rw,nosuid,noexec,relatime,seclabel,gid<span class="o">=</span>5,mode<span class="o">=</span>620,ptmxmode<span class="o">=</span>000
</span><span class='line'>│ ├─/dev/mqueue                  mqueue     mqueue     rw,relatime,seclabel
</span><span class='line'>│ └─/dev/hugepages               hugetlbfs  hugetlbfs  rw,relatime,seclabel
</span><span class='line'>├─/run                           tmpfs      tmpfs      rw,nosuid,nodev,seclabel,mode<span class="o">=</span>755
</span><span class='line'>│ └─/run/user/1002               tmpfs      tmpfs      rw,nosuid,nodev,relatime,seclabel,size<span class="o">=</span>3265340k,mode<span class="o">=</span>700,uid<span class="o">=</span>1002,gid<span class="o">=</span>1002
</span><span class='line'>├─/var/lib/nfs/rpc_pipefs        rpc_pipefs rpc_pipefs rw,relatime
</span><span class='line'>├─/var/lib/ceph/osd/ceph-1       /dev/sdb1  xfs        rw,noatime,seclabel,attr2,inode64,logbsize<span class="o">=</span>256k,sunit<span class="o">=</span>512,swidth<span class="o">=</span>512,noquota
</span><span class='line'>├─/var/lib/ceph/osd/ceph-3       /dev/sdc1  xfs        rw,noatime,seclabel,attr2,inode64,logbsize<span class="o">=</span>256k,sunit<span class="o">=</span>512,swidth<span class="o">=</span>512,noquota
</span><span class='line'>├─/var/lib/ceph/osd/ceph-10      /dev/sdg1  xfs        rw,noatime,seclabel,attr2,inode64,logbsize<span class="o">=</span>256k,sunit<span class="o">=</span>512,swidth<span class="o">=</span>512,noquota
</span><span class='line'>├─/var/lib/ceph/osd/ceph-9       /dev/sdf1  xfs        rw,noatime,seclabel,attr2,inode64,logbsize<span class="o">=</span>256k,sunit<span class="o">=</span>512,swidth<span class="o">=</span>512,noquota
</span><span class='line'>├─/var/lib/ceph/osd/ceph-4       /dev/sdd1  xfs        rw,noatime,seclabel,attr2,inode64,logbsize<span class="o">=</span>256k,sunit<span class="o">=</span>512,swidth<span class="o">=</span>512,noquota
</span><span class='line'>└─/var/lib/ceph/osd/ceph-7       /dev/sde1  xfs        rw,noatime,seclabel,attr2,inode64,logbsize<span class="o">=</span>256k,sunit<span class="o">=</span>512,swidth<span class="o">=</span>512,noquota
</span></code></pre></td></tr></table></div></figure></p>

<h2>ss</h2>

<p>The <code>ss</code> (soscket statistics) command is a replacement for the good old <code>netstat</code> command. It comes in the <em>iproute</em> package which is an essential part of all modern Linux distributions. I found <code>ss</code> command available on systems where the <code>netstat</code> command was missing. Here is a sample output of the <code>ss</code> command running on my Linux desktop:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>sudo ss -tlnp <span class="p">|</span> cat
</span><span class='line'>State      Recv-Q Send-Q Local Address:Port               Peer Address:Port
</span><span class='line'>LISTEN     <span class="m">0</span>      <span class="m">100</span>          &lt;em&gt;:15929                    &lt;/em&gt;:&lt;em&gt;                   users:<span class="o">((</span><span class="p">&amp;</span>ldquo<span class="p">;</span>skype<span class="p">&amp;</span>rdquo<span class="p">;</span>,pid<span class="o">=</span>25943,fd<span class="o">=</span>49<span class="o">))</span>
</span><span class='line'>LISTEN     <span class="m">0</span>      <span class="m">50</span>           &lt;/em&gt;:39964                    &lt;em&gt;:&lt;/em&gt;                   users:<span class="o">((</span><span class="p">&amp;</span>ldquo<span class="p">;</span>java<span class="p">&amp;</span>rdquo<span class="p">;</span>,pid<span class="o">=</span>6015,fd<span class="o">=</span>146<span class="o">))</span>
</span><span class='line'>LISTEN     <span class="m">0</span>      <span class="m">5</span>      192.168.122.1:53                       &lt;em&gt;:&lt;/em&gt;                   users:<span class="o">((</span><span class="p">&amp;</span>ldquo<span class="p">;</span>dnsmasq<span class="p">&amp;</span>rdquo<span class="p">;</span>,pid<span class="o">=</span>1625,fd<span class="o">=</span>6<span class="o">))</span>
</span><span class='line'>LISTEN     <span class="m">0</span>      <span class="m">128</span>          &lt;em&gt;:22                       &lt;/em&gt;:&lt;em&gt;                   users:<span class="o">((</span><span class="p">&amp;</span>ldquo<span class="p">;</span>sshd<span class="p">&amp;</span>rdquo<span class="p">;</span>,pid<span class="o">=</span>1442,fd<span class="o">=</span>3<span class="o">))</span>
</span><span class='line'>LISTEN     <span class="m">0</span>      <span class="m">128</span>         :::22                      :::&lt;/em&gt;                   users:<span class="o">((</span><span class="p">&amp;</span>ldquo<span class="p">;</span>sshd<span class="p">&amp;</span>rdquo<span class="p">;</span>,pid<span class="o">=</span>1442,fd<span class="o">=</span>4<span class="o">))</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>I&rsquo;m switching from using the <code>netstat</code> command to <code>ss</code>. What about you?</p>

<h2>ip</h2>

<p>After years of using the <code>ifconfig</code> utility, it took me some effort to move to its modern replacement - the <code>ip</code> command. Recently, I discovered two useful features of the <code>ip</code> utility.</p>

<p>To obtain a detailed information about the packets transferred by individual network interfaces, use the <code>-s</code> (statistics) parameter. For example:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>ip -s link
</span><span class='line'>1: lo: <span class="p">&amp;</span>lt<span class="p">;</span>LOOPBACK,UP,LOWER_UP&gt; mtu <span class="m">65536</span> qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1
</span><span class='line'>    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
</span><span class='line'>    RX: bytes  packets  errors  dropped overrun mcast
</span><span class='line'>    <span class="m">1416492</span>    <span class="m">20158</span>    <span class="m">0</span>       <span class="m">0</span>       <span class="m">0</span>       0
</span><span class='line'>    TX: bytes  packets  errors  dropped carrier collsns
</span><span class='line'>    <span class="m">1416492</span>    <span class="m">20158</span>    <span class="m">0</span>       <span class="m">0</span>       <span class="m">0</span>       0
</span><span class='line'>2: enp0s31f6: <span class="p">&amp;</span>lt<span class="p">;</span>BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span class="m">1500</span> qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000
</span><span class='line'>    link/ether 18:66:da:21:33:87 brd ff:ff:ff:ff:ff:ff
</span><span class='line'>    RX: bytes  packets  errors  dropped overrun mcast
</span><span class='line'>    <span class="m">13776256661</span> <span class="m">26486602</span> <span class="m">0</span>       <span class="m">0</span>       <span class="m">0</span>       1661059
</span><span class='line'>    TX: bytes  packets  errors  dropped carrier collsns
</span><span class='line'>    <span class="m">2313484427</span> <span class="m">9811792</span>  <span class="m">0</span>       <span class="m">0</span>       <span class="m">0</span>       0
</span></code></pre></td></tr></table></div></figure></p>

<p>To figure out which network interface would be used to send a packet to the specified IP address:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>ip route get 192.168.0.1
</span><span class='line'>192.168.0.1 via 10.5.0.1 dev enp0s31f6  src 10.5.0.225
</span><span class='line'>    cache
</span></code></pre></td></tr></table></div></figure></p>

<p>When sending a packet to the target destination <code>192.168.0.1</code>, the kernel will route the packet via the <code>enp0s31f6</code> interface. The IP <code>10.5.0.1</code> is my default route.</p>

<h2>lscpu</h2>

<p>On modern machines the output of <code>cat /proc/cpuinfo</code> can be really long. To find out what CPU configuration a machine comes with I prefer to use the <code>lscpu</code> command. This is an example output of the <code>lscpu</code> command running on an OpenStack compute node:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>lscpu
</span><span class='line'>Architecture:          x86_64
</span><span class='line'>CPU op-mode<span class="o">(</span>s<span class="o">)</span>:        32-bit, 64-bit
</span><span class='line'>Byte Order:            Little Endian
</span><span class='line'>CPU<span class="o">(</span>s<span class="o">)</span>:                32
</span><span class='line'>On-line CPU<span class="o">(</span>s<span class="o">)</span> list:   0-31
</span><span class='line'>Thread<span class="o">(</span>s<span class="o">)</span> per core:    2
</span><span class='line'>Core<span class="o">(</span>s<span class="o">)</span> per socket:    8
</span><span class='line'>Socket<span class="o">(</span>s<span class="o">)</span>:             2
</span><span class='line'>NUMA node<span class="o">(</span>s<span class="o">)</span>:          2
</span><span class='line'>Vendor ID:             GenuineIntel
</span><span class='line'>CPU family:            6
</span><span class='line'>Model:                 63
</span><span class='line'>Model name:            Intel<span class="p">&amp;</span>reg<span class="p">;</span> Xeon<span class="p">&amp;</span>reg<span class="p">;</span> CPU E5-2640 v3 @ 2.60GHz
</span><span class='line'>Stepping:              2
</span><span class='line'>CPU MHz:               1200.062
</span><span class='line'>BogoMIPS:              5198.45
</span><span class='line'>Virtualization:        VT-x
</span><span class='line'>L1d cache:             32K
</span><span class='line'>L1i cache:             32K
</span><span class='line'>L2 cache:              256K
</span><span class='line'>L3 cache:              20480K
</span><span class='line'>NUMA node0 CPU<span class="o">(</span>s<span class="o">)</span>:     0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30
</span><span class='line'>NUMA node1 CPU<span class="o">(</span>s<span class="o">)</span>:     1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31
</span></code></pre></td></tr></table></div></figure></p>

<p>In the above output, the interesting lines are the <code>Socket(s)</code>, <code>Core(s) per socket</code>, <code>Thread(s) per core</code> and <code>CPU(s)</code>. In our case, we&rsquo;re looking at a machine with 2 physical CPUs (Sockets), each of them having 8 physical cores (Cores per socket). Each of the physical cores has 2 processing threads (Threads per core) aka logical CPUs due to the Hyper-Threading technology. In total, there are 32 logical CPUs available to the Linux scheduler to schedule a task on.</p>

<h2>lspci</h2>

<p>The last command in our overview is the <code>lspci</code> command. If you ever wondered which kernel driver is controlling your hardware device, you can find out with:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>lspci -k&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;<span class="p">&amp;</span>hellip<span class="p">;</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;01:05.0 VGA compatible controller: Advanced Micro Devices, Inc. <span class="o">[</span>AMD/ATI<span class="o">]</span> RS880 <span class="o">[</span>Radeon HD 4250<span class="o">]</span>
</span><span class='line'>        Subsystem: ASUSTeK Computer Inc. M5A88-V EVO
</span><span class='line'>        Kernel driver in use: radeon
</span><span class='line'>01:05.1 Audio device: Advanced Micro Devices, Inc. <span class="o">[</span>AMD/ATI<span class="o">]</span> RS880 HDMI Audio <span class="o">[</span>Radeon HD <span class="m">4200</span> Series<span class="o">]</span>
</span><span class='line'>        Subsystem: ASUSTeK Computer Inc. M5A88-V EVO
</span><span class='line'>        Kernel driver in use: snd_hda_intel
</span><span class='line'>02:00.0 FireWire <span class="o">(</span>IEEE 1394<span class="o">)</span>: VIA Technologies, Inc. VT6315 Series Firewire Controller
</span><span class='line'>        Subsystem: ASUSTeK Computer Inc. M5A88-V EVO
</span><span class='line'>        Kernel driver in use: firewire_ohci
</span><span class='line'>02:00.1 IDE interface: VIA Technologies, Inc. VT6415 PATA IDE Host Controller <span class="o">(</span>rev a0<span class="o">)</span>
</span><span class='line'>        Subsystem: ASUSTeK Computer Inc. Motherboard
</span><span class='line'>        Kernel driver in use: pata_via
</span><span class='line'>03:00.0 USB controller: ASMedia Technology Inc. ASM1042 SuperSpeed USB Host Controller
</span><span class='line'>        Subsystem: ASUSTeK Computer Inc. P8B WS Motherboard
</span><span class='line'>        Kernel driver in use: xhci_hcd
</span><span class='line'>05:00.0 Network controller: Realtek Semiconductor Co., Ltd. RTL8192CE PCIe Wireless Network Adapter <span class="o">(</span>rev 01<span class="o">)</span>
</span><span class='line'>        Subsystem: Realtek Semiconductor Co., Ltd. RTL8192CE PCIe Wireless Network Adapter
</span><span class='line'>        Kernel driver in use: rtl8192ce
</span><span class='line'>06:00.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL8111/8168/8411 PCI Express Gigabit Ethernet Controller <span class="o">(</span>rev 06<span class="o">)</span>
</span><span class='line'>        Subsystem: ASUSTeK Computer Inc. P8P67 and other motherboards
</span><span class='line'>        Kernel driver in use: r8169
</span></code></pre></td></tr></table></div></figure></p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[First Impressions about ansible-container]]></title>
    <link href="http://alesnosek.com/blog/2016/09/12/first-impressions-about-ansible-container/"/>
    <updated>2016-09-12T20:06:49-07:00</updated>
    <id>http://alesnosek.com/blog/2016/09/12/first-impressions-about-ansible-container</id>
    <content type="html"><![CDATA[<p>At Red Hat summit I learned about the new project <a href="https://www.ansible.com/ansible-container">ansible-container</a>. I was very excited and looked forward to building Docker containers with Ansible instead of the Dockerfiles. The project seemed to came just on time as in our company we&rsquo;re starting to Dockerize our software products. How did <em>ansible-container</em> work out for us? Read on!</p>

<!-- more -->


<p>The ansible-container is a very young project. The first release 0.1.0 came out on July 28, 2016. The project&rsquo;s Git repository is hosted on <a href="https://github.com/ansible/ansible-container">GitHub</a>. Additionaly, there&rsquo;s a repository with examples of how to use ansible-container available <a href="https://github.com/ansible/ansible-container-examples">here</a>.</p>

<h2>How ansible-container works</h2>

<p>Here is how you build Docker containers with ansible-container. After crafting your Ansible playbook, you run the command <code>ansible-container build</code> which roughly executes these steps:</p>

<ol>
<li>Spin up a <em>builder container</em> and mount your Ansible playbook directory as a volume inside of this container.</li>
<li>Spin up a <em>base container</em> from the Docker image of your choice.</li>
<li>Run ansible-playbook inside of the <em>builder container</em>. Ansible will connect to the <em>base container</em> and configure it accordingly to the playbook.</li>
<li>Shutdown the <em>base container</em> and commit it as a new image.</li>
</ol>


<p>Besides building one Docker image at a time, it&rsquo;s rather straight forward to build a group of images at once. Under the hood, ansible-container leverages Docker Compose to start any number of containers to configure them with Ansible.</p>

<p>After your images are built, you can start them as containers on the local machine using the <code>ansible-container run</code> command. And finally, the <code>ansible-container shipit</code> command can help you to deploy your containers to Kubernetes or OpenShift.</p>

<h2>Evaluating ansible-container</h2>

<p>Software development in our company is organized in a way that can probably be found in many other places, too. We maintain base software libraries in a project called <em>platform</em>. On top of <em>platform</em>, many different product components are developed. We distribute our software in the form of RPM packages. As a first step on our way towards containers, we are going to install the RPMs on top of the Docker base images.</p>

<p>After spending a week with ansible-container, we decided that we are not going to adopt it at this time. The main reasons were:</p>

<ol>
<li><p>We would have to introduce another dependency to our build process. Our build machines would need to have Ansible, ansible-container and Docker Compose installed in order to build Docker containers. We prefer to keep our build dependencies to the minimum.</p></li>
<li><p>We need to build around 20 containers. During the container build we run a shell script to install an RPM package and to do a few more modifications. Our container build process is rather simple and Dockerfiles can just get the job done. For a more involved build process, Ansible would be preferable.</p></li>
<li><p>The ansible-container project shows its potential while it&rsquo;s still in its infancy. The builds with ansible-container take longer than the Dockerfile builds that leverage caching of image layers. The build specification file <code>container.yml</code> is not flexible enough for us. For example, it&rsquo;s not easy to build only a subset of the declared containers. Also it&rsquo;s not possible to build in one pass containers which inherit from each other.</p></li>
</ol>


<h2>Conclusion</h2>

<p>The ansible-container project is definitely very promising and we will keep our eye on it. For now, we&rsquo;re moving forward with generating and running Dockerfiles.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Deploying Kubernetes on OpenStack using Heat]]></title>
    <link href="http://alesnosek.com/blog/2016/06/26/deploying-kubernetes-on-openstack-using-heat/"/>
    <updated>2016-06-26T08:28:11-07:00</updated>
    <id>http://alesnosek.com/blog/2016/06/26/deploying-kubernetes-on-openstack-using-heat</id>
    <content type="html"><![CDATA[<p>Want to install Kubernetes on top of OpenStack? There are <a href="http://kubernetes.io/docs/getting-started-guides/">many ways</a> how to install a Kubernetes cluster. The upcoming Kubernetes 1.3 release comes with yet another method called <a href="http://kubernetes.io/docs/getting-started-guides/openstack-heat/">OpenStack Heat</a>. In this article, we&rsquo;re going to explore this deployment method when creating a minimum Kubernetes cluster on top of OpenStack.</p>

<!-- more -->


<p>In this tutorial, there are three OpenStack virtual machines involved. The first machine, called the <em>Kubernetes installer</em> machine, is created manually and is used for compiling Kubernetes from source and running the Kubernetes installer. The other two OpenStack machines, <em>Kubernetes master</em> and <em>Kubernetes node</em>, are created during the installation process.</p>

<p>The Kubernetes installer machine and both of the Kubernetes machines run on the CentOS-7-x86_64-GenericCloud-1605 image. You can download this image from the <a href="http://cloud.centos.org/centos/7/images/">CentOS image repository</a>. After I uploaded the CentOS 7 image into OpenStack, it has been assigned ID <code>17e4e783-321c-48c1-9308-6f99d67c5fa6</code> for me.</p>

<h2>Building Kubernetes from source</h2>

<p>First off, let&rsquo;s spin up a Kubernetes installer machine in OpenStack. I recommend using the <code>m1.large</code> flavor that comes with 8 GB of RAM. The compilation of Kubernetes is rather memory intensive.</p>

<p>To ensure consistent and reproducible builds, a Docker container is created at the beginning of the build process and the build proceeds within the container. So, let&rsquo;s quickly setup Docker on our build machine:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo yum install docker
</span></code></pre></td></tr></table></div></figure></p>

<p>Configure the Docker service to start on boot and then start it:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo systemctl <span class="nb">enable </span>docker
</span><span class='line'>sudo systemctl start docker
</span></code></pre></td></tr></table></div></figure></p>

<p>The Kubernetes build scripts expect that the <code>docker</code> command can successfully contact the Docker daemon. In the default CentOS configuration, the <code>sudo docker</code> is required in order to connect to the <code>/var/run/docker.sock</code> socket which is owned by the user root. To overcome the permission problem, let&rsquo;s create a wrapper script that will invoke the <code>docker</code> command using <code>sudo</code>:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>mkdir bin
</span><span class='line'><span class="nb">echo</span> -e <span class="p">&amp;</span>lsquo<span class="p">;</span><span class="c">#!/bin/bash\nexec sudo /usr/bin/docker &amp;ldquo;$@&amp;rdquo;&amp;rsquo; &gt; bin/docker</span>
</span><span class='line'>chmod <span class="m">755</span> bin/docker
</span><span class='line'><span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span>~/bin:<span class="nv">$PATH</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>You can test your changes with the <code>docker info</code> command which should work now.</p>

<p>Kubernetes is written in the Go language and its source code is stored in a Git repository. So, let&rsquo;s install the Go language environment and Git:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo yum install golang git
</span></code></pre></td></tr></table></div></figure></p>

<p>Next we&rsquo;ll clone the Kubernetes Git repository and start the build. The <code>quick-release</code> make target creates a build for the amd64 architecture only and doesn&rsquo;t run any tests.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>git clone &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://github.com/kubernetes/kubernetes.git&quot;</span>&gt;https://github.com/kubernetes/kubernetes.git&lt;/a&gt;
</span><span class='line'><span class="nb">cd </span>kubernetes
</span><span class='line'>make quick-release
</span></code></pre></td></tr></table></div></figure></p>

<p>After about 15 minutes when the build was successful, you&rsquo;ll find the distribution tarballs <code>kubernetes.tar.gz</code> and <code>kubernetes-salt.tar.gz</code> in the <code>_output/release-tars</code> directory.</p>

<h2>Setting up the OpenStack CLI tools</h2>

<p>The Kubernetes installer uses the OpenStack CLI tools to talk to OpenStack in order to create a Kubernetes cluster. Before you can install the OpenStack CLI tools on CentOS 7, you have to enable the OpenStack Mitaka RPM repository:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo yum install centos-release-openstack-mitaka
</span></code></pre></td></tr></table></div></figure></p>

<p>Install the OpenStack CLI tools that are used by the Kubernetes installer when creating a cluster with:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo yum install python-openstackclient python-swiftclient python-glanceclient python-novaclient python-heatclient
</span></code></pre></td></tr></table></div></figure></p>

<p>Next, you have to obtain your OpenStack <code>openrc.sh</code> file and source it into your environment:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>. openrc.sh
</span></code></pre></td></tr></table></div></figure></p>

<p>You should be able to talk to OpenStack now. For example, check if you can list the available OpenStack networks with:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>openstack network list
</span></code></pre></td></tr></table></div></figure></p>

<h2>Configuring the Kubernetes installer</h2>

<p>In this section, we&rsquo;re going to more or less follow the instructions found in the chapter <a href="http://kubernetes.io/docs/getting-started-guides/openstack-heat/">OpenStack Heat</a> of the Kubernetes documentation.</p>

<p>When deploying the Kubernetes cluster, the installer executes the following steps that you can find in <code>cluster/openstack-heat/util.sh</code>:</p>

<ul>
<li>Upload the distribution tarballs <code>kubernetes.tar.gz</code> and <code>kubernetes-salt.tar.gz</code> into the <code>kubernetes</code> container in Swift</li>
<li>Upload the virtual machine image for the Kubernetes VMs into Glance</li>
<li>Add the user&rsquo;s keypair into Nova</li>
<li>Run a Heat script in order to create the Kubernetes VMs and put them on a newly created private network. Create a router connecting the private network with an external network.</li>
<li>At the first boot, the Kubernetes VMs download the distribution tarballs from Swift and install the Kubernetes software using Salt</li>
</ul>


<p>Let&rsquo;s create an <code>openstack-heat.sh</code> file with the configuration values for the Kubernetes installer:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nb">export </span><span class="nv">KUBERNETES_PROVIDER</span><span class="o">=</span>openstack-heat
</span><span class='line'><span class="nb">export </span><span class="nv">STACK_NAME</span><span class="o">=</span>kubernetes
</span><span class='line'><span class="nb">export </span><span class="nv">KUBERNETES_KEYPAIR_NAME</span><span class="o">=</span>mykeypair
</span><span class='line'><span class="nb">export </span><span class="nv">NUMBER_OF_MINIONS</span><span class="o">=</span>1
</span><span class='line'><span class="nb">export </span><span class="nv">MAX_NUMBER_OF_MINIONS</span><span class="o">=</span>1
</span><span class='line'><span class="nb">export </span><span class="nv">EXTERNAL_NETWORK</span><span class="o">=</span>gateway
</span><span class='line'><span class="nb">export </span><span class="nv">CREATE_IMAGE</span><span class="o">=</span><span class="nb">false</span>
</span><span class='line'><span class="nb">export </span><span class="nv">DOWNLOAD_IMAGE</span><span class="o">=</span><span class="nb">false</span>
</span><span class='line'><span class="nb">export </span><span class="nv">IMAGE_ID</span><span class="o">=</span>17e4e783-321c-48c1-9308-6f99d67c5fa6
</span><span class='line'><span class="nb">export </span><span class="nv">DNS_SERVER</span><span class="o">=</span>10.0.0.10
</span><span class='line'><span class="nb">export </span><span class="nv">SWIFT_SERVER_URL</span><span class="o">=</span>&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://openstack.localdomain:13808/swift/v1&quot;</span>&gt;https://openstack.localdomain:13808/swift/v1&lt;/a&gt;
</span></code></pre></td></tr></table></div></figure></p>

<p>The above configuration will create exactly one Kubernetes master and one Kubernetes node. It will inject the keypair called <code>mykeypair</code> into both of them. Note that you have to ensure that the keypair <code>mykeypair</code> exists in Nova before proceeding. You probably want to change the name of the external network to a network available in your OpenStack. We&rsquo;re going to use the same CentOS 7 image for both of our Kubernetes VMs. This CentOS image has already been uploaded into OpenStack and in my case it was assigned ID <code>17e4e783-321c-48c1-9308-6f99d67c5fa6</code>. You also want to change the IP address of the DNS server to something that suits your environment. The Swift server URL is the public endpoint of your Swift server that you can obtain from the output of the command <code>openstack catalog show object-store</code>.</p>

<p>When your configuration is ready, you can source it into your environment:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>. openstack-heat.sh
</span></code></pre></td></tr></table></div></figure></p>

<p>Next, in my environment I had a problem where the IP range of the private network created by Kubernetes collided with the existing corporate network in my company. I had to directly edit the file <code>cluster/openstack-heat/kubernetes-heat/kubecluster.yaml</code> to change the <code>10.0.0.0/24</code> CIDR to something like <code>10.123.0.0/24</code>. If you don&rsquo;t have this problem you can safely use the default settings.</p>

<p>The Kubernetes cluster can leverage the underlying OpenStack cloud to attach existing Cinder volumes to the Kubernetes pods and to create external loadbalancers. For this to work, Kubernetes has to know how to connect to OpenStack APIs. With regard to the external loadbalancers, we also need to tell Kubernetes what Neutron subnet the loadbalancer&rsquo;s VIP should be placed on.</p>

<p>The OpenStack configuration can be found in the <em>cloud-config</em> script <code>cluster/openstack-heat/kubernetes-heat/fragments/configure-salt.yaml</code>. You can see that this script will create a configuration file <code>/srv/kubernetes/openstack.conf</code> on the Kubernetes machine which contains the OpenStack settings. In my case, I changed the original block:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[Global]
</span><span class='line'>auth-url=$OS_AUTH_URL
</span><span class='line'>username=$OS_USERNAME
</span><span class='line'>password=$OS_PASSWORD
</span><span class='line'>region=$OS_REGION_NAME
</span><span class='line'>tenant-id=$OS_TENANT_ID</span></code></pre></td></tr></table></div></figure></p>

<p>to read:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[Global]
</span><span class='line'>auth-url=$OS_AUTH_URL
</span><span class='line'>username=$OS_USERNAME
</span><span class='line'>password=$OS_PASSWORD
</span><span class='line'>region=$OS_REGION_NAME
</span><span class='line'>tenant-id=$OS_TENANT_ID
</span><span class='line'>domain-name=MyDomain # Keystone V3 domain
</span><span class='line'>[LoadBalancer]
</span><span class='line'>lb-version=v1
</span><span class='line'>subnet-id=73f8eb91-90cf-42f4-85d0-dcff44077313</span></code></pre></td></tr></table></div></figure></p>

<p>Besides adding the <code>LoadBalancer</code> section, I also appended the <code>domain-name</code> option to the end of the <code>Global</code> section, as in my OpenStack environment I want to authenticate against a non-default Keystone V3 domain.</p>

<h2>Installing the Kubernetes cluster</h2>

<p>After you&rsquo;ve sourced both the <code>openrc.sh</code> and <code>openstack-heat.sh</code> environment settings, you can kick off the installation of the Kubernetes cluster with:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>./cluster/kube-up.sh
</span></code></pre></td></tr></table></div></figure></p>

<p>After about 25 minutes, you should have a Kubernetes cluster up and running. You can check the status of the Kubernetes pods with the command:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>./cluster/kubectl.sh get pods <span class="p">&amp;</span>ndash<span class="p">;</span>namespace kube-system
</span></code></pre></td></tr></table></div></figure></p>

<p>All pods should be running. The network topology of the Kubernetes cluster as displayed by Horizon:</p>

<p><img class="center" src="/images/posts/kube.png"></p>

<h2>Accessing the Kubernetes cluster</h2>

<p><strong> Update 9/5/2016 </strong></p>

<p>At first, we will copy the <code>kubectl</code> client binary from the Kubernetes installer machine onto the remote host from where we are going to access our Kubernetes cluster:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>scp ./_output/release-stage/client/linux-amd64/kubernetes/client/bin/kubectl user@remote.host.com:
</span></code></pre></td></tr></table></div></figure></p>

<p>Remember to replace the <code>remote.host.com</code> with the name of your remote machine.</p>

<p>Next, we&rsquo;re going to start a kubectl proxy to allow access to Kubernetes APIs and the web UI from the remote host. The proxy can be brought up directly on the Kubernetes installer machine using the command:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>./cluster/kubectl.sh proxy <span class="p">&amp;</span>ndash<span class="p">;</span><span class="nv">address</span><span class="o">=</span>0.0.0.0 <span class="p">&amp;</span>ndash<span class="p">;</span><span class="nv">port</span><span class="o">=</span><span class="m">8080</span> <span class="p">&amp;</span>ndash<span class="p">;</span>accept-hosts<span class="o">=</span>.*
</span></code></pre></td></tr></table></div></figure></p>

<p>The proxy listens on port 8080 on all network interfaces and accepts connections from remote hosts with any IP address. This configuration is very unsecure but is good enough for our test environment. If your Kubernetes installer machine runs on the cloud, you might want to modify the security group rules to provide access to port 8080.</p>

<p>Now, we can access the Kubernetes APIs from the remote machine using the command:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>./kubectl <span class="p">&amp;</span>ndash<span class="p">;</span>server &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;http://kubernetes.installer.com:8080&quot;</span>&gt;http://kubernetes.installer.com:8080&lt;/a&gt; cluster-info
</span></code></pre></td></tr></table></div></figure></p>

<p>The web UI of your Kubernetes cluster should be available at the URL:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;a href="http://kubernetes.installer.com:8080/ui">http://kubernetes.installer.com:8080/ui&lt;/a></span></code></pre></td></tr></table></div></figure></p>

<p>Note that you want to replace the <code>kubernetes.installer.com</code> with the name of your Kubernetes installer machine. When accessing the Kubernetes installer machine on port 8080, the request will be forwarded to your Kubernetes master node.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Bootstrapping a Galera Cluster on RHEL7]]></title>
    <link href="http://alesnosek.com/blog/2016/01/31/bootstrapping-a-galera-cluster-on-rhel7/"/>
    <updated>2016-01-31T15:24:55-08:00</updated>
    <id>http://alesnosek.com/blog/2016/01/31/bootstrapping-a-galera-cluster-on-rhel7</id>
    <content type="html"><![CDATA[<p>The MariaDB Galera packages provided by the RDO project in their OpenStack repositories don&rsquo;t seem to include a command or script to bootstrap the cluster. Let&rsquo;s look at an alternative way to bring the cluster up.</p>

<!-- more -->


<p>RHEL7 comes with the init system <code>systemd</code>. Unfortunately, systemd doesn&rsquo;t provide a way to pass command-line arguments to the unit files. Hence, doing something like this won&rsquo;t work:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@rhel1 ~<span class="o">]</span><span class="nv">$ </span>systemctl start mariadb <span class="p">&amp;</span>ndash<span class="p">;</span>wsrep_new_cluster
</span><span class='line'>systemctl: unrecognized option <span class="p">&amp;</span>lsquo<span class="p">;&amp;</span>ndash<span class="p">;</span>wsrep_new_cluster<span class="p">&amp;</span>rsquo<span class="p">;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Instead of passing command-line arguments, systemd allows for creating <a href="http://0pointer.de/blog/projects/instances.html">multiple instances</a> of the same service where each instance can obtain it&rsquo;s own set of environment variables. The Percona XtraDB Cluster includes the standard and the bootstrap service instance definitions in the RPM package <code>Percona-XtraDB-Cluster-server</code>. To boostrap the Percona cluster, the first node can be started with the following command:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@percona1 ~<span class="o">]</span><span class="nv">$ </span>systemctl start &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;&amp;#109;&amp;#97;&amp;#x69;&amp;#108;&amp;#116;&amp;#111;&amp;#x3a;&amp;#109;&amp;#x79;&amp;#x73;&amp;#x71;&amp;#108;&amp;#64;&amp;#98;&amp;#111;&amp;#x6f;&amp;#116;&amp;#115;&amp;#116;&amp;#114;&amp;#x61;&amp;#x70;&amp;#46;&amp;#x73;&amp;#101;&amp;#114;&amp;#118;&amp;#105;&amp;#99;&amp;#101;&quot;</span>&gt;<span class="p">&amp;</span><span class="c">#109;&amp;#121;&amp;#x73;&amp;#113;&amp;#108;&amp;#x40;&amp;#98;&amp;#111;&amp;#x6f;&amp;#x74;&amp;#x73;&amp;#116;&amp;#x72;&amp;#97;&amp;#112;&amp;#46;&amp;#x73;&amp;#101;&amp;#114;&amp;#x76;&amp;#105;&amp;#99;&amp;#x65;&lt;/a&gt;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>At the moment, this boostrap service definition is missing in the RDO OpenStack packages. Before a similar <code>mysql@.service</code> script is available in RDO you can start the MariaDB Galera cluster as follows:</p>

<ul>
<li><p>On the first node, start the MariaDB with the <code>--wsrep-new-cluster</code> to create a new cluster:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@rhel1 ~<span class="o">]</span><span class="nv">$ </span>/usr/bin/mysqld_safe <span class="p">&amp;</span>ndash<span class="p">;</span>wsrep-new-cluster
</span></code></pre></td></tr></table></div></figure>
Let the command run in the foreground.</p></li>
<li><p>On the remaining cluster nodes start the mariadb service as usual:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@rhel2 ~<span class="o">]</span><span class="nv">$ </span>systemctl start mariadb
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>After the cluster has been fully formed, stop the mariadb on the first node by sending it a SIGQUIT (press CTRL + \ on the console).</p></li>
<li><p>On the first node, start the mariadb service via systemd:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@rhel1 ~<span class="o">]</span><span class="nv">$ </span>systemctl start mariadb
</span></code></pre></td></tr></table></div></figure></p></li>
</ul>


<p>That&rsquo;s it. You can check the status of each of the cluster nodes by running the following command:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@rhel1 ~<span class="o">]</span><span class="nv">$ </span>mysql -e <span class="p">&amp;</span>ldquo<span class="p">;</span>SHOW GLOBAL STATUS LIKE <span class="p">&amp;</span>lsquo<span class="p">;</span>wsrep%<span class="p">&amp;</span>rsquo<span class="p">;;&amp;</span>rdquo<span class="p">;</span>
</span></code></pre></td></tr></table></div></figure></p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Monitoring OpenStack Cluster with Icinga]]></title>
    <link href="http://alesnosek.com/blog/2015/11/30/monitoring-openstack-cluster-with-icinga/"/>
    <updated>2015-11-30T21:13:33-08:00</updated>
    <id>http://alesnosek.com/blog/2015/11/30/monitoring-openstack-cluster-with-icinga</id>
    <content type="html"><![CDATA[<p>If you don&rsquo;t monitor it, it&rsquo;s not in production! To get an OpenStack cloud ready for production, monitoring is a must. Let&rsquo;s take a look at two projects providing Nagios/Icinga plugins for checking the health of OpenStack services.</p>

<!-- more -->


<p>First, a few words about <a href="https://www.icinga.org/" title="Icinga">Icinga</a>. I started using Icinga 2 only recently and I&rsquo;m very pleased with this flexible and well-documented software. I&rsquo;ve listened to a German presentation about Icinga where they said that Icinga was not that widely spread in the US as it was the case in Europe. Dear Icinga team, you have one more happy user in the US now. Your software just works and your web GUI is beautiful.</p>

<p>I found two very useful projects for monitoring the OpenStack APIs both hosted on GitHub:</p>

<ul>
<li><a href="https://github.com/cirrax/openstack-nagios-plugins">OpenStack Nagios Plugins</a></li>
<li><a href="https://github.com/openstack/monitoring-for-openstack">Monitoring for OpenStack</a></li>
</ul>


<h2>OpenStack Nagios Plugins</h2>

<p><a href="https://github.com/cirrax/openstack-nagios-plugins">OpenStack Nagios Plugins</a> provides a collection of checks for the OpenStack services Nova, Neutron, Cinder, Keystone and Ceilometer. Available plugins worked right away with my OpenStack Liberty cluster. The Nova Hypervisor check monitors the &ldquo;virtual&rdquo; CPU and memory usage across your compute nodes. The name virtual CPU is a little misleading here. In reality, the number of physical cores is monitored as the Nova API actually reports the number of physical cores. I stick to the OpenStack default settings that overcommit the CPUs by factor of 16 and the memory by factor of 1.5. To accommodate this fact, I changed the warning and critical ranges for the check_nova-hypervisors plugin as follows:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>check_nova-hypervisors <span class="p">&amp;</span>ndash<span class="p">;</span>warn_memory_percent 0:135 <span class="p">&amp;</span>ndash<span class="p">;</span>critical_memory_percent 0:142 <span class="p">&amp;</span>ndash<span class="p">;</span>warn_vcpus_percent 0:1440 <span class="p">&amp;</span>ndash<span class="p">;</span>critical_vcpus_percent 0:1520
</span></code></pre></td></tr></table></div></figure></p>

<h2>Monitoring for OpenStack</h2>

<p>Plugins coming with the <a href="(https://github.com/openstack/monitoring-for-openstack">Monitoring for OpenStack</a> project provide deeper checks of OpenStack functionality. I liked the following ones the best:</p>

<ul>
<li><code>check_nova_instance</code>: Creates an instance on your cloud and deletes it again as soon as it is active. It&rsquo;s recommended to use a small disk image like cirros for this check.</li>
<li><code>cinder_volume</code>: Allocates a volume of size 1GB and deletes it again.</li>
<li><code>neutron_floating_ip</code>: Tries to allocate a floating IP. You have to configure the network where to allocate the IP from.</li>
<li><code>glance_upload</code>: Uploads 1MB of data as an image into Glance.</li>
<li><code>check_horizon_login</code>: Given a user name and a password the plugin will log into the Horizon dashboard.</li>
</ul>


<p>Some of the plugins didn&rsquo;t work for me due to incompatibilities with the Liberty client APIs. If you encounter the same problem you can try out my fixed version of the plugins on GitHub <a href="https://github.com/noseka1/monitoring-for-openstack">here</a>.</p>

<h2>Icinga 2 Screenshot</h2>

<p>And this is how the OpenStack APIs service group looks in Icinga Web 2. Happy monitoring!</p>

<p><img class="center" src="/images/posts/osmon.png"></p>
]]></content>
  </entry>

</feed>
