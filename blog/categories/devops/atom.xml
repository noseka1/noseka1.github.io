<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: devops | Ales Nosek - The Software Practitioner]]></title>
  <link href="http://alesnosek.com/blog/categories/devops/atom.xml" rel="self"/>
  <link href="http://alesnosek.com/"/>
  <updated>2020-09-13T18:57:22-07:00</updated>
  <id>http://alesnosek.com/</id>
  <author>
    <name><![CDATA[Ales Nosek]]></name>

  </author>
  <generator uri="http://octopress.org/">Octopress</generator>


  <entry>
    <title type="html"><![CDATA[Apache Airflow Architecture on OpenShift]]></title>
    <link href="http://alesnosek.com/blog/2020/09/13/apache-airflow-architecture-on-openshift/"/>
    <updated>2020-09-13T18:48:20-07:00</updated>
    <id>http://alesnosek.com/blog/2020/09/13/apache-airflow-architecture-on-openshift</id>
    <content type="html"><![CDATA[<p>This blog will walk you through the Apache Airflow architecture on OpenShift. We are going to discuss the function of the individual Airflow components and how they can be deployed to OpenShift. This article focuses on the latest Apache Airflow version 1.10.12.</p>

<!-- more -->


<h2>Architecture overview</h2>

<p>The three main components of Apache Airflow are the Webserver, Scheduler, and Workers. The Webserver provides the Web UI which is the Airflow&rsquo;s main user interface. It allows users to visualize their DAGs (Directed Acyclic Graph) and control the execution of their DAGs. In addition to the Web UI, the Webserver also provides an experimental REST API that allows controlling Airflow programatically as opposed to through the Web UI. The second component &mdash; the Airflow Scheduler &mdash; orchestrates the execution of DAGs by starting the DAG tasks at the right time and in the right order. Both Airflow Webserver and Scheduler are long-running services. On the other hand, Airflow Workers &mdash; the last of the three main components &mdash; run as ephemeral pods. They are created by the Kubernetes Executor and their sole purpose is to execute a single DAG task. After the task execution is complete, the Worker pod is deleted. The following diagram depicts the Aiflow architecture on OpenShift:</p>

<p><img src="/images/posts/apache_airflow_architecture_on_openshift.png"></p>

<h2>Shared database</h2>

<p>As shown in the architecture diagram above, none of the Airflow components communicate directly with each other. Instead, they all read and modify the state that is stored in the <em>shared database</em>. For instance,  the Webserver reads the current state of the DAG execution from the database and displays it in the Web UI. If you trigger a DAG in the Web UI, the Webserver will update the DAG in the database accordingly. Next comes the Scheduler that checks the DAG state in the database periodically. It finds the triggered DAG and if the time is right, it will schedule the new tasks for execution. After the execution of the specific task is complete, the Worker marks that state of the task in the database as done. Finally, the Web UI will learn the new state of the task from the database and will show it to the user.</p>

<p>The shared database architecture provides Airflow components with a perfectly consistent view of the current state. On the other hand, as the number of tasks to execute grows, the database becomes a performance bottleneck as more and more Workers connect to the database. To alleviate the load on the database, a connection pool like <a href="https://www.pgbouncer.org/">PgBouncer</a> may be deployed in front of the database. The pool manages a relatively small amount of database connections which are re-used to serve requests of different Workers.</p>

<p>Regarding the choice of a particular DBMS, in production deployments the database of choice is typically PostgreSQL or MySQL. You can choose to run the database directly on OpenShift. In that case, you will need to put it on an RWO (ReadWriteOnce) persistent volume provided for example by <a href="https://www.redhat.com/en/technologies/cloud-computing/openshift-container-storage">OpenShift Container Storage</a>. Or, you can use an external database. For instance, if you are hosting OpenShift on top of AWS, you can leverage a fully managed database provided by <a href="https://aws.amazon.com/rds/">Amazon RDS</a>.</p>

<h2>Making DAGs accessible to Airflow components</h2>

<p>All three Airflow components Webserver, Scheduler, and Workers assume that the DAG definitions can be read from the local filesystem.  The question is, how to make the DAGs available on the local filesystem in the container? There are two approaches to achieve this. In the first approach, a shared volume is created to hold all the DAGs. This volume is then attached to the Airflow pods. The second approach assumes that your DAGs are hosted in a git repository. A sidecar container is deployed along with the Airflow Server and Scheduler. This sidecar container synchronizes the latest version of your DAGs with the local filesystem periodically. For the Worker pods, the pulling of the DAGs from the git repository is done only once by the init container before the Worker is brought up.</p>

<p>Note that since Airflow 1.10.10, you can use the <a href="https://airflow.apache.org/docs/1.10.10/dag-serialization.html">DAG Serialization</a> feature. With DAG Serialization, the Scheduler reads the DAGs from the local filesystem and saves them in the database. The Airflow Webserver then reads the DAGs from the database instead of the local filesystem. For the Webserver container, you can avoid the need to mount a shared volume or configure git-sync if you enable the DAG Serialization.</p>

<p>To synchronize the DAGs with the local filesystem, I personally prefer using git-sync over the shared volumes approach. First, you want to keep you DAGs in the source control anyway to facilitate the development of the DAGs. Second, git-sync seems to be easier to troubleshoot and recover in the case of failure.</p>

<h2>Airflow monitoring</h2>

<p>As the old saying goes, &ldquo;If you are not monitoring it, it&rsquo;s not in production&rdquo;. So, how can we monitor Apache Airflow running on OpenShift? <a href="https://prometheus.io/">Prometheus</a> is a monitoring system widely used for monitoring Kubernetes workloads and I recommend that you consider it for monitoring Airflow as well. Airflow itself reports metrics using the statsd protocol, so you will need to deploy the <a href="https://github.com/prometheus/statsd_exporter">statsd_exporter</a> piece between Airflow and the Prometheus server. This exporter will aggregate the statsd metrics, convert them into Prometheus format and expose them for the Prometheus server to scrape.</p>

<h2>Collecting Airflow logs</h2>

<p>By default, Apache Airflow writes the logs to the local filesystem. If you have an RWX (ReadWriteMany) persistent volume available, you can attach it to the Webserver, Scheduler, and Worker pods to capture the logs. As the Worker logs are written to the shared volume, they are instantly accessible by the Webserver. This allows for viewing the logs live in the Web UI.</p>

<p>An alternative approach to handling the Airflow logs is to enable remote logging.  With remote logging, the Worker logs can be pushed to the remote location like S3. The logs are then grabbed from S3 by the Webserver to display them in the Web UI. Note that when using an object store as your remote location, the Worker logs are uploaded to the object store only after the task run is complete. That means that you won&rsquo;t be able to view the logs live in the Web UI while the task is still running.</p>

<p>The remote logging feature in Airflow takes care of the Worker logs. How can you handle Webserver and Scheduler logs when not using a persistent volume? You can configure Airflow to dump the logs to stdout. The OpenShift logging will collect the logs and send them to the central location.</p>

<h2>Conclusion</h2>

<p>In this article, we reviewed the Apache Airflow architecture on OpenShift. We discussed the role of individual Airflow components and described how they interact with each other. We discussed the Airflow&rsquo;s shared database, explained how to make DAGs accessible to the Airflow components, and talked about Ariflow monitoring and log collection.</p>

<p>Apache Airflow can be deployed in several different ways. What is your favorite architecture for deploying Airflow? I would like to hear about your approach. If you have any further questions or comments, please add them to the comment section below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[CI/CD Pipeline Spanning Multiple OpenShift Clusters]]></title>
    <link href="http://alesnosek.com/blog/2020/06/30/ci-slash-cd-pipeline-spanning-multiple-openshift-clusters/"/>
    <updated>2020-06-30T17:53:09-07:00</updated>
    <id>http://alesnosek.com/blog/2020/06/30/ci-slash-cd-pipeline-spanning-multiple-openshift-clusters</id>
    <content type="html"><![CDATA[<p>This blog will cover how to create a CI/CD pipeline that spans multiple OpenShift clusters. It will show an example of a Jenkins-based pipeline, and design a pipeline that uses Tekton.</p>

<!-- more -->


<p>Traditionally, CI/CD pipelines were implemented on top of bare metal servers and virtual machines. Container platforms like Kubernetes and OpenShift appeared on the scene only later on. As more and more workloads are migrating to OpenShift, CI/CD pipelines are headed in the same direction. Pipeline jobs are executed in containers in the cluster.</p>

<p>In the real world, companies don&rsquo;t deploy a single OpenShift cluster but run multiple clusters. Why is that? They want to run their workloads in different public clouds as well as on-premise. Or, if they leverage a single platform provider, they want to run in multiple regions. Sometimes there is a need for multiple clusters in a single region, too. For example, when each cluster is deployed into a different security zone.</p>

<p>As there are plenty of reasons to use multiple OpenShift clusters, there is a need to create CI/CD pipelines that work across those clusters. The next sections are going to design such pipelines.</p>

<h2>CI/CD pipeline using Jenkins</h2>

<p>Jenkins is a legend among CI/CD tools. I remember meeting Jenkins back in the day when it was called Hudson but that&rsquo;s old history. How can we build a Jenkins pipeline that spans multiple OpenShift clusters? An important design goal for the pipeline is to achieve a single dashboard that can display output of all jobs involved in the pipeline. I gave it some thought and realized that achieving a single dashboard pretty much implies using a single Jenkins master. This Jenkins master is connected with each of the OpenShift clusters. During the pipeline execution, Jenkins master can run individual tasks on any of the clusters. The job output logs are collected and sent to the master as usual. If we consider having three OpenShift clusters Dev, Test, and Prod, the following diagram depicts the approach:</p>

<p><img class="center" src="/images/posts/ci_cd_pipeline_spanning_multiple_clusters_jenkins.png"></p>

<p>The Jenkins <a href="https://plugins.jenkins.io/kubernetes/">Kubernetes plugin</a> is a perfect plugin for connecting Jenkins to OpenShift. It allows the Jenkins master to create ephemeral workers on the cluster. Each cluster can be assigned a different node label. You can run each stage of your pipeline on a different cluster by specifying the label. A simple pipeline definition for our example would look like this:</p>

<pre><code>stage ('Build') {
  node ("dev") {
    // running on dev cluster
  }
}

stage ('Test') {
  node ("test") {
    // running on test cluster
  }
}

stage ('Prod') {
  node ("prod") {
    // running on prod cluster
  }
}
</code></pre>

<p>OpenShift comes with a Jenkins template which can be found in the <code>openshift</code> project. This template allows you to create a Jenkins master that is pre-configured to spin up worker pods on the same cluster. Further effort will be needed to connect this master to additional OpenShift clusters. A tricky part of this set up is networking. Jenkins worker pod, after it starts up, connects back to the Jenkins master. This requires the master to be reachable from the worker running on any of the OpenShift clusters.</p>

<p>One last point I wanted to discuss is security. As long as the Jenkins master can spin up worker pods on OpenShift, it can execute arbitrary code on those workers. The OpenShift cluster has no means to control what code the Jenkins worker will run. The job definition is managed by Jenkins and it is solely up to the access controls in Jenkins to enforce which job is allowed to execute on which cluster.</p>

<h2>Kubernetes-native Tekton pipeline</h2>

<p>In this section, we are going to use Tekton to implement the CI/CD pipeline. In contrast to Jenkins, Tekton is a Kubernetes-native solution. It is implemented using Kubernetes building blocks and it is tightly integrated with Kubernetes. A single Kubernetes cluster is a natural boundary for Tekton. So, how can we build a Tekton pipeline that spans multiple OpenShift clusters?</p>

<p>I came up with an idea of composing the Tekton pipelines. To compose multiple pipelines into a single pipeline, I implemented the <a href="https://github.com/noseka1/execute-remote-pipeline">execute-remote-pipeline</a> task that can execute a Tekton pipeline located on a remote OpenShift cluster. The task will tail the output of the remote pipeline while the remote pipeline is executing. With the help of this task, I can now combine Tekton pipelines across OpenShift clusters and run them as a single pipeline. For example, the diagram below shows a composition of three pipelines. Each of the pipelines is located on a different OpenShift cluster Dev, Test, and Prod:</p>

<p><img class="center" src="/images/posts/ci_cd_pipeline_spanning_multiple_clusters_tekton.png"></p>

<p>The execution of this pipeline is started on the Dev cluster. The Dev pipeline will trigger the Test pipeline which will in turn trigger the Prod pipeline. The combined logs can be followed on the terminal:</p>

<pre><code>$ tkn pipeline start dev --showlog
Pipelinerun started: dev-run-bd5fs
Waiting for logs to be available...
[execute-remote-pipeline : execute-remote-pipeline-step] Logged into "https://api.cluster-affc.sandbox1480.opentlc.com:6443" as "system:serviceaccount:test-pipeline:pipeline-starter" using the token provided.
[execute-remote-pipeline : execute-remote-pipeline-step]
[execute-remote-pipeline : execute-remote-pipeline-step] You have one project on this server: "test-pipeline"
[execute-remote-pipeline : execute-remote-pipeline-step]
[execute-remote-pipeline : execute-remote-pipeline-step] Using project "test-pipeline".
[execute-remote-pipeline : execute-remote-pipeline-step] Welcome! See 'oc help' to get started.
[execute-remote-pipeline : execute-remote-pipeline-step] [execute-remote-pipeline : execute-remote-pipeline-step] Logged into "https://api.cluster-affc.sandbox1480.opentlc.com:6443" as "system:serviceaccount:prod-pipeline:pipeline-starter" using the token provided.
[execute-remote-pipeline : execute-remote-pipeline-step] [execute-remote-pipeline : execute-remote-pipeline-step]
[execute-remote-pipeline : execute-remote-pipeline-step] [execute-remote-pipeline : execute-remote-pipeline-step] You have one project on this server: "prod-pipeline"
[execute-remote-pipeline : execute-remote-pipeline-step] [execute-remote-pipeline : execute-remote-pipeline-step]
[execute-remote-pipeline : execute-remote-pipeline-step] [execute-remote-pipeline : execute-remote-pipeline-step] Using project "prod-pipeline".
[execute-remote-pipeline : execute-remote-pipeline-step] [execute-remote-pipeline : execute-remote-pipeline-step] Welcome! See 'oc help' to get started.
[execute-remote-pipeline : execute-remote-pipeline-step] [execute-remote-pipeline : execute-remote-pipeline-step] [prod : prod-step] Running on prod cluster
[execute-remote-pipeline : execute-remote-pipeline-step] [execute-remote-pipeline : execute-remote-pipeline-step]
[execute-remote-pipeline : execute-remote-pipeline-step]
</code></pre>

<p>Note that this example is showing a cascading execution of Tekton pipelines. Another way of composing pipelines would be executing multiple remote pipelines in sequence.</p>

<p>Before moving on to the final section of this blog, let&rsquo;s briefly discuss the pipeline composition in terms of security. As a Kubernetes-native solution, Tekton&rsquo;s access control is managed by RBAC. Before the task running on a local cluster can trigger a pipeline on a remote cluster, it has to be granted appropriate permissions. These permissions are defined by the remote cluster. This way a remote cluster running in a higher environment (Prod) can impose access restrictions on the tasks running in the lower environment (Test). For example, a Prod cluster will allow the Test cluster to only trigger pre-defined production pipelines. The Test cluster won&rsquo;t have permissions to create new pipelines in the Prod cluster.</p>

<h2>Conclusion</h2>

<p>This blog showed how to create CI/CD pipelines that span multiple OpenShift clusters using Jenkins and Tekton. It designed the pipelines and discussed some of the security aspects. The execute-remote-pipeline Tekton task was used to compose pipelines located on different OpenShift clusters into a single pipeline.</p>

<p>Needless to say, containerized pipelines work the same way on any OpenShift cluster regardless of whether the cluster itself is running on top of a public cloud or on-premise. The vision of the hybrid cloud is well showcased here.</p>

<p>Do you create pipelines that span multiple clusters? Would you like to share some of your design ideas?  I would be happy to hear about your thoughts. Please, feel free to leave your comments in the comment section below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[OpenShift UPI Using Static IPs]]></title>
    <link href="http://alesnosek.com/blog/2020/04/21/openshift-upi-using-static-ips/"/>
    <updated>2020-04-21T20:29:52-07:00</updated>
    <id>http://alesnosek.com/blog/2020/04/21/openshift-upi-using-static-ips</id>
    <content type="html"><![CDATA[<p>Recently, I have been working on the <a href="https://github.com/noseka1/openshift-auto-upi">openshift-auto-upi</a> project, which automates UPI deployments of OpenShift.  I was looking for a way to configure OpenShift nodes with static IP addresses. After several failed attempts, I found a working approach that can be easily automated. If you prefer using static IPs over the default DHCP provisioning, please read on as I share my approach with you.</p>

<p>The blog is published at <a href="https://www.openshift.com/blog/openshift-upi-using-static-ips">openshift.com/blog</a>.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Configuring Envoy to Auto-Discover Pods on Kubernetes]]></title>
    <link href="http://alesnosek.com/blog/2019/08/19/configuring-envoy-to-audo-discover-pods-on-kubernetes/"/>
    <updated>2019-08-19T11:04:51-07:00</updated>
    <id>http://alesnosek.com/blog/2019/08/19/configuring-envoy-to-audo-discover-pods-on-kubernetes</id>
    <content type="html"><![CDATA[<p>Pods on Kubernetes are ephemeral and can be created and destroyed at any time. In order for Envoy to load balance the traffic across pods, Envoy needs to be able to track the IP addresses of the pods over time. In this blog post, I am going to show you how to leverage Envoy&rsquo;s Strict DNS discovery in combination with a headless service in Kubernetes to accomplish this.</p>

<!-- more -->


<h2>Overview</h2>

<p>Envoy provides several <a href="https://www.envoyproxy.io/docs/envoy/v1.10.0/intro/arch_overview/service_discovery">options</a> on how to discover back-end servers. When using the <a href="https://www.envoyproxy.io/docs/envoy/v1.10.0/intro/arch_overview/service_discovery#strict-dns">Strict DNS</a> option,  Envoy will periodically query a specified DNS name. If there are multiple IP addresses included in the response to Envoy&rsquo;s query, each returned IP address will be considered a back-end server. Envoy will load balance the inbound traffic across all of them.</p>

<p>How to configure a DNS server to return multiple IP addresses to Envoy? Kubernetes comes with a <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Service</a> object which, roughly speaking, provides two functions. It can create a single DNS name for a group of pods for discovery and it can load balance the traffic across those pods. We are not interested in the load balancing feature as we aim to use Envoy for that. However, we can make a good use of the discovery mechanism. The Service configuration we are looking for is called a <a href="https://kubernetes.io/docs/concepts/services-networking/service/#headless-services">headless service</a> with selectors.</p>

<p>The diagram below depicts how to configure Envoy to auto-discover pods on Kubernetes. We are combining Envoy&rsquo;s Strict DNS service discovery with a headless service in Kubernetes:</p>

<p><img class="center" src="/images/posts/envoy_auto_discovery.png"></p>

<h2>Practical implementation</h2>

<p>To put this configuration into practice, I used <a href="https://www.okd.io/minishift/">Minishift</a> 3.11 which is a variant of Minikube developed by Red Hat. First, I deployed two replicas of the httpd server on Kubernetes to play the role of back-end services. Next, I created a headless service using the following definition:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">apiVersion</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">v1</span>
</span><span class='line'><span class="l-Scalar-Plain">kind</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Service</span>
</span><span class='line'><span class="l-Scalar-Plain">metadata</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">httpd-discovery</span>
</span><span class='line'><span class="l-Scalar-Plain">spec</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">clusterIP</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">None</span>
</span><span class='line'>  <span class="l-Scalar-Plain">ports</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">http</span>
</span><span class='line'>      <span class="l-Scalar-Plain">port</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">8080</span>
</span><span class='line'>  <span class="l-Scalar-Plain">selector</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="l-Scalar-Plain">app</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">httpd</span>
</span><span class='line'>  <span class="l-Scalar-Plain">type</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">ClusterIP</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Note that we are explicitly specifying &ldquo;None&rdquo; for the cluster IP in the service definition. As a result, Kubernetes creates the respective Endpoints object containing the IP addresses of the discovered httpd pods:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>oc get endpoints
</span><span class='line'>NAME              ENDPOINTS                                                        AGE
</span><span class='line'>httpd-discovery   172.17.0.21:8080,172.17.0.22:8080                                30s
</span></code></pre></td></tr></table></div></figure></p>

<p> If you ssh to one of the cluster nodes or rsh to any of the pods running on the cluster, you can verify that the DNS discovery is working:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>host httpd-discovery
</span><span class='line'>httpd-discovery.mynamespace.svc.cluster.local has address 172.17.0.21
</span><span class='line'>httpd-discovery.mynamespace.svc.cluster.local has address 172.17.0.22
</span></code></pre></td></tr></table></div></figure></p>

<p>Next, I used the container image <code>docker.io/envoyproxy/envoy:v1.7.0</code> to create an Envoy proxy. I deployed the proxy into the same Kubernetes namespace called <code>mynamespace</code> where I created the headless service before. A minimum Envoy configuration that can accomplish our goal looks as follows:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">static_resources</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">listeners</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">listener_0</span>
</span><span class='line'>    <span class="l-Scalar-Plain">address</span><span class="p-Indicator">:</span>
</span><span class='line'>      <span class="l-Scalar-Plain">socket_address</span><span class="p-Indicator">:</span>
</span><span class='line'>        <span class="l-Scalar-Plain">protocol</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">TCP</span>
</span><span class='line'>        <span class="l-Scalar-Plain">address</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">0.0.0.0</span>
</span><span class='line'>        <span class="l-Scalar-Plain">port_value</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">10000</span>
</span><span class='line'>    <span class="l-Scalar-Plain">filter_chains</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">filters</span><span class="p-Indicator">:</span>
</span><span class='line'>      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">envoy.http_connection_manager</span>
</span><span class='line'>        <span class="l-Scalar-Plain">config</span><span class="p-Indicator">:</span>
</span><span class='line'>          <span class="l-Scalar-Plain">stat_prefix</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">ingress_http</span>
</span><span class='line'>          <span class="l-Scalar-Plain">route_config</span><span class="p-Indicator">:</span>
</span><span class='line'>            <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">local_route</span>
</span><span class='line'>            <span class="l-Scalar-Plain">virtual_hosts</span><span class="p-Indicator">:</span>
</span><span class='line'>            <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">local_service</span>
</span><span class='line'>              <span class="l-Scalar-Plain">domains</span><span class="p-Indicator">:</span> <span class="p-Indicator">[</span><span class="nl">&amp;ldquo</span><span class="nv">;*&amp;rdquo;</span><span class="p-Indicator">]</span>
</span><span class='line'>              <span class="l-Scalar-Plain">routes</span><span class="p-Indicator">:</span>
</span><span class='line'>              <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">match</span><span class="p-Indicator">:</span>
</span><span class='line'>                  <span class="l-Scalar-Plain">prefix</span><span class="p-Indicator">:</span> <span class="nl">&amp;ldquo</span><span class="l-Scalar-Plain">;/&amp;rdquo;</span>
</span><span class='line'>                <span class="l-Scalar-Plain">route</span><span class="p-Indicator">:</span>
</span><span class='line'>                  <span class="l-Scalar-Plain">host_rewrite</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">httpd</span>
</span><span class='line'>                  <span class="l-Scalar-Plain">cluster</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">httpd</span>
</span><span class='line'>          <span class="l-Scalar-Plain">http_filters</span><span class="p-Indicator">:</span>
</span><span class='line'>          <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">envoy.router</span>
</span><span class='line'>  <span class="l-Scalar-Plain">clusters</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">httpd</span>
</span><span class='line'>    <span class="l-Scalar-Plain">connect_timeout</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">0.25s</span>
</span><span class='line'>    <span class="l-Scalar-Plain">type</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">STRICT_DNS</span>
</span><span class='line'>    <span class="l-Scalar-Plain">dns_lookup_family</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">V4_ONLY</span>
</span><span class='line'>    <span class="l-Scalar-Plain">lb_policy</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">ROUND_ROBIN</span>
</span><span class='line'>    <span class="l-Scalar-Plain">hosts</span><span class="p-Indicator">:</span>
</span><span class='line'>      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">socket_address</span><span class="p-Indicator">:</span>
</span><span class='line'>          <span class="l-Scalar-Plain">address</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">httpd-discovery</span>
</span><span class='line'>          <span class="l-Scalar-Plain">port_value</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">8080</span>
</span><span class='line'><span class="l-Scalar-Plain">admin</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">access_log_path</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">/tmp/admin_access.log</span>
</span><span class='line'>  <span class="l-Scalar-Plain">address</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="l-Scalar-Plain">socket_address</span><span class="p-Indicator">:</span>
</span><span class='line'>      <span class="l-Scalar-Plain">protocol</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">TCP</span>
</span><span class='line'>      <span class="l-Scalar-Plain">address</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">127.0.0.1</span>
</span><span class='line'>      <span class="l-Scalar-Plain">port_value</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">9901</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Note that in the above configuration,  I instructed Envoy to use the Strict DNS discovery and pointed it to the DNS name <code>httpd-discovery</code> that is managed by Kubernetes.</p>

<p>That&rsquo;s all that was needed to be done! Envoy is load balancing the inbound traffic across the two httpd pods now. And if you create a third pod replica, Envoy is going to route the traffic to this replica as well.</p>

<h2>Conclusion</h2>

<p>In this article, I shared with you the idea of using Envoy&rsquo;s Strict DNS service discovery in combination with the headless service in Kubernetes to allow Envoy to auto-discover the back-end pods. While writing this article, I discovered this <a href="https://blog.markvincze.com/how-to-use-envoy-as-a-load-balancer-in-kubernetes/">blog post</a> by Mark Vincze that describes the same idea and you should take a look at it as well.</p>

<p>This idea opens the door for you to utilize the advanced features of Envoy proxy in your microservices architecture. However, if you find yourself looking for a more complex solution down the road, I would suggest that you evaluate the <a href="https://istio.io/">Istio</a> project. Istio provides a control plane that can manage Envoy proxies for you achieving the so called service mesh.</p>

<p>Hope you found this article useful. If you are using Envoy proxy on top of Kubernetes I would be happy to hear about your experiences. You can leave your comments in the comment section below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Installing OpenShift 4.1 Using Libvirt and KVM]]></title>
    <link href="http://alesnosek.com/blog/2019/07/08/installing-openshift-4-dot-1-using-libvirt-and-kvm/"/>
    <updated>2019-07-08T11:53:54-07:00</updated>
    <id>http://alesnosek.com/blog/2019/07/08/installing-openshift-4-dot-1-using-libvirt-and-kvm</id>
    <content type="html"><![CDATA[<p>In this blog post, I am going to talk about how I installed OpenShift 4.1 on a Fedora laptop with 16 GB of RAM. If you are interested in deploying your own OpenShift instance whether for evaluation or testing please follow along with me.</p>

<!-- more -->


<p>OpenShift 4.1 is the first GA release in the OpenShift 4 series. It is a significant leap forward in the evolution of OpenShift mainly due to the incorporation of features developed by the folks at CoreOS. In order to take a closer look at the latest and greatest version of OpenShift, I installed OpenShift 4.1 on my laptop using Libvirt and KVM. How did I accomplish this?</p>

<p>I essentially followed the <a href="https://docs.openshift.com/container-platform/4.1/installing/installing_bare_metal/installing-bare-metal.html">guide</a> for installing the OpenShift cluster on bare metal and I recommend that you read this guide first. After you make yourself familiar with the bare metal installation process, read on to learn the details on how I made this process work on Libvirt and KVM.</p>

<h2>Deployment overview</h2>

<p>First, let&rsquo;s take a look at the diagram showing the deployment of the OpenShift cluster on Libvirt/KVM. In addition to the OpenShift cluster nodes, the diagram also depicts supplementary pieces of the user-provisioned infrastructure that you will need to deploy:</p>

<p><img class="center" src="/images/posts/openshift_4_on_libvirt.png"></p>

<p>In the diagram, you can see that there is an HTTP server and an oc client installed directly on the host machine. The remaining boxes in the diagram are virtual machines. I outlined the purpose of the virtual machines for you in the following table:</p>

<table>
<thead>
<tr>
<th> VM Name </th>
<th> Operating System </th>
<th> Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td> <em>dns</em> </td>
<td> RHEL7 </td>
<td> Custom Dnsmasq DNS server used by the load balancer, bootstrap node and OpenShift nodes. </td>
</tr>
<tr>
<td> <em>loadbalancer</em> </td>
<td> RHEL 7 </td>
<td> HAProxy load balancer. Facilitates bootstrapping, balances the load between the master nodes and also between the ingress router pods. </td>
</tr>
<tr>
<td> <em>bootstrap</em> </td>
<td> RHCOS </td>
<td> The bootstrap machine. Used one-time to initialize the OpenShift cluster. </td>
</tr>
<tr>
<td> <em>master</em> </td>
<td> RHCOS </td>
<td> OpenShift master node. </td>
</tr>
<tr>
<td> <em>worker-1</em> </td>
<td> RHCOS </td>
<td> OpenShift worker node. </td>
</tr>
</tbody>
</table>


<p>Note that the virtual machines are deployed across two Libvirt networks: <code>openshift-dns</code> and <code>openshift-cluster</code>. Using two Libvirt networks allowed me to meet the OpenShift DNS requirements and I will elaborate on this design later on in this post.</p>

<p>After reviewing the big picture, let&rsquo;s roll up our sleeves and get to work. We are going to deal with the HTTP server first.</p>

<h2>Setting up HTTP server</h2>

<p>The OpenShift installation process assumes installation on empty virtual machines with no operating system pre-installed. There are two provisioning methods available to choose from. You can either provision OpenShift nodes by booting from an ISO image or you can leverage the PXE boot. I find the PXE boot option to take a bit more effort to configure and hence went with the ISO image method.</p>

<p>Using the ISO image method, you are supposed to boot the virtual machines using the <code>rhcos-4.1.0-x86_64-installer.iso</code> CD-ROM image. During the boot from this image, the Red Hat CoreOS installer starts up and provisions an empty virtual machine in two steps:</p>

<ol>
<li>It downloads a disk image <code>rhcos-4.1.0-x86_64-metal-bios.raw.gz</code> from a URL you specify and writes it to the virtual machine&rsquo;s disk.</li>
<li>It downloads one of the ignition files (e.g. <code>bootstrap.ign</code>, <code>master.ign</code>, or <code>worker.ign</code>) and installs it on the virtual machine&rsquo;s file system.  This ignition file contains configuration required for the bootstrap of the OpenShift cluster that is triggered on the next reboot.</li>
</ol>


<p>You are expected to host the aforementioned files on an HTTP server that is reachable from the OpenShift nodes during the provisioning process. To meet this requirement, I installed an Apache HTTP server on my Fedora host machine and copied the disk image and ignition files to the <code>/var/www/html</code> directory which is the default <code>DocumentRoot</code> directory on a Fedora host.</p>

<h2>Addressing OpenShift DNS requirements</h2>

<p>OpenShift <a href="https://docs.openshift.com/container-platform/4.1/installing/installing_bare_metal/installing-bare-metal.html#installation-dns-user-infra_installing-bare-metal">requires</a> a set of records to be configured in your DNS. In addition to simple A records, you must also configure a wildcard DNS record that points to the load balancer and an SRV DNS record for each of the etcd nodes.</p>

<p>Libvirt allows you to insert custom A and SRV records into DNS. You can specify them using the <a href="https://libvirt.org/formatnetwork.html">network descriptor</a>. However, Libvirt doesn&rsquo;t support creating wildcard DNS records. The respective feature request can be found <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1532856">here</a>. It would be great if it would be possible to meet the OpenShift DNS requirements by just configuring the DNS records in the Libvirt&rsquo;s network descriptor. However, as the wildcard DNS records were not supported at the time of this writing, I had to look for an alternative solution. After giving it some thought, I decided to spin up my own DNS server and instructed Libvirt to forward the DNS queries sent by the OpenShift nodes to this server. In order to achieve this, I had to define two networks in Libvirt: <code>openshift-dns</code> and <code>openshift-cluster</code>.</p>

<p>Let&rsquo;s tackle the <code>openshift-dns</code> network first. A single virtual machine is connected to this network. This virtual machine hosts the custom DNS server. Here is the respective network XML descriptor:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;network&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>openshift-dns<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;forward</span> <span class="na">mode=</span><span class="s">&#39;nat&#39;</span><span class="nt">&gt;</span>
</span><span class='line'>    <span class="nt">&lt;nat&gt;</span>
</span><span class='line'>      <span class="nt">&lt;port</span> <span class="na">start=</span><span class="s">&#39;1024&#39;</span> <span class="na">end=</span><span class="s">&#39;65535&#39;</span><span class="nt">/&gt;</span>
</span><span class='line'>    <span class="nt">&lt;/nat&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/forward&gt;</span>
</span><span class='line'>  <span class="nt">&lt;bridge</span> <span class="na">name=</span><span class="s">&#39;virbr-oshd&#39;</span> <span class="na">stp=</span><span class="s">&#39;on&#39;</span> <span class="na">delay=</span><span class="s">&#39;0&#39;</span><span class="nt">/&gt;</span>
</span><span class='line'>  <span class="nt">&lt;mac</span> <span class="na">address=</span><span class="s">&#39;52:54:00:2c:00:00&#39;</span><span class="nt">/&gt;</span>
</span><span class='line'>  <span class="nt">&lt;domain</span> <span class="na">name=</span><span class="s">&#39;mycluster.example.com&#39;</span> <span class="na">localOnly=</span><span class="s">&#39;no&#39;</span><span class="nt">/&gt;</span>
</span><span class='line'>  <span class="nt">&lt;ip</span> <span class="na">address=</span><span class="s">&#39;192.168.130.1&#39;</span> <span class="na">netmask=</span><span class="s">&#39;255.255.255.0&#39;</span><span class="nt">&gt;</span>
</span><span class='line'>    <span class="nt">&lt;dhcp&gt;</span>
</span><span class='line'>      <span class="nt">&lt;range</span> <span class="na">start=</span><span class="s">&#39;192.168.130.10&#39;</span> <span class="na">end=</span><span class="s">&#39;192.168.130.254&#39;</span><span class="nt">/&gt;</span>
</span><span class='line'>      <span class="nt">&lt;host</span> <span class="na">mac=</span><span class="s">&#39;52:54:00:2c:00:10&#39;</span> <span class="na">name=</span><span class="s">&#39;dns.mycluster.example.com&#39;</span> <span class="na">ip=</span><span class="s">&#39;192.168.130.10&#39;</span><span class="nt">/&gt;</span>
</span><span class='line'>    <span class="nt">&lt;/dhcp&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/ip&gt;</span>
</span><span class='line'><span class="nt">&lt;/network&gt;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>As you can see in the descriptor, I prefer to manage virtual machine&rsquo;s MAC addresses and the associated IP addresses and host names by hand. The virtual machine <code>dns</code> that hosts my DNS server is connected to the <code>openshift-dns</code> network by including these settings in its domain configuration:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;domain</span> <span class="na">type=</span><span class="s">&#39;kvm&#39;</span><span class="nt">&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>dns.mycluster.example.com<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="ni">&amp;hellip;</span>
</span><span class='line'>    <span class="nt">&lt;interface</span> <span class="na">type=</span><span class="s">&#39;network&#39;</span><span class="nt">&gt;</span>
</span><span class='line'>      <span class="nt">&lt;mac</span> <span class="na">address=</span><span class="s">&#39;52:54:00:2c:00:10&#39;</span><span class="nt">/&gt;</span>
</span><span class='line'>      <span class="nt">&lt;source</span> <span class="na">network=</span><span class="s">&#39;openshift-dns&#39;</span><span class="nt">/&gt;</span>
</span><span class='line'>      <span class="nt">&lt;model</span> <span class="na">type=</span><span class="s">&#39;virtio&#39;</span><span class="nt">/&gt;</span>
</span><span class='line'>      <span class="nt">&lt;address</span> <span class="na">type=</span><span class="s">&#39;pci&#39;</span> <span class="na">domain=</span><span class="s">&#39;0x0000&#39;</span> <span class="na">bus=</span><span class="s">&#39;0x00&#39;</span> <span class="na">slot=</span><span class="s">&#39;0x03&#39;</span> <span class="na">function=</span><span class="s">&#39;0x0&#39;</span><span class="nt">/&gt;</span>
</span><span class='line'>    <span class="nt">&lt;/interface&gt;</span>
</span><span class='line'>    <span class="ni">&amp;hellip;</span>
</span><span class='line'><span class="nt">&lt;/domain&gt;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>I installed Dnsmasq on this <code>dns</code> virtual machine and replaced the content of <code>/etc/dnsmasq.conf</code> with my own configuration:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='ini'><span class='line'><span class="na">local</span><span class="o">=</span><span class="s">/mycluster.example.com/</span>
</span><span class='line'><span class="na">address</span><span class="o">=</span><span class="s">/apps.mycluster.example.com/192.168.131.10</span>
</span><span class='line'><span class="na">srv-host</span><span class="o">=</span><span class="s">&lt;em&gt;etcd-server-ssl.&lt;/em&gt;tcp.mycluster.example.com,master.mycluster.example.com,2380,0,10</span>
</span><span class='line'><span class="err">no-hosts</span>
</span><span class='line'><span class="na">addn-hosts</span><span class="o">=</span><span class="s">/etc/dnsmasq.openshift.addnhosts</span>
</span><span class='line'><span class="na">conf-dir</span><span class="o">=</span><span class="s">/etc/dnsmasq.d,.rpmnew,.rpmsave,.rpmorig</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>The listing of the <code>/etc/dnsmasq.openshift.addnhosts</code> file referred to in the above configuration looks as follows:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>192.168.130.10 dns.mycluster.example.com
</span><span class='line'>192.168.131.10 loadbalancer.mycluster.example.com  api.mycluster.example.com  api-int.mycluster.example.com
</span><span class='line'>192.168.131.11 bootstrap.mycluster.example.com
</span><span class='line'>192.168.131.12 master.mycluster.example.com  etcd-0.mycluster.example.com
</span><span class='line'>192.168.131.13 worker-1.mycluster.example.com
</span></code></pre></td></tr></table></div></figure></p>

<p>This configuration addresses the user-provisioned DNS requirements as specified in the <a href="https://docs.openshift.com/container-platform/4.1/installing/installing_bare_metal/installing-bare-metal.html">installation guide</a>.</p>

<p>In the next step, we want to make the load balancer machine and OpenShift nodes resolve their DNS queries using our custom DNS server. In order to achieve that, we define a second Libvirt network called <code>openshift-cluster</code> and place the load balancer and OpenShift nodes onto this network. The definition of the <code>openshift-cluster</code> network looks like this:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class='xml'><span class='line'><span class="nt">&lt;network&gt;</span>
</span><span class='line'>  <span class="nt">&lt;name&gt;</span>openshift-cluster<span class="nt">&lt;/name&gt;</span>
</span><span class='line'>  <span class="nt">&lt;forward</span> <span class="na">mode=</span><span class="s">&#39;nat&#39;</span><span class="nt">&gt;</span>
</span><span class='line'>    <span class="nt">&lt;nat&gt;</span>
</span><span class='line'>      <span class="nt">&lt;port</span> <span class="na">start=</span><span class="s">&#39;1024&#39;</span> <span class="na">end=</span><span class="s">&#39;65535&#39;</span><span class="nt">/&gt;</span>
</span><span class='line'>    <span class="nt">&lt;/nat&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/forward&gt;</span>
</span><span class='line'>  <span class="nt">&lt;bridge</span> <span class="na">name=</span><span class="s">&#39;virbr-osh&#39;</span> <span class="na">stp=</span><span class="s">&#39;on&#39;</span> <span class="na">delay=</span><span class="s">&#39;0&#39;</span><span class="nt">/&gt;</span>
</span><span class='line'>  <span class="nt">&lt;mac</span> <span class="na">address=</span><span class="s">&#39;52:54:00:2c:01:00&#39;</span><span class="nt">/&gt;</span>
</span><span class='line'>  <span class="nt">&lt;domain</span> <span class="na">name=</span><span class="s">&#39;mycluster.example.com&#39;</span> <span class="na">localOnly=</span><span class="s">&#39;no&#39;</span><span class="nt">/&gt;</span>
</span><span class='line'>  <span class="nt">&lt;dns&gt;</span>
</span><span class='line'>    <span class="nt">&lt;forwarder</span> <span class="na">addr=</span><span class="s">&#39;192.168.130.10&#39;</span><span class="nt">/&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/dns&gt;</span>
</span><span class='line'>  <span class="nt">&lt;ip</span> <span class="na">address=</span><span class="s">&#39;192.168.131.1&#39;</span> <span class="na">netmask=</span><span class="s">&#39;255.255.255.0&#39;</span><span class="nt">&gt;</span>
</span><span class='line'>    <span class="nt">&lt;dhcp&gt;</span>
</span><span class='line'>      <span class="nt">&lt;range</span> <span class="na">start=</span><span class="s">&#39;192.168.131.10&#39;</span> <span class="na">end=</span><span class="s">&#39;192.168.131.254&#39;</span><span class="nt">/&gt;</span>
</span><span class='line'>      <span class="nt">&lt;host</span> <span class="na">mac=</span><span class="s">&#39;52:54:00:2c:01:10&#39;</span> <span class="na">name=</span><span class="s">&#39;loadbalancer.mycluster.example.com&#39;</span> <span class="na">ip=</span><span class="s">&#39;192.168.131.10&#39;</span><span class="nt">/&gt;</span>
</span><span class='line'>      <span class="nt">&lt;host</span> <span class="na">mac=</span><span class="s">&#39;52:54:00:2c:01:11&#39;</span> <span class="na">name=</span><span class="s">&#39;bootstrap.mycluster.example.com&#39;</span> <span class="na">ip=</span><span class="s">&#39;192.168.131.11&#39;</span><span class="nt">/&gt;</span>
</span><span class='line'>      <span class="nt">&lt;host</span> <span class="na">mac=</span><span class="s">&#39;52:54:00:2c:01:12&#39;</span> <span class="na">name=</span><span class="s">&#39;master.mycluster.example.com&#39;</span> <span class="na">ip=</span><span class="s">&#39;192.168.131.12&#39;</span><span class="nt">/&gt;</span>
</span><span class='line'>      <span class="nt">&lt;host</span> <span class="na">mac=</span><span class="s">&#39;52:54:00:2c:01:13&#39;</span> <span class="na">name=</span><span class="s">&#39;worker-1.mycluster.example.com&#39;</span> <span class="na">ip=</span><span class="s">&#39;192.168.131.13&#39;</span><span class="nt">/&gt;</span>
</span><span class='line'>    <span class="nt">&lt;/dhcp&gt;</span>
</span><span class='line'>  <span class="nt">&lt;/ip&gt;</span>
</span><span class='line'><span class="nt">&lt;/network&gt;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Note the <code>&lt;forwarder addr='192.168.130.10'/&gt;</code> setting which allows all DNS requests from the load balancer and OpenShift nodes deployed on this network to be forwarded to our custom DNS server. Remember that the IP address <code>192.168.130.10</code> is the address of our custom DNS server that we configured previously.</p>

<p>With the DNS configuration out of the way, let&rsquo;s continue with deploying a load balancer in the next section.</p>

<h2>Setting up a load balancer</h2>

<p>Installing OpenShift on a user-provisioned infrastructure requires you to provision a load balancer. The details on how the load balancer should be configured can be found in the <a href="https://docs.openshift.com/container-platform/4.1/installing/installing_bare_metal/installing-bare-metal.html#installation-network-user-infra_installing-bare-metal">networking requirements</a> section of the OpenShift installation guide.</p>

<p>The load balancer is used during the bootstrapping process to route the requests to the bootstrap and the master nodes. After the OpenShift installation is complete, the load balancer remains part of the deployment and balances load between the master nodes and also between the ingress router pods.</p>

<p>I created a dedicated virtual machine called <code>loadbalancer</code> and installed HAProxy on top of it. The HAProxy configuration is pretty straight forward. Here is the listing of the <code>/etc/haproxy/haproxy.cfg</code> file:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>global
</span><span class='line'>    log         127.0.0.1 local2 info
</span><span class='line'>    chroot      /var/lib/haproxy
</span><span class='line'>    pidfile     /var/run/haproxy.pid
</span><span class='line'>    maxconn     4000
</span><span class='line'>    user        haproxy
</span><span class='line'>    group       haproxy
</span><span class='line'>    daemon&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>defaults
</span><span class='line'>    timeout connect         5s
</span><span class='line'>    timeout client          30s
</span><span class='line'>    timeout server          30s
</span><span class='line'>    log                     global&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>frontend kubernetes_api
</span><span class='line'>    bind 0.0.0.0:6443
</span><span class='line'>    default_backend kubernetes_api&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>backend kubernetes_api
</span><span class='line'>    balance roundrobin
</span><span class='line'>    option ssl-hello-chk
</span><span class='line'>    server bootstrap bootstrap.mycluster.example.com:6443 check
</span><span class='line'>    server master master.mycluster.example.com:6443 check&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>frontend machine_config
</span><span class='line'>    bind 0.0.0.0:22623
</span><span class='line'>    default_backend machine_config&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>backend machine_config
</span><span class='line'>    balance roundrobin
</span><span class='line'>    option ssl-hello-chk
</span><span class='line'>    server bootstrap bootstrap.mycluster.example.com:22623 check
</span><span class='line'>    server master master.mycluster.example.com:22623 check&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>frontend router_https
</span><span class='line'>    bind 0.0.0.0:443
</span><span class='line'>    default_backend router_https&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>backend router_https
</span><span class='line'>    balance roundrobin
</span><span class='line'>    option ssl-hello-chk
</span><span class='line'>    server worker-1 worker-1.mycluster.example.com:443 check&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>frontend router_http
</span><span class='line'>    mode http
</span><span class='line'>    option httplog
</span><span class='line'>    bind 0.0.0.0:80
</span><span class='line'>    default_backend router_http&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>backend router_http
</span><span class='line'>    mode http
</span><span class='line'>    balance roundrobin
</span><span class='line'>    server worker-1 worker-1.mycluster.example.com:80 check</span></code></pre></td></tr></table></div></figure></p>

<p>With the load balancer in place, we will move on to creating OpenShift virtual machines in the next section.</p>

<h2>Creating OpenShift virtual machines</h2>

<p>The official installation guide <a href="https://docs.openshift.com/container-platform/4.1/installing/installing_bare_metal/installing-bare-metal.html#machine-requirements_installing-bare-metal">defines</a> minimum machine requirements for installing an OpenShift cluster as follows:</p>

<ul>
<li>One bootstrap machine</li>
<li>Three control plane, or master, machines</li>
<li>At least two compute, or worker, machines</li>
</ul>


<p>If you can meet these requirements, you will achieve the smallest <em>highly available</em> OpenShift cluster. However, do we really need high availability for our test installation?</p>

<p>Internally, OpenShift uses <a href="https://etcd.io/">etcd</a> to store its state. Since etcd is a quorum-based cluster, it requires at least three nodes to achieve high availability. These etcd nodes are installed on OpenShift master machines which is the reason for the minimum requirement of three OpenShift master machines. In our limited environment, we are going to give up on high availability and instead save up two master machines. OpenShift can install with a single master machine just fine if you can accept the fact that the OpenShift control plane won&rsquo;t be highly available.</p>

<p>And what about the requirement of two worker machines? The minimum requirement of two worker machines ensures that there will be at least two OpenShift routers running on the cluster. OpenShift router is an ingress point for external traffic to reach application pods running on OpenShift. Production installations require that at least two routers are installed to avoid a single point of failure. Furthermore, a highly available load balancer is deployed in front of the two routers. In a data center, a hardware load balancer is typically used, in cloud environments like AWS an Elastic Load Balancer can be utilized. As we don&rsquo;t pursue a highly available deployment, we are going to install an OpenShift cluster with a single worker machine. There will be a single router running on top of this cluster which we hereby accept.</p>

<p>This discussion leads us to the minimum requirements for a <em>not highly available</em> OpenShift cluster:</p>

<ul>
<li>One bootstrap machine</li>
<li>One control plane, or master, machine</li>
<li>One compute, or worker, machine</li>
</ul>


<p>In regards to the minimum memory requirements for each of the machines, I was able to install OpenShift on virtual machines with the following memory configuration:</p>

<table>
<thead>
<tr>
<th> Machine        </th>
<th> RAM  </th>
</tr>
</thead>
<tbody>
<tr>
<td> Bootstrap      </td>
<td> 4 GB </td>
</tr>
<tr>
<td> Control plane  </td>
<td> 6 GB </td>
</tr>
<tr>
<td> Compute        </td>
<td> 6 GB </td>
</tr>
</tbody>
</table>


<p>Note that the above memory requirements allow you to properly deploy the OpenShift cluster including the monitoring and log collection components. Furthermore, there will be enough capacity left on the worker node for you to run several hello world applications.</p>

<p>This concludes the user-provisioned infrastructure setup. At this point, we have HTTP server, DNS server, load balancer and a set of empty virtual machines in place. Let&rsquo;s dive into the OpenShift installation in the next section.</p>

<h2>Installing OpenShift 4.1</h2>

<p>The installation of OpenShift 4 starts with crafting an installation configuration file. You can use the <code>install-config.yaml</code> configuration file that I created, just remember to replace the placeholders with your own pull secret and public SSH key:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">apiVersion</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">v1</span>
</span><span class='line'><span class="l-Scalar-Plain">baseDomain</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">example.com</span>
</span><span class='line'><span class="l-Scalar-Plain">compute</span><span class="p-Indicator">:</span>
</span><span class='line'><span class="p-Indicator">-</span> <span class="l-Scalar-Plain">hyperthreading</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Enabled</span>
</span><span class='line'>  <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">worker</span>
</span><span class='line'>  <span class="l-Scalar-Plain">platform</span><span class="p-Indicator">:</span> <span class="p-Indicator">{}</span>
</span><span class='line'>  <span class="l-Scalar-Plain">replicas</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">0</span>
</span><span class='line'><span class="l-Scalar-Plain">controlPlane</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">hyperthreading</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">Enabled</span>
</span><span class='line'>  <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">master</span>
</span><span class='line'>  <span class="l-Scalar-Plain">platform</span><span class="p-Indicator">:</span> <span class="p-Indicator">{}</span>
</span><span class='line'>  <span class="l-Scalar-Plain">replicas</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">1</span>
</span><span class='line'><span class="l-Scalar-Plain">metadata</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">creationTimestamp</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">null</span>
</span><span class='line'>  <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">mycluster</span>
</span><span class='line'><span class="l-Scalar-Plain">networking</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">clusterNetwork</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">cidr</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">10.128.0.0/14</span>
</span><span class='line'>    <span class="l-Scalar-Plain">hostPrefix</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">23</span>
</span><span class='line'>  <span class="l-Scalar-Plain">networkType</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">OpenShiftSDN</span>
</span><span class='line'>  <span class="l-Scalar-Plain">serviceNetwork</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">172.30.0.0/16</span>
</span><span class='line'><span class="l-Scalar-Plain">platform</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">none</span><span class="p-Indicator">:</span> <span class="p-Indicator">{}</span>
</span><span class='line'><span class="l-Scalar-Plain">pullSecret</span><span class="p-Indicator">:</span> <span class="nl">&amp;lsquo</span><span class="l-Scalar-Plain">;&lt;INSERT_YOUR_PULL_SECRET_HERE&gt;&amp;rsquo;</span>
</span><span class='line'><span class="l-Scalar-Plain">sshKey</span><span class="p-Indicator">:</span> <span class="nl">&amp;lsquo</span><span class="l-Scalar-Plain">;&lt;INSERT_YOUR_PUBLIC_SSH_KEY_HERE&gt;&amp;rsquo;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>The OpenShift installation is actually driven by the ignition configuration files. You can issue this command to generate ignition configuration files out of your <code>install-config.yaml</code> file:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>./openshift-install create ignition-configs
</span></code></pre></td></tr></table></div></figure></p>

<p>Beware that the above command will remove your handcrafted <code>install-config.yaml</code> from the disk. I found this behavior of the    <code>openshift-install</code> tool rather annoying. In order to not lose my configuration settings, I protect the <code>install-config.yaml</code> file from deletion by creating a hard link like this:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>ln install-config.yaml install-config.yaml.hardlink
</span></code></pre></td></tr></table></div></figure></p>

<p>And after the <code>install-config.yaml</code> file is deleted, I can simply recreate it with:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>ln install-config.yaml.hardlink install-config.yaml
</span></code></pre></td></tr></table></div></figure></p>

<p>Finally, we can use our ignition files to kick off the OpenShift installation process which deploys OpenShift cluster on our fleet of virtual machines. The whole process takes about 30 minutes and consists of several steps:</p>

<ol>
<li>Provision and reboot the bootstrap machine</li>
<li>Provision and reboot the master machine</li>
<li>Bootstrap the master machine</li>
<li>Shut down the bootstrap machine</li>
<li>Provision and reboot the worker machine</li>
<li>Worker machine joins the OpenShift cluster</li>
</ol>


<p>Note that after you bootstrap the master machine, you should shut down the bootstrap machine. Only after that, you should boot up the worker machine. On startup, the worker node registers with the master node and forms an OpenShift cluster.</p>

<h2>Conclusion</h2>

<p>In this blog post, we discussed how to deploy OpenShift 4.1 into the Libvirt/KVM-based virtualized environment. We created and configured a bunch of user-provisioned infrastructure which was a prerequisite for the OpenShift installation. With the user-provisioned infrastructure in place, we followed the OpenShift bare metal deployment guide to create an OpenShift cluster.</p>

<p>I hope that you found this article useful and you have your OpenShift 4.1 cluster running by now. If you have any questions or comments please feel free to add them to the comment section below.</p>
]]></content>
  </entry>

</feed>
