<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: devops | Ales Nosek - The Software Practitioner]]></title>
  <link href="http://alesnosek.com/blog/categories/devops/atom.xml" rel="self"/>
  <link href="http://alesnosek.com/"/>
  <updated>2016-07-05T00:19:24-07:00</updated>
  <id>http://alesnosek.com/</id>
  <author>
    <name><![CDATA[Ales Nosek]]></name>

  </author>
  <generator uri="http://octopress.org/">Octopress</generator>


  <entry>
    <title type="html"><![CDATA[Deploying Kubernetes on OpenStack using Heat]]></title>
    <link href="http://alesnosek.com/blog/2016/06/26/deploying-kubernetes-on-openstack-using-heat/"/>
    <updated>2016-06-26T08:28:11-07:00</updated>
    <id>http://alesnosek.com/blog/2016/06/26/deploying-kubernetes-on-openstack-using-heat</id>
    <content type="html"><![CDATA[<p>Want to install Kubernetes on top of OpenStack? There are <a href="http://kubernetes.io/docs/getting-started-guides/">many ways</a> how to install a Kubernetes cluster. The upcoming Kubernetes 1.3 release comes with yet another method called <a href="http://kubernetes.io/docs/getting-started-guides/openstack-heat/">OpenStack Heat</a>. In this article, we&rsquo;re going to explore this deployment method when creating a minimum Kubernetes cluster on top of OpenStack.</p>

<!-- more -->


<p>In this tutorial, there are three OpenStack virtual machines involved. The first machine, called the <em>Kubernetes installer</em> machine, is created manually and is used for compiling Kubernetes from source and running the Kubernetes installer. The other two OpenStack machines, <em>Kubernetes master</em> and <em>Kubernetes node</em>, are created during the installation process.</p>

<p>The Kubernetes installer machine and both of the Kubernetes machines run on the CentOS-7-x86_64-GenericCloud-1605 image. You can download this image from the <a href="http://cloud.centos.org/centos/7/images/">CentOS image repository</a>. After I uploaded the CentOS 7 image into OpenStack, it has been assigned ID <code>17e4e783-321c-48c1-9308-6f99d67c5fa6</code> for me.</p>

<h2>Building Kubernetes from source</h2>

<p>First off, let&rsquo;s spin up a Kubernetes installer machine in OpenStack. I recommend using the <code>m1.large</code> flavor that comes with 8 GB of RAM. The compilation of Kubernetes is rather memory intensive.</p>

<p>To ensure consistent and reproducible builds, a Docker container is created at the beginning of the build process and the build proceeds within the container. So, let&rsquo;s quickly setup Docker on our build machine:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo yum install docker
</span></code></pre></td></tr></table></div></figure></p>

<p>Configure the Docker service to start on boot and then start it:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo systemctl <span class="nb">enable </span>docker
</span><span class='line'>sudo systemctl start docker
</span></code></pre></td></tr></table></div></figure></p>

<p>The Kubernetes build scripts expect that the <code>docker</code> command can successfully contact the Docker daemon. In the default CentOS configuration, the <code>sudo docker</code> is required in order to connect to the <code>/var/run/docker.sock</code> socket which is owned by the user root. To overcome the permission problem, let&rsquo;s create a wrapper script that will invoke the <code>docker</code> command using <code>sudo</code>:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>mkdir bin
</span><span class='line'><span class="nb">echo</span> -e <span class="p">&amp;</span>lsquo<span class="p">;</span><span class="c">#!/bin/bash\nexec sudo /usr/bin/docker &amp;ldquo;$@&amp;rdquo;&amp;rsquo; &gt; bin/docker</span>
</span><span class='line'>chmod <span class="m">755</span> bin/docker
</span><span class='line'><span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span>~/bin:<span class="nv">$PATH</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>You can test your changes with the <code>docker info</code> command which should work now.</p>

<p>Kubernetes is written in the Go language and its source code is stored in a Git repository. So, let&rsquo;s install the Go language environment and Git:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo yum install golang git
</span></code></pre></td></tr></table></div></figure></p>

<p>Next we&rsquo;ll clone the Kubernetes Git repository and start the build. The <code>quick-release</code> make target creates a build for the amd64 architecture only and doesn&rsquo;t run any tests.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>git clone &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://github.com/kubernetes/kubernetes.git&quot;</span>&gt;https://github.com/kubernetes/kubernetes.git&lt;/a&gt;
</span><span class='line'><span class="nb">cd </span>kubernetes
</span><span class='line'>make quick-release
</span></code></pre></td></tr></table></div></figure></p>

<p>After about 15 minutes when the build was successful, you&rsquo;ll find the distribution tarballs <code>kubernetes.tar.gz</code> and <code>kubernetes-salt.tar.gz</code> in the <code>_output/release-tars</code> directory.</p>

<h2>Setting up the OpenStack CLI tools</h2>

<p>The Kubernetes installer uses the OpenStack CLI tools to talk to OpenStack in order to create a Kubernetes cluster. Before you can install the OpenStack CLI tools on CentOS 7, you have to enable the OpenStack Mitaka RPM repository:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo yum install centos-release-openstack-mitaka
</span></code></pre></td></tr></table></div></figure></p>

<p>Install the OpenStack CLI tools that are used by the Kubernetes installer when creating a cluster with:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>sudo yum install python-openstackclient python-swiftclient python-glanceclient python-novaclient python-heatclient
</span></code></pre></td></tr></table></div></figure></p>

<p>Next, you have to obtain your OpenStack <code>openrc.sh</code> file and source it into your environment:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>. openrc.sh
</span></code></pre></td></tr></table></div></figure></p>

<p>You should be able to talk to OpenStack now. For example, check if you can list the available OpenStack networks with:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>openstack network list
</span></code></pre></td></tr></table></div></figure></p>

<h2>Configuring the Kubernetes installer</h2>

<p>In this section, we&rsquo;re going to more or less follow the instructions found in the chapter <a href="http://kubernetes.io/docs/getting-started-guides/openstack-heat/">OpenStack Heat</a> of the Kubernetes documentation.</p>

<p>When deploying the Kubernetes cluster, the installer executes the following steps that you can find in <code>cluster/openstack-heat/util.sh</code>:</p>

<ul>
<li>Upload the distribution tarballs <code>kubernetes.tar.gz</code> and <code>kubernetes-salt.tar.gz</code> into the <code>kubernetes</code> container in Swift</li>
<li>Upload the virtual machine image for the Kubernetes VMs into Glance</li>
<li>Add the user&rsquo;s keypair into Nova</li>
<li>Run a Heat script in order to create the Kubernetes VMs and put them on a newly created private network. Create a router connecting the private network with an external network.</li>
<li>At the first boot, the Kubernetes VMs download the distribution tarballs from Swift and install the Kubernetes software using Salt</li>
</ul>


<p>Let&rsquo;s create an <code>openstack-heat.sh</code> file with the configuration values for the Kubernetes installer:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nb">export </span><span class="nv">KUBERNETES_PROVIDER</span><span class="o">=</span>openstack-heat
</span><span class='line'><span class="nb">export </span><span class="nv">STACK_NAME</span><span class="o">=</span>kubernetes
</span><span class='line'><span class="nb">export </span><span class="nv">KUBERNETES_KEYPAIR_NAME</span><span class="o">=</span>mykeypair
</span><span class='line'><span class="nb">export </span><span class="nv">NUMBER_OF_MINIONS</span><span class="o">=</span>1
</span><span class='line'><span class="nb">export </span><span class="nv">MAX_NUMBER_OF_MINIONS</span><span class="o">=</span>1
</span><span class='line'><span class="nb">export </span><span class="nv">EXTERNAL_NETWORK</span><span class="o">=</span>gateway
</span><span class='line'><span class="nb">export </span><span class="nv">CREATE_IMAGE</span><span class="o">=</span><span class="nb">false</span>
</span><span class='line'><span class="nb">export </span><span class="nv">DOWNLOAD_IMAGE</span><span class="o">=</span><span class="nb">false</span>
</span><span class='line'><span class="nb">export </span><span class="nv">IMAGE_ID</span><span class="o">=</span>17e4e783-321c-48c1-9308-6f99d67c5fa6
</span><span class='line'><span class="nb">export </span><span class="nv">DNS_SERVER</span><span class="o">=</span>10.0.0.10
</span><span class='line'><span class="nb">export </span><span class="nv">SWIFT_SERVER_URL</span><span class="o">=</span>&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://openstack.localdomain:13808/swift/v1&quot;</span>&gt;https://openstack.localdomain:13808/swift/v1&lt;/a&gt;
</span></code></pre></td></tr></table></div></figure></p>

<p>The above configuration will create exactly one Kubernetes master and one Kubernetes node. It will inject the keypair called <code>mykeypair</code> into both of them. Note that you have to ensure that the keypair <code>mykeypair</code> exists in Nova before proceeding. You probably want to change the name of the external network to a network available in your OpenStack. We&rsquo;re going to use the same CentOS 7 image for both of our Kubernetes VMs. This CentOS image has already been uploaded into OpenStack and in my case it was assigned ID <code>17e4e783-321c-48c1-9308-6f99d67c5fa6</code>. You also want to change the IP address of the DNS server to something that suits your environment. The Swift server URL is the public endpoint of your Swift server that you can obtain from the output of the command <code>openstack catalog show object-store</code>.</p>

<p>When your configuration is ready, you can source it into your environment:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>. openstack-heat.sh
</span></code></pre></td></tr></table></div></figure></p>

<p>Next, in my environment I had a problem where the IP range of the private network created by Kubernetes collided with the existing corporate network in my company. I had to directly edit the file <code>cluster/openstack-heat/kubernetes-heat/kubecluster.yaml</code> to change the <code>10.0.0.0/24</code> CIDR to something like <code>10.123.0.0/24</code>. If you don&rsquo;t have this problem you can safely use the default settings.</p>

<p>The Kubernetes cluster can leverage the underlying OpenStack cloud to attach existing Cinder volumes to the Kubernetes pods and to create external loadbalancers. For this to work, Kubernetes has to know how to connect to OpenStack APIs. With regard to the external loadbalancers, we also need to tell Kubernetes what Neutron subnet the loadbalancer&rsquo;s VIP should be placed on.</p>

<p>The OpenStack configuration can be found in the <em>cloud-config</em> script <code>cluster/openstack-heat/kubernetes-heat/fragments/configure-salt.yaml</code>. You can see that this script will create a configuration file <code>/srv/kubernetes/openstack.conf</code> on the Kubernetes machine which contains the OpenStack settings. In my case, I changed the original block:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[Global]
</span><span class='line'>auth-url=$OS_AUTH_URL
</span><span class='line'>username=$OS_USERNAME
</span><span class='line'>password=$OS_PASSWORD
</span><span class='line'>region=$OS_REGION_NAME
</span><span class='line'>tenant-id=$OS_TENANT_ID</span></code></pre></td></tr></table></div></figure></p>

<p>to read:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>[Global]
</span><span class='line'>auth-url=$OS_AUTH_URL
</span><span class='line'>username=$OS_USERNAME
</span><span class='line'>password=$OS_PASSWORD
</span><span class='line'>region=$OS_REGION_NAME
</span><span class='line'>tenant-id=$OS_TENANT_ID
</span><span class='line'>domain-name=MyDomain # Keystone V3 domain
</span><span class='line'>[LoadBalancer]
</span><span class='line'>lb-version=v1
</span><span class='line'>subnet-id=73f8eb91-90cf-42f4-85d0-dcff44077313</span></code></pre></td></tr></table></div></figure></p>

<p>Besides adding the <code>LoadBalancer</code> section, I also appended the <code>domain-name</code> option to the end of the <code>Global</code> section, as in my OpenStack environment I want to authenticate against a non-default Keystone V3 domain.</p>

<h2>Installing the Kubernetes cluster</h2>

<p>After you&rsquo;ve sourced both the <code>openrc.sh</code> and <code>openstack-heat.sh</code> environment settings, you can kick off the installation of the Kubernetes cluster with:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>./cluster/kube-up.sh
</span></code></pre></td></tr></table></div></figure></p>

<p>After about 25 minutes, you should have a Kubernetes cluster up and running. You can check the status of the Kubernetes pods with the command:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>./cluster/kubectl.sh get pods <span class="p">&amp;</span>ndash<span class="p">;</span>namespace kube-system
</span></code></pre></td></tr></table></div></figure></p>

<p>All pods should be running. The network topology of the Kubernetes cluster as displayed by Horizon:</p>

<p><img class="center" src="/images/posts/kube.png"></p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Bootstrapping a Galera Cluster on RHEL7]]></title>
    <link href="http://alesnosek.com/blog/2016/01/31/bootstrapping-a-galera-cluster-on-rhel7/"/>
    <updated>2016-01-31T15:24:55-08:00</updated>
    <id>http://alesnosek.com/blog/2016/01/31/bootstrapping-a-galera-cluster-on-rhel7</id>
    <content type="html"><![CDATA[<p>The MariaDB Galera packages provided by the RDO project in their OpenStack repositories don&rsquo;t seem to include a command or script to bootstrap the cluster. Let&rsquo;s look at an alternative way to bring the cluster up.</p>

<!-- more -->


<p>RHEL7 comes with the init system <code>systemd</code>. Unfortunately, systemd doesn&rsquo;t provide a way to pass command-line arguments to the unit files. Hence, doing something like this won&rsquo;t work:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@rhel1 ~<span class="o">]</span><span class="nv">$ </span>systemctl start mariadb <span class="p">&amp;</span>ndash<span class="p">;</span>wsrep_new_cluster
</span><span class='line'>systemctl: unrecognized option <span class="p">&amp;</span>lsquo<span class="p">;&amp;</span>ndash<span class="p">;</span>wsrep_new_cluster<span class="p">&amp;</span>rsquo<span class="p">;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Instead of passing command-line arguments, systemd allows for creating <a href="http://0pointer.de/blog/projects/instances.html">multiple instances</a> of the same service where each instance can obtain it&rsquo;s own set of environment variables. The Percona XtraDB Cluster includes the standard and the bootstrap service instance definitions in the RPM package <code>Percona-XtraDB-Cluster-server</code>. To boostrap the Percona cluster, the first node can be started with the following command:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@percona1 ~<span class="o">]</span><span class="nv">$ </span>systemctl start &lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;&amp;#x6d;&amp;#97;&amp;#x69;&amp;#108;&amp;#x74;&amp;#x6f;&amp;#58;&amp;#x6d;&amp;#x79;&amp;#115;&amp;#113;&amp;#108;&amp;#x40;&amp;#98;&amp;#x6f;&amp;#111;&amp;#116;&amp;#115;&amp;#x74;&amp;#114;&amp;#x61;&amp;#112;&amp;#x2e;&amp;#115;&amp;#101;&amp;#x72;&amp;#118;&amp;#105;&amp;#99;&amp;#101;&quot;</span>&gt;<span class="p">&amp;</span><span class="c">#109;&amp;#121;&amp;#115;&amp;#x71;&amp;#108;&amp;#64;&amp;#98;&amp;#x6f;&amp;#x6f;&amp;#x74;&amp;#x73;&amp;#x74;&amp;#x72;&amp;#97;&amp;#112;&amp;#x2e;&amp;#115;&amp;#101;&amp;#x72;&amp;#x76;&amp;#105;&amp;#99;&amp;#x65;&lt;/a&gt;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>At the moment, this boostrap service definition is missing in the RDO OpenStack packages. Before a similar <code>mysql@.service</code> script is available in RDO you can start the MariaDB Galera cluster as follows:</p>

<ul>
<li><p>On the first node, start the MariaDB with the <code>--wsrep-new-cluster</code> to create a new cluster:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@rhel1 ~<span class="o">]</span><span class="nv">$ </span>/usr/bin/mysqld_safe <span class="p">&amp;</span>ndash<span class="p">;</span>wsrep-new-cluster
</span></code></pre></td></tr></table></div></figure>
Let the command run in the foreground.</p></li>
<li><p>On the remaining cluster nodes start the mariadb service as usual:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@rhel2 ~<span class="o">]</span><span class="nv">$ </span>systemctl start mariadb
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>After the cluster has been fully formed, stop the mariadb on the first node by sending it a SIGQUIT (press CTRL + \ on the console).</p></li>
<li><p>On the first node, start the mariadb service via systemd:
<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@rhel1 ~<span class="o">]</span><span class="nv">$ </span>systemctl start mariadb
</span></code></pre></td></tr></table></div></figure></p></li>
</ul>


<p>That&rsquo;s it. You can check the status of each of the cluster nodes by running the following command:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="o">[</span>root@rhel1 ~<span class="o">]</span><span class="nv">$ </span>mysql -e <span class="p">&amp;</span>ldquo<span class="p">;</span>SHOW GLOBAL STATUS LIKE <span class="p">&amp;</span>lsquo<span class="p">;</span>wsrep%<span class="p">&amp;</span>rsquo<span class="p">;;&amp;</span>rdquo<span class="p">;</span>
</span></code></pre></td></tr></table></div></figure></p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Monitoring OpenStack Cluster with Icinga]]></title>
    <link href="http://alesnosek.com/blog/2015/11/30/monitoring-openstack-cluster-with-icinga/"/>
    <updated>2015-11-30T21:13:33-08:00</updated>
    <id>http://alesnosek.com/blog/2015/11/30/monitoring-openstack-cluster-with-icinga</id>
    <content type="html"><![CDATA[<p>If you don&rsquo;t monitor it, it&rsquo;s not in production! To get an OpenStack cloud ready for production, monitoring is a must. Let&rsquo;s take a look at two projects providing Nagios/Icinga plugins for checking the health of OpenStack services.</p>

<!-- more -->


<p>First, a few words about <a href="https://www.icinga.org/" title="Icinga">Icinga</a>. I started using Icinga 2 only recently and I&rsquo;m very pleased with this flexible and well-documented software. I&rsquo;ve listened to a German presentation about Icinga where they said that Icinga was not that widely spread in the US as it was the case in Europe. Dear Icinga team, you have one more happy user in the US now. Your software just works and your web GUI is beautiful.</p>

<p>I found two very useful projects for monitoring the OpenStack APIs both hosted on GitHub:</p>

<ul>
<li><a href="https://github.com/cirrax/openstack-nagios-plugins">OpenStack Nagios Plugins</a></li>
<li><a href="https://github.com/openstack/monitoring-for-openstack">Monitoring for OpenStack</a></li>
</ul>


<h2>OpenStack Nagios Plugins</h2>

<p><a href="https://github.com/cirrax/openstack-nagios-plugins">OpenStack Nagios Plugins</a> provides a collection of checks for the OpenStack services Nova, Neutron, Cinder, Keystone and Ceilometer. Available plugins worked right away with my OpenStack Liberty cluster. The Nova Hypervisor check monitors the &ldquo;virtual&rdquo; CPU and memory usage across your compute nodes. The name virtual CPU is a little misleading here. In reality, the number of physical cores is monitored as the Nova API actually reports the number of physical cores. I stick to the OpenStack default settings that overcommit the CPUs by factor of 16 and the memory by factor of 1.5. To accommodate this fact, I changed the warning and critical ranges for the check_nova-hypervisors plugin as follows:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>check_nova-hypervisors <span class="p">&amp;</span>ndash<span class="p">;</span>warn_memory_percent 0:135 <span class="p">&amp;</span>ndash<span class="p">;</span>critical_memory_percent 0:142 <span class="p">&amp;</span>ndash<span class="p">;</span>warn_vcpus_percent 0:1440 <span class="p">&amp;</span>ndash<span class="p">;</span>critical_vcpus_percent 0:1520
</span></code></pre></td></tr></table></div></figure></p>

<h2>Monitoring for OpenStack</h2>

<p>Plugins coming with the <a href="(https://github.com/openstack/monitoring-for-openstack">Monitoring for OpenStack</a> project provide deeper checks of OpenStack functionality. I liked the following ones the best:</p>

<ul>
<li><code>check_nova_instance</code>: Creates an instance on your cloud and deletes it again as soon as it is active. It&rsquo;s recommended to use a small disk image like cirros for this check.</li>
<li><code>cinder_volume</code>: Allocates a volume of size 1GB and deletes it again.</li>
<li><code>neutron_floating_ip</code>: Tries to allocate a floating IP. You have to configure the network where to allocate the IP from.</li>
<li><code>glance_upload</code>: Uploads 1MB of data as an image into Glance.</li>
<li><code>check_horizon_login</code>: Given a user name and a password the plugin will log into the Horizon dashboard.</li>
</ul>


<p>Some of the plugins didn&rsquo;t work for me due to incompatibilities with the Liberty client APIs. If you encounter the same problem you can try out my fixed version of the plugins on GitHub <a href="https://github.com/noseka1/monitoring-for-openstack">here</a>.</p>

<h2>Icinga 2 Screenshot</h2>

<p>And this is how the OpenStack APIs service group looks in Icinga Web 2. Happy monitoring!</p>

<p><img class="center" src="/images/posts/osmon.png"></p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Assigning Roles to Nodes Directly in RDO]]></title>
    <link href="http://alesnosek.com/blog/2015/11/09/assigning-roles-to-nodes-directly-in-rdo/"/>
    <updated>2015-11-09T20:49:40-08:00</updated>
    <id>http://alesnosek.com/blog/2015/11/09/assigning-roles-to-nodes-directly-in-rdo</id>
    <content type="html"><![CDATA[<p>RDO Manager defines multiple roles that nodes can play in OpenStack deployment. For large-sized installations, RDO features automatic assignment of roles to nodes. This assignment is based on the facts that RDO obtained about each node during the introspection. However, for smaller deployments, you might prefer to assign the roles to the available nodes by hand. It was not straight forward for me to find out about this manual option even when it is described in the <a href="http://docs.openstack.org/developer/tripleo-docs/advanced_deployment/profile_matching.html#optional-manually-add-the-profiles-to-the-nodes" title="TripleO documentation">TripleO documentation</a>. Let&rsquo;s review the required configuration steps in this blogpost.</p>

<!-- more -->


<p>The relationship between roles and nodes is organized via flavors. A flavor is a set of properties that the target node must match in order to be eligible for deployment of a specific role. The manual assignment of a role to a node is a three-step process:</p>

<ol>
<li>Define a flavor with a property <code>capabilities:profile</code> set to the role name</li>
<li>Add the same profile to the capabilities list of the target node</li>
<li>Tell RDO what flavor to use for a specific role when beginning the deployment</li>
</ol>


<p>The creation of flavors with the associated <code>capabilities:profile</code> property looks like this:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>openstack flavor create <span class="p">&amp;</span>ndash<span class="p">;</span>id auto <span class="p">&amp;</span>ndash<span class="p">;</span>ram <span class="m">4096</span> <span class="p">&amp;</span>ndash<span class="p">;</span>disk <span class="m">40</span> <span class="p">&amp;</span>ndash<span class="p">;</span>vcpus <span class="m">1</span> ceph
</span><span class='line'>openstack flavor create <span class="p">&amp;</span>ndash<span class="p">;</span>id auto <span class="p">&amp;</span>ndash<span class="p">;</span>ram <span class="m">4096</span> <span class="p">&amp;</span>ndash<span class="p">;</span>disk <span class="m">40</span> <span class="p">&amp;</span>ndash<span class="p">;</span>vcpus <span class="m">1</span> cinder
</span><span class='line'>openstack flavor create <span class="p">&amp;</span>ndash<span class="p">;</span>id auto <span class="p">&amp;</span>ndash<span class="p">;</span>ram <span class="m">4096</span> <span class="p">&amp;</span>ndash<span class="p">;</span>disk <span class="m">40</span> <span class="p">&amp;</span>ndash<span class="p">;</span>vcpus <span class="m">1</span> compute
</span><span class='line'>openstack flavor create <span class="p">&amp;</span>ndash<span class="p">;</span>id auto <span class="p">&amp;</span>ndash<span class="p">;</span>ram <span class="m">4096</span> <span class="p">&amp;</span>ndash<span class="p">;</span>disk <span class="m">40</span> <span class="p">&amp;</span>ndash<span class="p">;</span>vcpus <span class="m">1</span> controller
</span><span class='line'>openstack flavor create <span class="p">&amp;</span>ndash<span class="p">;</span>id auto <span class="p">&amp;</span>ndash<span class="p">;</span>ram <span class="m">4096</span> <span class="p">&amp;</span>ndash<span class="p">;</span>disk <span class="m">40</span> <span class="p">&amp;</span>ndash<span class="p">;</span>vcpus <span class="m">1</span> swift&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;openstack flavor <span class="nb">set</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>cpu_arch<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>x86_64<span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>capabilities:boot_option<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span><span class="nb">local</span><span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>capabilities:profile<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>ceph<span class="p">&amp;</span>rdquo<span class="p">;</span> ceph
</span><span class='line'>openstack flavor <span class="nb">set</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>cpu_arch<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>x86_64<span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>capabilities:boot_option<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span><span class="nb">local</span><span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>capabilities:profile<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>cinder<span class="p">&amp;</span>rdquo<span class="p">;</span> cinder
</span><span class='line'>openstack flavor <span class="nb">set</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>cpu_arch<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>x86_64<span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>capabilities:boot_option<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span><span class="nb">local</span><span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>capabilities:profile<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>compute<span class="p">&amp;</span>rdquo<span class="p">;</span> compute
</span><span class='line'>openstack flavor <span class="nb">set</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>cpu_arch<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>x86_64<span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>capabilities:boot_option<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span><span class="nb">local</span><span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>capabilities:profile<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>controller<span class="p">&amp;</span>rdquo<span class="p">;</span> controller
</span><span class='line'>openstack flavor <span class="nb">set</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>cpu_arch<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>x86_64<span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>capabilities:boot_option<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span><span class="nb">local</span><span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>property <span class="p">&amp;</span>ldquo<span class="p">;</span>capabilities:profile<span class="p">&amp;</span>rdquo<span class="p">;</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>swift<span class="p">&amp;</span>rdquo<span class="p">;</span> swift
</span></code></pre></td></tr></table></div></figure></p>

<p>Now we need to add the profiles to the capabilities list of the respective nodes:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>ironic node-update &lt;node1 UUID here&gt; replace properties/capabilities<span class="o">=</span><span class="p">&amp;</span>lsquo<span class="p">;</span>profile:ceph,boot_option:local<span class="p">&amp;</span>rsquo<span class="p">;</span>
</span><span class='line'>ironic node-update &lt;node2 UUID here&gt; replace properties/capabilities<span class="o">=</span><span class="p">&amp;</span>lsquo<span class="p">;</span>profile:cinder,boot_option:local<span class="p">&amp;</span>rsquo<span class="p">;</span>
</span><span class='line'>ironic node-update &lt;node3 UUID here&gt; replace properties/capabilities<span class="o">=</span><span class="p">&amp;</span>lsquo<span class="p">;</span>profile:compute,boot_option:local<span class="p">&amp;</span>rsquo<span class="p">;</span>
</span><span class='line'>ironic node-update &lt;node4 UUID here&gt; replace properties/capabilities<span class="o">=</span><span class="p">&amp;</span>lsquo<span class="p">;</span>profile:controller,boot_option:local<span class="p">&amp;</span>rsquo<span class="p">;</span>
</span><span class='line'>ironic node-update &lt;node5 UUID here&gt; replace properties/capabilities<span class="o">=</span><span class="p">&amp;</span>lsquo<span class="p">;</span>profile:swift,boot_option:local<span class="p">&amp;</span>rsquo<span class="p">;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>When deploying the OpenStack cloud, we need to tell the RDO manager what flavor to use for each specific role:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>openstack overcloud deploy <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>templates /usr/share/openstack-tripleo-heat-templates <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>ceph-storage-scale <span class="m">1</span> <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>block-storage-scale <span class="m">1</span> <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>compute-scale <span class="m">1</span> <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>control-scale <span class="m">1</span> <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>swift-storage-scale <span class="m">1</span> <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>ceph-storage-flavor ceph <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>block-storage-flavor cinder <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>compute-flavor compute <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>control-flavor controller <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>swift-storage-flavor swift
</span></code></pre></td></tr></table></div></figure></p>

<p>And that&rsquo;s all for today. Hope you&rsquo;re enjoying the full control over your OpenStack cloud deployment.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Installing OpenStack Liberty on RHEL7]]></title>
    <link href="http://alesnosek.com/blog/2015/10/19/installing-openstack-liberty-on-rhel7/"/>
    <updated>2015-10-19T22:30:48-07:00</updated>
    <id>http://alesnosek.com/blog/2015/10/19/installing-openstack-liberty-on-rhel7</id>
    <content type="html"><![CDATA[<p>The OpenStack Liberty was released last week. In this article I&rsquo;ll briefly describe how to deploy the OpenStack Liberty on RHEL7 using RDO Manager.</p>

<!-- more -->


<p>The <a href="https://www.rdoproject.org/" title="RDO project">RDO project</a> packages the OpenStack software for the Red Hat based platforms. Currently, there are Liberty packages in status testing/release candidate available from the project. Apart from a couple of configuration issues the installation went pretty well and I obtained a basic 2-node cluster.</p>

<p>If you intalled OpenStack Kilo using RDO Manager before I have a good news for you. The installation procedure remains pretty much the same. You can follow the <a href="http://docs.openstack.org/developer/tripleo-docs/" title="TripleO Doc">great guide</a> provided by the TripleO project to get the installation going. First, add the following two repositories:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;a href="http://trunk.rdoproject.org/centos7/current-tripleo/delorean.repo">http://trunk.rdoproject.org/centos7/current-tripleo/delorean.repo&lt;/a>
</span><span class='line'>&lt;a href="http://trunk.rdoproject.org/centos7/delorean-deps.repo">http://trunk.rdoproject.org/centos7/delorean-deps.repo&lt;/a></span></code></pre></td></tr></table></div></figure></p>

<p>Now you can install the RDO Manager with:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>sudo yum install python-tripleoclient</span></code></pre></td></tr></table></div></figure></p>

<p>In the Kilo release, the <code>python-tripleoclient</code> package was called <code>python-rdomanager-oscplugin</code>. You can continue with the guide. To build the overcloud images I issue the commands:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nb">export </span><span class="nv">NODE_DIST</span><span class="o">=</span>rhel7
</span><span class='line'><span class="nb">export </span><span class="nv">DIB_LOCAL_IMAGE</span><span class="o">=</span>rhel-guest-image-7.1-20150224.0.x86_64.qcow2
</span><span class='line'><span class="nb">export </span><span class="nv">REG_METHOD</span><span class="o">=</span>disable
</span><span class='line'><span class="nb">export </span><span class="nv">DIB_DEBUG_TRACE</span><span class="o">=</span>1
</span><span class='line'><span class="nb">export </span><span class="nv">DIB_YUM_REPO_CONF</span><span class="o">=</span>/etc/yum.repos.d/rhel7_mirror.repo
</span><span class='line'><span class="nb">export </span><span class="nv">USE_DELOREAN_TRUNK</span><span class="o">=</span>1
</span><span class='line'><span class="nb">export </span><span class="nv">DELOREAN_TRUNK_REPO</span><span class="o">=</span>&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;http://trunk.rdoproject.org/centos7/current-tripleo&quot;</span>&gt;http://trunk.rdoproject.org/centos7/current-tripleo&lt;/a&gt;
</span><span class='line'><span class="nb">export </span><span class="nv">DELOREAN_REPO_FILE</span><span class="o">=</span>delorean.repo
</span><span class='line'>openstack overcloud image build <span class="p">&amp;</span>ndash<span class="p">;</span>all
</span></code></pre></td></tr></table></div></figure></p>

<p>As you can see, I&rsquo;m not really registering the OpenStack nodes with the Red Hat portal. Instead, I&rsquo;m pulling the RHEL7 packages from the local mirror.</p>

<p>After the installation of the overcloud has completed I realized that some of the OpenStack processes on the overcloud nodes were segfaulting. After switching SELinux from enforcing to permissive mode everything started working as expected.</p>

<h2>A final note</h2>

<p>The deployment of OpenStack is rather an involved process even when leveraging the tools like RDO Manager. To truly automate the installation in my specific environment I use a set of Ansible scripts to drive the RDO manager installation. Now that Red Hat acquired Ansible I&rsquo;m wondering if we&rsquo;re going to get an even better OpenStack installation experience on Red Hat based distributions.</p>
]]></content>
  </entry>

</feed>
