<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: devops | Ales Nosek - The Software Practitioner]]></title>
  <link href="http://alesnosek.com/blog/categories/devops/atom.xml" rel="self"/>
  <link href="http://alesnosek.com/"/>
  <updated>2018-03-08T20:11:04-08:00</updated>
  <id>http://alesnosek.com/</id>
  <author>
    <name><![CDATA[Ales Nosek]]></name>

  </author>
  <generator uri="http://octopress.org/">Octopress</generator>


  <entry>
    <title type="html"><![CDATA[18 Months with OpenStack, Our Experience, Part I]]></title>
    <link href="http://alesnosek.com/blog/2018/02/19/18-months-with-openstack-our-experience-part-i/"/>
    <updated>2018-02-19T21:06:17-08:00</updated>
    <id>http://alesnosek.com/blog/2018/02/19/18-months-with-openstack-our-experience-part-i</id>
    <content type="html"><![CDATA[<p>It has been 18 months since we deployed OpenStack cloud in our company. In this article, I would like to review our time with OpenStack and describe some of the experience we gained. Are you thinking about building an OpenStack-based cloud? This article might provide you with additional insights and tips that will help you succeed.</p>

<!-- more -->


<h2>Introduction</h2>

<p>Right at the beginning, I&rsquo;d like to say that our company is not a cloud provider. We didn&rsquo;t build a cloud to provide a service to customers. Instead, we built a private cloud to support our engineering team in their software development efforts. Our OpenStack is running development machines, build machines and test machines. The requirements on the availability and reliability of our OpenStack cluster are therefore lower than the requirements that a public cloud would have to meet.</p>

<p>We started evaluating OpenStack around the Kilo release and ended up going with Mitaka into production. Around that time, Internet was flooded with articles criticizing the complexity of OpenStack including scary stories of companies failing their OpenStack projects. I can confirm that deploying OpenStack was a real challenge and that as we were working on it, the OpenStack version of the Kennedy&rsquo;s famous words passed through my mind several times:</p>

<blockquote><p>We choose to deploy OpenStack not because it is easy, but because it is hard.</p></blockquote>

<p>However, nothing can scare away a proficent software practitioner. Eventually, we got the job done and the invested effort did pay off.</p>

<h2>OpenStack essentials</h2>

<p><a href="https://www.openstack.org/">OpenStack</a> is an open-source infrastructure-as-a-service (IaaS) cloud platform. It&rsquo;s important to understand that OpenStack itself is only a controlling layer that relies on other software projects to provide the implementation of the underlying functionality. For instance, OpenStack can spin up virtual machines, however, you would not find any code in the OpenStack project that would actually implement a hypervisor. Instead, OpenStack integrates with an existing hypervisor software to do the job. A very popular choice of the hypervisor used along with OpenStack is <a href="https://www.linux-kvm.org">KVM</a> and so when the user creates a virtual machine on OpenStack, OpenStack merely calls the KVM hypervisor to spin up the virtual machine. The same holds true for other areas of OpenStack functionality like networking and storage where OpenStack drives the underlying networking and storage software to do the actual job.</p>

<p>When planning an OpenStack cluster, you will have to choose from a variety of underlying technologies. For instance, in addition to the KVM hypervisor, OpenStack also supports Xen, VMware vSphere and Hyper-V. Most of the time you will just pick the technology you already run at your place and for which you have the staff to manage it. The freedom of choice you have with OpenStack is amazing, however, I would recommend to always look at the most popular choices first because their integration with OpenStack tends to be more solid. In our case, we chose KVM as a hypervisor, <a href="http://www.openvswitch.org/">Open vSwitch</a> as a networking implementation and <a href="https://ceph.com/">Ceph</a> to provide object and block storage.</p>

<p>OpenStack is an umbrella project under which you can find a host of projects each dealing with a different portion of the cloud functionality. The core projects that can find installed in the majority of OpenStack deployments are:</p>

<ul>
<li><a href="https://wiki.openstack.org/wiki/Nova">Nova</a>. Manages virtual machines.</li>
<li><a href="https://wiki.openstack.org/wiki/Neutron">Neutron</a>. Provides networking to virtual machines.</li>
<li><a href="https://wiki.openstack.org/wiki/Cinder">Cinder</a>. Provides block storage that can be attached to virtual machines.</li>
<li><a href="https://wiki.openstack.org/wiki/Glance">Glance</a>. Stores virtual machine images.</li>
<li><a href="https://wiki.openstack.org/wiki/Horizon">Horizon</a>. Web-based user interface to OpenStack services.</li>
<li><a href="https://wiki.openstack.org/wiki/Keystone">Keystone</a>. Identity service.</li>
</ul>


<p>Additionally, project <a href="https://wiki.openstack.org/wiki/Heat">Heat</a>, <a href="https://wiki.openstack.org/wiki/Swift">Swift</a>, and <a href="https://wiki.openstack.org/wiki/Telemetry">Ceilometer</a> are also rather popular. You can find plenty of other OpenStack projects listed on the <a href="https://www.openstack.org/software/project-navigator">Project navigator</a> page. When choosing OpenStack projects for your deployment, you should always consider the adoption and the maturity of the projects. Many projects on the list are still in the early stages of development and not ready for production use.</p>

<p>OpenStack was designed for massive scale deployments as you can tell if you look at the <a href="https://docs.openstack.org/arch-design/design.html">OpenStack architecture diagram</a>. Each OpenStack project consists of multiple services (daemons) that can be deployed on separate physical machines allowing OpenStack to scale out. OpenStack services communicate with each other over the network using RESTful APIs. In addition, some of the projects like Nova, Neutron and Cinder chose to leverage a message broker for internal communication. The high number of services that form an OpenStack deployment contributes to its operational complexity.</p>

<h2>Getting started with OpenStack</h2>

<p>If you are new to OpenStack, a great place to start learning OpenStack is the <a href="https://docs.openstack.org/devstack/latest/">DevStack project</a>. DevStack allows you to create an all-in-one deployment of OpenStack. With DevStack you can access debug logs of individual OpenStack services as well as easily restart OpenStack services after you changed their configuration. It took me a while to figure out which configuration option affects which OpenStack service and how OpenStack services communicate with each other. I learned a lot by re-deploying DevStack many times, trying to make the individual OpenStack features work properly.</p>

<p>OpenStack is a fast moving project with two major releases per year. Especially in the past, the project documentation could not keep up with the many changes packed in each release. The documentation was outdated on many places or was missing altogether. When working with OpenStack I quickly realized that reading the OpenStack&rsquo;s Python code was necessary in order to understand how some of the configuration options worked or when troubleshooting various issues.</p>

<blockquote><p> Ability to read the OpenStack source code was required to succeed.</p></blockquote>

<p>OpenStack is written using a beautiful idiomatic Python code which was most of the time a pleasure to read. At first, I started walking through the code of simpler projects like Glance and learned the patterns that were commonly used in other OpenStack projects, too. Only later I dived deeper into the internals of Nova, the OpenStack&rsquo;s brain that schedules and creates virtual machines. When between OpenStack releases configuration options were renamed or moved to different INI file sections, I just grepped through the source code and learned about the changes avoiding any further frustration.</p>

<p>If you are getting started with OpenStack, prepare for a steep learning curve. Apart from studying the OpenStack project documentation, you will have to refer to the documentation of the technologies that you integrate with OpenStack, too. For instance, I spend quite a bit of time studying the documentation of <a href="https://www.rabbitmq.com/documentation.html">RabbitMQ</a>, <a href="https://libvirt.org/docs.html">libvirt</a>, <a href="http://docs.openvswitch.org">Open vSwitch</a>, and <a href="http://docs.ceph.com">Ceph</a>.</p>

<h2>Choosing an OpenStack distribution</h2>

<p>There are several OpenStack distributions available out there. For us the choice was pretty straight forward. As we are a Red Hat shop, we went with <a href="https://www.rdoproject.org/">RDO</a> installed on top of RHEL7. I spent large amounts of time working with RDO and yeah, it was challenging, at least in its Mitaka release. For further details on our experience with OpenStack RDO, you can refer to articles: <a href="/blog/2016/03/27/tripleo-installer-the-good/">1</a>, <a href="/blog/2017/01/15/tripleo-installer-production-ready">2</a>. Due to the complexity of OpenStack, it is rather difficult to create a tool to manage its life-cycle and I&rsquo;m certain that further development effort will have to be spent before reaching perfection.</p>

<p>By the way, some OpenStack distributions come with a GUI-based installer. As there are dozens of configuration parameters to set during the installation, I don&rsquo;t see the point of using a graphical interface to do this. Instead, a well commented configuration file seems more desirable to me. Does the GUI-based installer enable product managers to make a check mark on their data sheet? I would say yes, but you can safely ignore it when choosing your OpenStack distribution.</p>

<h2>Choosing server hardware</h2>

<p>We started with a small OpenStack deployment comprised of 3 controller nodes, 2 compute nodes and 3 Ceph nodes. Over time, we added further nodes to meet the growing demand and ended up with the current size of the cluster being 3 controller nodes, 13 compute nodes and 7 Ceph nodes. Majority of the nodes are HP ProLiant DL360 Gen9 machines with the following hardware parameters:</p>

<ul>
<li><strong>Controller nodes.</strong> 96GB RAM, 2 x 300GB SAS 10K HDD in RAID1, 1Gbit Ethernet NICs</li>
<li><strong>Compute nodes.</strong> 288GB RAM, 8 x 300GB SAS 10K HDD in RAID10 (for the OS + instance ephemeral storage), 1Gbit and 10Gbit Ethernet NICs</li>
<li><strong>Ceph nodes.</strong> 32GB RAM, 2 x 300GB SAS 10K HDD in RAID1 (for the OS), 6 x 1.2TB SAS 10K HDD (Ceph OSD storage drives), 1Gbit and 10Gbit Ethernet NICs</li>
</ul>


<p>1Gbit Ethernet NICs are used to access OpenStack APIs on the controller nodes. Compute nodes and Ceph storage nodes are interconnected using 10Gbit Ethernet. All network interfaces are bonded and connected to two different switches to avoid a single-point-of-failure. There is a dedicated 1Gbit link attached to each of the OpenStack nodes used for node management via SSH.</p>

<p>From our experience, each of the compute nodes can run up to 40-50 virtual machines using the default OpenStack RDO settings: cpu_allocation_ratio=16.0, ram_allocation_ratio=1.0 and disk_allocation_ratio=1.0. Our current limit preventing us to achieve even higher density is the amount of provisioned RAM on the nodes. In the future, we are considering adding more RAM to the compute nodes or increasing the ram_allocation_ratio.</p>

<h2>Deployment overview</h2>

<p>Finally, we are going to take a look at the high-level overview of our OpenStack deployment. In the diagram below you can see the OpenStack projects that we chose for the deployment:</p>

<p><img src="/images/posts/18_months_with_openstack_components.png" width="800" height="1000" title="OpenStack Components" ></p>

<p>Let me comment on some of the projects we deployed:</p>

<ul>
<li><strong><a href="https://docs.openstack.org/ironic">Ironic</a>.</strong> We deployed Ironic in order to manage baremetal machines that we use for performance testing. Performance tests are more accurate when carried out in an isolated baremetal environment than on the virtual machines that share the resources of the hypervisor. To this date we didn&rsquo;t realize this our plan but we will get back to it in the future.</li>
<li><strong><a href="https://docs.openstack.org/magnum">Magnum</a>.</strong> Magnum project simplifies the deployment of container orchestrators like Kubernetes, Swarm and Mesos on top of OpenStack. To accomplish this, Magnum leverages Heat templates behind the scenes and the actual provisioning is done by Heat. To be honest, we never really started using Magnum. When deploying Kubernetes, we preferred to use Heat templates provided by the Kubernetes project. This approach turned to be more straight forward than involving yet another service like Magnum. You can read about it <a href="/blog/2016/06/26/deploying-kubernetes-on-openstack-using-heat">here</a>.</li>
<li><strong><a href="https://docs.openstack.org/sahara">Sahara</a>.</strong> Sahara project allows you to deploy big data frameworks like Apache Hadoop and Apache Spark on top of OpenStack. We made similar experience with Sahara as we made with Magnum. We just didn&rsquo;t start using it at all. It turned out that there were already pre-existing deployment scripts provided by Hortonworks and others that it made no sense for us to use Sahara. While Hortonworks <a href="https://github.com/hortonworks/ansible-hortonworks">scripts</a> can deploy Hadoop on any of the major clouds, Sahara would be an OpenStack-only solution.</li>
<li><strong><a href="https://docs.openstack.org/manila">Manila</a>.</strong> While not depicted in the diagram, we also deployed OpenStack Manila. Manila is a shared file system service and we use it to provision NFS shares. Manila project started as a code copy of the Cinder project and perhaps that&rsquo;s why it was pretty stable and usable soon after its inception. I wrote an <a href="http://alesnosek.com/blog/2016/05/22/test-driving-openstack-manila/">article</a> about Manila at the time we were evaluating it.</li>
<li><strong><a href="https://docs.openstack.org/designate">Designate</a>.</strong>  Designate is a DNS as a service for OpenStack. After evaluating this project, we realized that for our simple purpose Designate was too involved. We ended up writing a Python script that dynamically registers OpenStack virtual machines with our internal DNS server. This script works reliably ever since and you can read about it in this <a href="/blog/2015/05/31/openstack-dynamic-dns-updates">blog post</a>.</li>
</ul>


<h2>Conclusion</h2>

<p>In this post, we described some of our experience with planning the OpenStack cloud and deploying it. In the second blog post, we are going to share the lessons learned when operating OpenStack.</p>

<p>If you have battle scars from working with OpenStack, I would love to hear from you. Please, feel free to share your comments and stories in the comment section below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Edge Security for Your Cloud Application, Part II]]></title>
    <link href="http://alesnosek.com/blog/2018/01/12/edge-security-for-your-cloud-application-part-ii/"/>
    <updated>2018-01-12T21:21:19-08:00</updated>
    <id>http://alesnosek.com/blog/2018/01/12/edge-security-for-your-cloud-application-part-ii</id>
    <content type="html"><![CDATA[<p>In this article, we&rsquo;re going to create a proof-of-concept deployment featuring a non-TLS client connecting to our cloud application. We are going to leverage the architecture approach discussed in the <a href="/blog/2018/01/10/edge-security-for-your-cloud-application-part-i">previous blog post</a>. A secure communication channel is going to be established between the client and the cloud application including mutual authentication.</p>

<!-- more -->


<h2>Deployment overview</h2>

<p>Before we get our hands dirty, let&rsquo;s gain a better understanding of what we are trying to achieve. The diagram depicting our test deployment looks as follows:</p>

<p><img src="/images/posts/edge_security_for_your_cloud_application_poc_arch.svg" width="1000" title="Architecture Overview" ></p>

<p>We&rsquo;re going to spin up two virtual instances. The edge instance will host our edge service. The client instance will host the client that will be accessing our edge service. For the sake of POC, the edge service is going to be an Apache web server and we&rsquo;re going to use the curl command-line utility in place of the client. The battle-proven HAProxy is going to play the role of the client-side as well as the server-side proxy, securing the client-server communication.</p>

<h2>Getting started</h2>

<p>You can start off with creating two CentOS 7 instances in AWS. Choose a minimalist t2.micro instance type which is sufficient for our proof of concept. In the security groups settings, make sure that in addition to the SSH port you have also enabled access to port 443 (HTTPS) from anywhere.</p>

<p>After the instances booted up, install HAProxy on both instances:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>yum install -y haproxy
</span></code></pre></td></tr></table></div></figure></p>

<p>And create a directory that will hold the keys and certificates required by HAProxy:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>mkdir /etc/haproxy/ssl
</span></code></pre></td></tr></table></div></figure></p>

<p>Make sure that you run the above command on both instances, too.</p>

<h2>PKI keys and certificates</h2>

<p>We are going to leverage the TLS protocol to establish a secure, mutually authenticated connection between the two proxies. TLS relies on PKI keys and certificates that we&rsquo;ll need to generate. The PKI setup for our company consists of a root CA, a layer of subordinate CAs and three end-entity certificates.</p>

<p><img src="/images/posts/edge_security_for_your_cloud_application_pki.svg" width="500" height="600" title="PKI" ></p>

<p>It is a common practice to sign the end-entity certificates by one or more subordinate CAs as it prevents the necessity of revoking a root certificate in the case that an end-entity certificate is incorrectly issued or compromised.</p>

<p>In total, we are going to generate three end-entity certificates. <code>Edge Service Certificate</code> is going to be used by the reverse proxy running on the edge instance in order to authenticate itself to the clients. <code>Customer1 Client Certificate</code> and <code>Customer2 Client Certificate</code> are certificates that our company securely distributes to the tenants (customers). Each tenant uses her certificate and the associated private key to authenticate herself when accessing the cloud application.</p>

<p>A PKI certificate can be created in three steps:</p>

<ol>
<li>Generate a private key.</li>
<li>Using the private key, generate a Certificate Signing Request (CSR).</li>
<li>Using a CA certificate along with the respective private key and the CSR, generate the certificate.</li>
</ol>


<p>An exeption from this three-step procedure is the root certificate which is self-signed.</p>

<p>In the following, we are going to generate seven certificates. All the commands are to be issued on the edge instance. Note that you can populate the certificate fields with pretty arbitrary values with one exception: the <code>Common Name</code> field of the end-entity certificates. <code>Common Name</code> of the <code>Edge Service Certificate</code> must match the DNS name of the edge instance. <code>Common Name</code> of the <code>Customer1 Client Certificate</code> and the <code>Customer2 Client Certificate</code> must match the HAProxy configuration. By inspecting the <code>Common Name</code> field, HAProxy is able to recognize which client is trying to access the cloud application and it is able to route the client request to the appropriate backend service.</p>

<h3>Company RootCA certificate</h3>

<p>Let&rsquo;s generate the company&rsquo;s greatest secret - the private key of the Company&rsquo;s RootCA:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl genrsa -out rootca.company.example.key.pem 4096
</span></code></pre></td></tr></table></div></figure></p>

<p>And create a self-signed RootCA certificate. Below you can see the sample input data:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h2&gt;<span class="nv">$ </span>openssl req -key rootca.company.example.key.pem -new -x509 -extensions v3_ca -out rootca.company.example.crt.pem&lt;/h2&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Country Name <span class="o">(</span><span class="m">2</span> letter code<span class="o">)</span> <span class="o">[</span>XX<span class="o">]</span>:US
</span><span class='line'>State or Province Name <span class="o">(</span>full name<span class="o">)</span> <span class="o">[]</span>:CA
</span><span class='line'>Locality Name <span class="o">(</span>eg, city<span class="o">)</span> <span class="o">[</span>Default City<span class="o">]</span>:San Diego
</span><span class='line'>Organization Name <span class="o">(</span>eg, company<span class="o">)</span> <span class="o">[</span>Default Company Ltd<span class="o">]</span>:Company RootCA
</span><span class='line'>Organizational Unit Name <span class="o">(</span>eg, section<span class="o">)</span> <span class="o">[]</span>:
</span><span class='line'>Common Name <span class="o">(</span>eg, your name or your server<span class="p">&amp;</span>rsquo<span class="p">;</span>s hostname<span class="o">)</span> <span class="o">[]</span>:rootca.company.example
</span><span class='line'>Email Address <span class="o">[]</span>:
</span></code></pre></td></tr></table></div></figure></p>

<p>For the sake of conciseness, I shortened the console output a bit. Note that we are adding the command-line option <code>-extensions v3_ca</code> to denote this certificate as a CA certificate. Otherwise, an end-entity certificate would have been generated by default.</p>

<h3>Company SubCA certificate</h3>

<p>Next, issue the command to generate the private key of the Company&rsquo;s SubCA:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl genrsa -out subca.company.example.key.pem 4096
</span></code></pre></td></tr></table></div></figure></p>

<p>Using the private key, let&rsquo;s create a Certificate Signing Request:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h2&gt;<span class="nv">$ </span>openssl req -new -key subca.company.example.key.pem -out subca.company.example.csr.pem&lt;/h2&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Country Name <span class="o">(</span><span class="m">2</span> letter code<span class="o">)</span> <span class="o">[</span>XX<span class="o">]</span>:US
</span><span class='line'>State or Province Name <span class="o">(</span>full name<span class="o">)</span> <span class="o">[]</span>:CA
</span><span class='line'>Locality Name <span class="o">(</span>eg, city<span class="o">)</span> <span class="o">[</span>Default City<span class="o">]</span>:San Diego
</span><span class='line'>Organization Name <span class="o">(</span>eg, company<span class="o">)</span> <span class="o">[</span>Default Company Ltd<span class="o">]</span>:Company SubCA
</span><span class='line'>Organizational Unit Name <span class="o">(</span>eg, section<span class="o">)</span> <span class="o">[]</span>:
</span><span class='line'>Common Name <span class="o">(</span>eg, your name or your server<span class="p">&amp;</span>rsquo<span class="p">;</span>s hostname<span class="o">)</span> <span class="o">[]</span>:subca.company.example
</span><span class='line'>Email Address <span class="o">[]</span>:&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Please enter the following <span class="p">&amp;</span>lsquo<span class="p">;</span>extra<span class="p">&amp;</span>rsquo<span class="p">;</span> attributes
</span><span class='line'>to be sent with your certificate request
</span><span class='line'>A challenge password <span class="o">[]</span>:
</span><span class='line'>An optional company name <span class="o">[]</span>:
</span></code></pre></td></tr></table></div></figure></p>

<p>And finally, generate the certificate of the Company&rsquo;s SubCA:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl x509 -req -in subca.company.example.csr.pem -CA rootca.company.example.crt.pem -CAkey rootca.company.example.key.pem -CAcreateserial -extfile /etc/pki/tls/openssl.cnf -extensions v3_ca -out subca.company.example.crt.pem
</span></code></pre></td></tr></table></div></figure></p>

<h3>Customer1 SubCA certificate</h3>

<p>Steps to generate the Customer1 SubCA certificate should be quite clear now:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl genrsa -out subca.customer1.example.key.pem 4096
</span></code></pre></td></tr></table></div></figure></p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h2&gt;<span class="nv">$ </span>openssl req -new -key subca.customer1.example.key.pem -out subca.customer1.example.csr.pem&lt;/h2&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Country Name <span class="o">(</span><span class="m">2</span> letter code<span class="o">)</span> <span class="o">[</span>XX<span class="o">]</span>:US
</span><span class='line'>State or Province Name <span class="o">(</span>full name<span class="o">)</span> <span class="o">[]</span>:CA
</span><span class='line'>Locality Name <span class="o">(</span>eg, city<span class="o">)</span> <span class="o">[</span>Default City<span class="o">]</span>:San Diego
</span><span class='line'>Organization Name <span class="o">(</span>eg, company<span class="o">)</span> <span class="o">[</span>Default Company Ltd<span class="o">]</span>:Customer1 SubCA
</span><span class='line'>Organizational Unit Name <span class="o">(</span>eg, section<span class="o">)</span> <span class="o">[]</span>:
</span><span class='line'>Common Name <span class="o">(</span>eg, your name or your server<span class="p">&amp;</span>rsquo<span class="p">;</span>s hostname<span class="o">)</span> <span class="o">[]</span>:subca.customer1.example
</span><span class='line'>Email Address <span class="o">[]</span>:&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Please enter the following <span class="p">&amp;</span>lsquo<span class="p">;</span>extra<span class="p">&amp;</span>rsquo<span class="p">;</span> attributes
</span><span class='line'>to be sent with your certificate request
</span><span class='line'>A challenge password <span class="o">[]</span>:
</span><span class='line'>An optional company name <span class="o">[]</span>:
</span></code></pre></td></tr></table></div></figure></p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl x509 -req -in subca.customer1.example.csr.pem -CA rootca.company.example.crt.pem -CAkey rootca.company.example.key.pem -CAserial rootca.srl -extfile /etc/pki/tls/openssl.cnf -extensions v3_ca -out subca.customer1.example.crt.pem
</span></code></pre></td></tr></table></div></figure></p>

<h3>Customer2 SubCA certificate</h3>

<p>Analogicaly,  we are going to generate the Customer2 SubCA certificate:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl genrsa -out subca.customer2.example.key.pem 4096
</span></code></pre></td></tr></table></div></figure></p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h2&gt;<span class="nv">$ </span>openssl req -new -key subca.customer2.example.key.pem -out subca.customer2.example.csr.pem&lt;/h2&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Country Name <span class="o">(</span><span class="m">2</span> letter code<span class="o">)</span> <span class="o">[</span>XX<span class="o">]</span>:US
</span><span class='line'>State or Province Name <span class="o">(</span>full name<span class="o">)</span> <span class="o">[]</span>:CA
</span><span class='line'>Locality Name <span class="o">(</span>eg, city<span class="o">)</span> <span class="o">[</span>Default City<span class="o">]</span>:San Diego
</span><span class='line'>Organization Name <span class="o">(</span>eg, company<span class="o">)</span> <span class="o">[</span>Default Company Ltd<span class="o">]</span>:Customer2 SubCA
</span><span class='line'>Organizational Unit Name <span class="o">(</span>eg, section<span class="o">)</span> <span class="o">[]</span>:
</span><span class='line'>Common Name <span class="o">(</span>eg, your name or your server<span class="p">&amp;</span>rsquo<span class="p">;</span>s hostname<span class="o">)</span> <span class="o">[]</span>:subca.customer2.example
</span><span class='line'>Email Address <span class="o">[]</span>:&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Please enter the following <span class="p">&amp;</span>lsquo<span class="p">;</span>extra<span class="p">&amp;</span>rsquo<span class="p">;</span> attributes
</span><span class='line'>to be sent with your certificate request
</span><span class='line'>A challenge password <span class="o">[]</span>:
</span><span class='line'>An optional company name <span class="o">[]</span>:
</span></code></pre></td></tr></table></div></figure></p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl x509 -req -in subca.customer2.example.csr.pem -CA rootca.company.example.crt.pem -CAkey rootca.company.example.key.pem -CAserial rootca.srl -extfile /etc/pki/tls/openssl.cnf -extensions v3_ca -out subca.customer2.example.crt.pem
</span></code></pre></td></tr></table></div></figure></p>

<h3>Edge service certificate</h3>

<p>This is the first out of the three end-entity certificates. First, we will generate the private key:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl genrsa -out app.company.example.key.pem 2048
</span></code></pre></td></tr></table></div></figure></p>

<p>Next, we are going to create the CSR. Make sure you populate the <code>Common Name</code> field exactly as you can see below:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h2&gt;<span class="nv">$ </span>openssl req -new -key app.company.example.key.pem -out app.company.example.csr.pem&lt;/h2&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Country Name <span class="o">(</span><span class="m">2</span> letter code<span class="o">)</span> <span class="o">[</span>XX<span class="o">]</span>:US
</span><span class='line'>State or Province Name <span class="o">(</span>full name<span class="o">)</span> <span class="o">[]</span>:CA
</span><span class='line'>Locality Name <span class="o">(</span>eg, city<span class="o">)</span> <span class="o">[</span>Default City<span class="o">]</span>:San Diego
</span><span class='line'>Organization Name <span class="o">(</span>eg, company<span class="o">)</span> <span class="o">[</span>Default Company Ltd<span class="o">]</span>:Company
</span><span class='line'>Organizational Unit Name <span class="o">(</span>eg, section<span class="o">)</span> <span class="o">[]</span>:
</span><span class='line'>Common Name <span class="o">(</span>eg, your name or your server<span class="p">&amp;</span>rsquo<span class="p">;</span>s hostname<span class="o">)</span> <span class="o">[]</span>:app.company.example
</span><span class='line'>Email Address <span class="o">[]</span>:&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Please enter the following <span class="p">&amp;</span>lsquo<span class="p">;</span>extra<span class="p">&amp;</span>rsquo<span class="p">;</span> attributes
</span><span class='line'>to be sent with your certificate request
</span><span class='line'>A challenge password <span class="o">[]</span>:
</span><span class='line'>An optional company name <span class="o">[]</span>:
</span></code></pre></td></tr></table></div></figure></p>

<p>And finally, type this command to generate the certificate:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl x509 -req -in app.company.example.csr.pem -CA subca.company.example.crt.pem -CAkey subca.company.example.key.pem -CAcreateserial -out app.company.example.crt.pem
</span></code></pre></td></tr></table></div></figure></p>

<p>Note that we didn&rsquo;t include the <code>-extensions v3_ca</code> option as we wanted to create an end-entity certificate.</p>

<h3>Customer1 client certificate</h3>

<p>Now we are going to create a certificate for our first customer.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl genrsa -out customer1.example.key.pem 2048
</span></code></pre></td></tr></table></div></figure></p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h2&gt;<span class="nv">$ </span>openssl req -new -key customer1.example.key.pem -out customer1.example.csr.pem&lt;/h2&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Country Name <span class="o">(</span><span class="m">2</span> letter code<span class="o">)</span> <span class="o">[</span>XX<span class="o">]</span>:US
</span><span class='line'>State or Province Name <span class="o">(</span>full name<span class="o">)</span> <span class="o">[]</span>:MA
</span><span class='line'>Locality Name <span class="o">(</span>eg, city<span class="o">)</span> <span class="o">[</span>Default City<span class="o">]</span>:Boston
</span><span class='line'>Organization Name <span class="o">(</span>eg, company<span class="o">)</span> <span class="o">[</span>Default Company Ltd<span class="o">]</span>:Customer1
</span><span class='line'>Organizational Unit Name <span class="o">(</span>eg, section<span class="o">)</span> <span class="o">[]</span>:
</span><span class='line'>Common Name <span class="o">(</span>eg, your name or your server<span class="p">&amp;</span>rsquo<span class="p">;</span>s hostname<span class="o">)</span> <span class="o">[]</span>:customer1.example
</span><span class='line'>Email Address <span class="o">[]</span>:&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Please enter the following <span class="p">&amp;</span>lsquo<span class="p">;</span>extra<span class="p">&amp;</span>rsquo<span class="p">;</span> attributes
</span><span class='line'>to be sent with your certificate request
</span><span class='line'>A challenge password <span class="o">[]</span>:
</span><span class='line'>An optional company name <span class="o">[]</span>:
</span></code></pre></td></tr></table></div></figure></p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl x509 -req -in customer1.example.csr.pem -CA subca.customer1.example.crt.pem -CAkey subca.customer1.example.key.pem -CAcreateserial -out customer1.example.crt.pem
</span></code></pre></td></tr></table></div></figure></p>

<h3>Customer2 client certificate</h3>

<p>And finally, we are going to create a certificate for our second customer.</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl genrsa -out customer2.example.key.pem 2048
</span></code></pre></td></tr></table></div></figure></p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h2&gt;<span class="nv">$ </span>openssl req -new -key customer2.example.key.pem -out customer2.example.csr.pem&lt;/h2&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Country Name <span class="o">(</span><span class="m">2</span> letter code<span class="o">)</span> <span class="o">[</span>XX<span class="o">]</span>:US
</span><span class='line'>State or Province Name <span class="o">(</span>full name<span class="o">)</span> <span class="o">[]</span>:TX
</span><span class='line'>Locality Name <span class="o">(</span>eg, city<span class="o">)</span> <span class="o">[</span>Default City<span class="o">]</span>:Austin
</span><span class='line'>Organization Name <span class="o">(</span>eg, company<span class="o">)</span> <span class="o">[</span>Default Company Ltd<span class="o">]</span>:Customer2
</span><span class='line'>Organizational Unit Name <span class="o">(</span>eg, section<span class="o">)</span> <span class="o">[]</span>:
</span><span class='line'>Common Name <span class="o">(</span>eg, your name or your server<span class="p">&amp;</span>rsquo<span class="p">;</span>s hostname<span class="o">)</span> <span class="o">[]</span>:customer2.example
</span><span class='line'>Email Address <span class="o">[]</span>:&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;Please enter the following <span class="p">&amp;</span>lsquo<span class="p">;</span>extra<span class="p">&amp;</span>rsquo<span class="p">;</span> attributes
</span><span class='line'>to be sent with your certificate request
</span><span class='line'>A challenge password <span class="o">[]</span>:
</span><span class='line'>An optional company name <span class="o">[]</span>:
</span></code></pre></td></tr></table></div></figure></p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>openssl x509 -req -in customer2.example.csr.pem -CA subca.customer2.example.crt.pem -CAkey subca.customer2.example.key.pem -CAcreateserial -out customer2.example.crt.pem
</span></code></pre></td></tr></table></div></figure></p>

<p>If everything went well, your working directory should contain a collection of PKI files similar to this list:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>ls -1
</span><span class='line'>app.company.example.crt.pem
</span><span class='line'>app.company.example.csr.pem
</span><span class='line'>app.company.example.key.pem
</span><span class='line'>customer1.example.crt.pem
</span><span class='line'>customer1.example.csr.pem
</span><span class='line'>customer1.example.key.pem
</span><span class='line'>customer2.example.crt.pem
</span><span class='line'>customer2.example.csr.pem
</span><span class='line'>customer2.example.key.pem
</span><span class='line'>rootca.company.example.crt.pem
</span><span class='line'>rootca.company.example.key.pem
</span><span class='line'>rootca.srl
</span><span class='line'>subca.company.example.crt.pem
</span><span class='line'>subca.company.example.csr.pem
</span><span class='line'>subca.company.example.key.pem
</span><span class='line'>subca.customer1.example.crt.pem
</span><span class='line'>subca.customer1.example.csr.pem
</span><span class='line'>subca.customer1.example.key.pem
</span><span class='line'>subca.customer2.example.crt.pem
</span><span class='line'>subca.customer2.example.csr.pem
</span><span class='line'>subca.customer2.example.key.pem
</span><span class='line'>subca.srl
</span></code></pre></td></tr></table></div></figure></p>

<h2>Installing the edge service</h2>

<p>In our POC project, the role of the edge service will be played by the Apache server. To install the Apache server, issue the following command on the edge instance:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>yum install -y httpd
</span></code></pre></td></tr></table></div></figure></p>

<p>You can start the Apache server by typing:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>systemctl start httpd
</span></code></pre></td></tr></table></div></figure></p>

<p>The default static web page served by Apache is for our purposes a bit too long. Let&rsquo;s replace it with a simple, one line message:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span><span class="nb">echo</span> <span class="p">&amp;</span>lsquo<span class="p">;</span>It works!<span class="p">&amp;</span>rsquo<span class="p">;</span> &gt; /var/www/html/index.html
</span></code></pre></td></tr></table></div></figure></p>

<p>To verify that Apache was installed properly and is running, issue the command:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl localhost
</span><span class='line'>It works!
</span></code></pre></td></tr></table></div></figure></p>

<h2>Configuring the reverse proxy on the edge instance</h2>

<p>In this section, we are going to set up the reverse proxy on the edge instance. First, let&rsquo;s prepare two files which will be referred to from the HAProxy configuration. The file <code>app.crt</code> will include the edge service certificate along with the CA chain and the respective private key. It is used by HAProxy to authenticate itself to the clients. In the following sections, we will configure the client-side HAProxy to trust these certificates and hence verify that it is connecting to the correct service and not for example to a service of an attacker. To create the <code>app.crt</code> file, you can type:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>cat <span class="se">\</span>
</span><span class='line'>app.company.example.crt.pem <span class="se">\</span>
</span><span class='line'>subca.company.example.crt.pem <span class="se">\</span>
</span><span class='line'>rootca.company.example.crt.pem <span class="se">\</span>
</span><span class='line'>app.company.example.key.pem <span class="se">\&lt;</span>/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;blockquote&gt;&lt;p&gt;/etc/haproxy/ssl/app.crt
</span></code></pre></td></tr></table></div></figure></p></blockquote>

<p>The important task of the server-side HAProxy is to authenticate the incoming client connections. In our project, we are looking at two customers that should be allowed to access our application. For that, we&rsquo;re going to include the CA certificate chains of the two customers into the <code>customer-ca.crt</code> file:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>cat <span class="se">\</span>
</span><span class='line'>subca.customer1.example.crt.pem <span class="se">\</span>
</span><span class='line'>subca.customer2.example.crt.pem <span class="se">\</span>
</span><span class='line'>rootca.company.example.crt.pem <span class="se">\&lt;</span>/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;blockquote&gt;&lt;p&gt;/etc/haproxy/ssl/customer-ca.crt
</span></code></pre></td></tr></table></div></figure></p></blockquote>

<p>As the last step in this section, we are going to configure HAProxy. You can open the HAProxy configuration file <code>/etc/haproxy/haproxy.cfg</code> in your favorite editor and replace its content with:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>global
</span><span class='line'>  tune.ssl.default-dh-param 1024&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;defaults
</span><span class='line'>  timeout client 30s
</span><span class='line'>  timeout server 30s
</span><span class='line'>  timeout connect 5s&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;frontend proxy
</span><span class='line'>  <span class="nb">bind </span>0.0.0.0:443 ssl crt /etc/haproxy/ssl/app.crt ca-file /etc/haproxy/ssl/customer-ca.crt verify required
</span><span class='line'>  mode http&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  use_backend edge_customer1 <span class="k">if</span> <span class="o">{</span> ssl_c_s_dn<span class="o">(</span>cn<span class="o">)</span> -i customer1.example <span class="o">}</span>
</span><span class='line'>  use_backend edge_customer2 <span class="k">if</span> <span class="o">{</span> ssl_c_s_dn<span class="o">(</span>cn<span class="o">)</span> -i customer2.example <span class="o">}</span>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;backend edge_customer1
</span><span class='line'>  mode http
</span><span class='line'>  server customer1 127.0.0.1:80 check&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;backend edge_customer2
</span><span class='line'>  mode http
</span><span class='line'>  server customer2 127.0.0.1:80 check
</span></code></pre></td></tr></table></div></figure></p>

<p>This is a minimalist configuration file, good enough for our proof of concept. HAProxy is going to listen on port 443 for incoming TLS connections. It will present the edge service certificate to the clients. At the same time, it will only accept connections from the clients sending the Customer1 or Customer2 certificate. Based on the <code>Common Name</code> field of the client&rsquo;s certificate, HAProxy will forward the request to the respective backend. In our case, both customers will be served by the same service listening on 127.0.0.1:80, however, you can imagine that in the real scenario there would be two separate edge services perhaps running on two different machines.</p>

<p>You can start the HAProxy service using the following command:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>systemctl restart haproxy
</span></code></pre></td></tr></table></div></figure></p>

<p>Before moving on, check the HAProxy logs. If everything worked well, there should be no errors or warnings:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>journalctl -u haproxy -e
</span></code></pre></td></tr></table></div></figure></p>

<h2>Configuring the proxy on the client instance</h2>

<p>In this section, we&rsquo;re going to turn our attention to the client instance. First, we&rsquo;ll need to copy some of the PKI keys and certificates from the edge instance to the client instance. I configured SSH between the two instances, so that I can issue the following command on the edge instance to copy the files to the client instance:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>scp <span class="se">\</span>
</span><span class='line'>customer1.example.crt.pem <span class="se">\</span>
</span><span class='line'>customer1.example.key.pem <span class="se">\</span>
</span><span class='line'>customer2.example.crt.pem <span class="se">\</span>
</span><span class='line'>customer2.example.key.pem <span class="se">\</span>
</span><span class='line'>subca.company.example.crt.pem <span class="se">\</span>
</span><span class='line'>rootca.company.example.crt.pem <span class="se">\</span>
</span><span class='line'>ip-172-31-33-109.us-west-2.compute.internal:
</span></code></pre></td></tr></table></div></figure></p>

<p>In the above command, make sure that you replace the target host name <code>ip-172-31-33-109.us-west-2.compute.internal</code> with the DNS name of your client instance.</p>

<p>On the client instance, we are going to add a line to the <code>/etc/hosts</code> file which will make the DNS name <code>app.company.example</code> resolve to the IP address of the edge instance. In the following command, replace the IP address with the IP address of your edge instance:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span><span class="nb">echo </span>172.31.44.105 app.company.example &gt;&gt; /etc/hosts
</span></code></pre></td></tr></table></div></figure></p>

<p>Let&rsquo;s check that the TLS client is able to connect to the edge service. On the client instance, type:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>cacert rootca.company.example.crt.pem <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>cert ./customer1.example.crt.pem <span class="se">\</span>
</span><span class='line'><span class="p">&amp;</span>ndash<span class="p">;</span>key ./customer1.example.key.pem <span class="se">\</span>
</span><span class='line'>&lt;a <span class="nv">href</span><span class="o">=</span><span class="s2">&quot;https://app.company.example&quot;</span>&gt;https://app.company.example&lt;/a&gt;
</span><span class='line'>It works!
</span></code></pre></td></tr></table></div></figure></p>

<p>Excellent! We&rsquo;ve just verified that the client with the built-in TLS support is able to successfully connect to our edge service and authenticate itself as Customer1.</p>

<p>The ultimate goal of this tutorial was to allow a client without TLS support to access the edge service, too. In order to achieve this goal, we&rsquo;re going to set up a client-side HAProxy. First, let&rsquo;s prepare two files that will be needed by HAProxy. The <code>customer1.crt</code> file enables HAProxy to authenticate itself to the edge service as Customer1. You can create this file by issuing the command:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>cat <span class="se">\</span>
</span><span class='line'>customer1.example.crt.pem <span class="se">\</span>
</span><span class='line'>customer1.example.key.pem <span class="se">\&lt;</span>/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;blockquote&gt;&lt;p&gt;/etc/haproxy/ssl/customer1.crt
</span></code></pre></td></tr></table></div></figure></p></blockquote>

<p>Second file allows HAProxy to verify the authenticity of the edge service. It includes a chain of CA certificates, against which the certificate presented by the edge service will be verified. You can create the <code>company-ca.crt</code> file using the following command:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>cat <span class="se">\</span>
</span><span class='line'>subca.company.example.crt.pem <span class="se">\</span>
</span><span class='line'>rootca.company.example.crt.pem <span class="se">\&lt;</span>/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;blockquote&gt;&lt;p&gt;/etc/haproxy/ssl/company-ca.crt
</span></code></pre></td></tr></table></div></figure></p></blockquote>

<p>And finally, we are going to configure the client-side HAProxy. On the client instance, open the file <code>/etc/haproxy/haproxy.cfg</code>  in your favorite editor and replace its content with the following configuration:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>global
</span><span class='line'>  tune.ssl.default-dh-param 1024&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;defaults
</span><span class='line'>  timeout client 30s
</span><span class='line'>  timeout server 30s
</span><span class='line'>  timeout connect 5s&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;frontend proxy
</span><span class='line'>  <span class="nb">bind </span>0.0.0.0:80
</span><span class='line'>  mode http&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  use_backend app&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;backend app
</span><span class='line'>  mode http
</span><span class='line'>  server app.company.example app.company.example:443 check ssl verify required ca-file /etc/haproxy/ssl/company-ca.crt crt /etc/haproxy/ssl/customer1.crt
</span></code></pre></td></tr></table></div></figure></p>

<p>HAProxy will listen on port 80 for the incoming HTTP connections. For each HTTP connection it will open a secure HTTPS connection to the edge service and it will pass the data back and forth between the two connections. You can start the HAProxy on the client instance by typing:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>systemctl restart haproxy
</span></code></pre></td></tr></table></div></figure></p>

<p>Double-check that there are no warnings or errors in the HAProxy logs:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>journalctl -u haproxy -e
</span></code></pre></td></tr></table></div></figure></p>

<p>If everything went well, you should be able to connect to the edge service using a non-TLS client. Here we go:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl localhost
</span><span class='line'>It works!
</span></code></pre></td></tr></table></div></figure></p>

<p>To verify that our client is recognized by the cloud application as Customer1, you can comment out the line <code>use_backend edge_customer1 if ...</code> in the <code>/etc/haproxy/haproxy.cfg</code> file on the edge instance. Remember to restart HAProxy after you modified the configuration. The repeated test from the client instance proves that indeed there&rsquo;s no backend for the Customer1 available anymore:</p>

<p><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="nv">$ </span>curl localhost
</span><span class='line'>&lt;html&gt;&lt;body&gt;&lt;h1&gt;503 Service Unavailable&lt;/h1&gt;
</span><span class='line'>No server is available to handle this request.
</span><span class='line'>&lt;/body&gt;&lt;/html&gt;
</span></code></pre></td></tr></table></div></figure></p>

<h2>Conclusion and final remarks</h2>

<p>In this blog post, we established a secure communication channel between the client and our application running in the cloud. As demonstrated, our approach is also suitable for client applications that don&rsquo;t support TLS.</p>

<p>In our example, the client certificate authentication was carried out by HAProxy and the edge service sitting behind the proxy didn&rsquo;t get any information about the client. To improve the design, HAProxy could be configured to forward the attributes of the certificate presented by the client to the backend, by setting them as HTTP request headers. HAProxy even allows to insert the entire client certificate into a request header for the backend.</p>

<p>Our server presented a certificate that was signed by our own CA. In practice, we would deploy a certificate signed by a trusted third-party CA. AWS provides <a href="https://aws.amazon.com/certificate-manager">AWS Certificate Manager</a> (ACM) service to generate certificates for ELBs, API Gateway and other AWS services. Unfortunately, we cannot utilize this service for our use case as it doesn&rsquo;t provide access to the private key. Instead, we can purchase a certificate from one of the trusted certificate authorities or leverage the free of charge <a href="https://letsencrypt.org/">Let&rsquo;s Encrypt</a> certificate authority.</p>

<p>This concludes our miniseries about edge security for cloud applications. We are still evaluating the proposed approach. What do you think about it? If you have any feedback, please, feel free to add your comments in the comment section below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Edge Security for Your Cloud Application, Part I]]></title>
    <link href="http://alesnosek.com/blog/2018/01/10/edge-security-for-your-cloud-application-part-i/"/>
    <updated>2018-01-10T23:09:17-08:00</updated>
    <id>http://alesnosek.com/blog/2018/01/10/edge-security-for-your-cloud-application-part-i</id>
    <content type="html"><![CDATA[<p>When designing a cloud application, one of the challenges you need to tackle is the secure communication of your clients with your servers over the public Internet. In this post, I&rsquo;m going to sketch a system architecture that allows you to authenticate the clients and exchange data between your clients and servers securely. The proposed architecture is also suitable when migrating an existing application, which didn&rsquo;t implement the secure communication, to the cloud.</p>

<!-- more -->


<p>In this article, we are going to assume that we&rsquo;re dealing with a multi-tenant cloud application whose architecture follows the <a href="https://en.wikipedia.org/wiki/Service-oriented_architecture">SOA</a> architecture principles. The application consists of several edge services that accept the incoming client requests. After validating the incoming request, the request is passed to the backend services for processing. Here is a diagram depicting such an application:</p>

<p><img src="/images/posts/edge_security_for_your_cloud_application_soa.svg" width="800" height="1000" title="SOA architecture" ></p>

<h2>Requirements</h2>

<p>Before we start walking through the design, let&rsquo;s take a look at the set of requirements that drove the design decisions:</p>

<ol>
<li><p>One of the first steps of request validation should be the client authentication. Requests originating from the clients that fail to authenticate should be refused immediately.</p></li>
<li><p>We don&rsquo;t impose any restrictions on what protocol is used by the clients to communicate with our application. Typically, clients will leverage the HTTP protocol but we want to support any TCP based protocol. In other words, our application doesn&rsquo;t have to be a web service.</p></li>
<li><p>From the past, we inherited some clients that are not able to communicate in a secure way. For instance, they are only able to talk plain HTTP and don&rsquo;t support HTTPS. We would like to make such clients work with our application without the need to modify the clients themselves.</p></li>
<li><p>And last but not least, we would like to avoid the need to establish a VPN connection between the client machine and the cloud. In practice, setting up a VPN connection requires quite invasive configuration of the client operating system. For our application tenants (our customers) this would impose a barrier that would hurt the adoption of our cloud application. Something we definitely don&rsquo;t want.</p></li>
</ol>


<h2>The edge layer</h2>

<p>SSL/TLS is a family of security protocols that many VPNs rely upon. Currently, the TLS protocol is considered to be one of the strongest and most mature security protocols available. It was a clear choice for us to leverage the TLS protocol for communication between the clients and servers including the mutual authentication using PKI certificates. Finally, here is a diagram showing the edge layer of our cloud application in detail:</p>

<p><img src="/images/posts/edge_security_for_your_cloud_application_server.svg" width="800" height="1000" title="The edge layer" ></p>

<h2>Internet-facing ELB</h2>

<p>Interestingly, AWS Elastic load balancers don&rsquo;t support client certificate authentication. Both, Classic load balancer and Application load balancer, support TLS offloading where they can terminate the TLS connection for you. However, they are not able to authenticate the client certificate and they also don&rsquo;t allow forwarding the certificate details to the backend application which could carry out the authentication by itself. In order to implement the client certificate authentication, you are pretty much left with two options: you can either use the Classic load balancer in the TCP mode, or you can employ the Network load balancer which operates on the TCP level. In both cases, the Elastic load balancer just passes the TLS connection through to your EC2 instances where you have to terminate and authenticate the TLS connection yourself. As the connection between ELB and the EC2 instances remains encrypted, you are gaining a plus from the security standpoint, too.</p>

<p>While ELB doesn&rsquo;t authenticate clients, it plays an important role in our architecture. First, ELB is highly-available and distributes the traffic to several edge instances that are not highly-available. Second, ELB implements a layer 3 (e.g. UDP reflection) and layer 4 (e.g. SYN flood) DDOS attack protection.</p>

<p>Btw., Amazon API Gateway doesn&rsquo;t support client certificate authentication either and so is less helpful for our scenario.</p>

<h2>Reverse proxy on edge instances</h2>

<p>Depending on your architecture, your edge services may or may not have a support for accepting TLS connections built in. I personally see the connection security to be a job for the infrastructure and would vote against implementing the TLS support in your application services. Here are some concrete arguments why to terminate the TLS connection outside of your application services:</p>

<ol>
<li><p>Your application services may be written in different languages. For each language and its runtime, the TLS configuration is different. You don&rsquo;t want to spend your time figuring out, how to configure TLS on Apache Tomcat, Eclipse Jetty, Apache server, Node.js and others, do you? Different technologies support different TLS features. For example, TLS SNI is supported only in Tomcat >= 8.5. Researching the supported feature set of every runtime is time consuming.</p></li>
<li><p>The proxy provides a good place to monitor and log what&rsquo;s going on on the wire. Remember that ELB won&rsquo;t have any insight into your traffic as the traffic is TLS encrypted.</p></li>
<li><p>You may want to automate the TLS certificate rotation and revocation. You&rsquo;ll have to implement this for all your different web servers. That&rsquo;s quite a bit of work.</p></li>
</ol>


<p>Instead of handling TLS in your application services, I would recommend deploying a reverse proxy in front of your services. This proxy will terminate the incoming TLS connection. This will allow you to manage all the TLS related settings in one place. The battle-tested proxies like <a href="http://www.haproxy.org/">HAProxy</a> or <a href="https://www.nginx.com/">NGINX</a> can authenticate the client certificate against a trusted certificate authority. They can also forward the certificate details to your edge service which can implement a more complex authentication logic if you need it.</p>

<p>You should deploy the reverse proxy on the same instance with your edge service. This way, the decrypted communication between the proxy and your service will never leave the instance. In order to capture the decrypted traffic, one would need to have a root access on the instance. The possibility to capture the communication in the clear will come handy when troubleshooting, though.</p>

<h2>The client side</h2>

<p>After discussing the server-side design, let&rsquo;s talk about the client-side part of the picture. As we mentioned in the requirements section above, our architecture has to also support clients that don&rsquo;t have the TLS functionality built in. In turns out that there are actually three different client types:</p>

<ol>
<li>Clients with native TLS mutual authentication support</li>
<li>Clients with no TLS support at all</li>
<li>Clients supporting TLS with one-way authentication only, i.e. the client is able to authenticate the server certificate but it doesn&rsquo;t present its own certificate to the server.</li>
</ol>


<p>There is nothing special to do for the clients with native TLS mutual authentication support. They can directly connect to the application servers. To ensure the secure communication for the remaining two client types, we&rsquo;re going to deploy a proxy on the client machines. This proxy will enforce the TLS mutually authenticated connection over the Internet. A diagram depicting the three client types looks as follows:</p>

<p><img src="/images/posts/edge_security_for_your_cloud_application_client.svg" width="600" height="800" title="The client side" ></p>

<p>In the case number two, the proxy is configured in the SSL/TLS encryption mode. It accepts an unencrypted connection from the client and forwards the communication over a TLS encrypted channel to the server.  For the case number three, the proxy must be configured using the SSL/TLS bridging aka re-encryption mode. The proxy accepts an encrypted connection from the client, creates a separate encrypted connection to the server and passes the data between the two connections. Both HAProxy and Nginx support these configuration modes. You can also refer to the TLS layouts described in the HAProxy <a href="https://www.haproxy.com/documentation/aloha/7-0/deployment-guides/tls-layouts/">documentation</a>.</p>

<p>Alternatively, as a proxy one could also employ <a href="https://www.stunnel.org">stunnel</a>. Before the SSL/TLS support was built into HAProxy, stunnel used to be deployed along with HAProxy to provide the SSL/TLS functionality. To accomplish the case number three using stunnel, one would actually need to combine two stunnels in series.</p>

<p>Recently, a modern <a href="https://www.envoyproxy.io/">Envoy</a> proxy emerged and I would like to encourage you to check it out. It provides a really impressive set of features that goes far beyond the load balancing functionality: automatic retries, circuit breaking, zone local load balancing, very detailed metrics etc. Envoy helps to solve several networking problems common to the cloud-native applications. In our proposed architecture, Envoy would be deployed in the role of an edge proxy on the server side as well as in the role of the service proxy on the client side.</p>

<p>Whichever proxy you choose on the client-side, remember to deploy it in a highly available fashion. You don&rsquo;t want to introduce a single point of failure into your system, do you?</p>

<h2>Conclusion</h2>

<p>In this post, we walked through the secure edge design explaining the reasoning behind the individual design decisions. In the <a href="/blog/2018/01/12/edge-security-for-your-cloud-application-part-ii">second blog post</a> of this miniseries, we&rsquo;re going to demonstrate a practical implementation of our approach using HAProxy.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Evaluating Application Metrics Solutions - Prometheus?]]></title>
    <link href="http://alesnosek.com/blog/2017/09/10/evaluating-application-metrics-solutions-prometheus/"/>
    <updated>2017-09-10T16:17:49-07:00</updated>
    <id>http://alesnosek.com/blog/2017/09/10/evaluating-application-metrics-solutions-prometheus</id>
    <content type="html"><![CDATA[<p>In our <a href="https://en.wikipedia.org/wiki/Service-oriented_architecture">SOA-based</a> application, the problem of application metrics hasn&rsquo;t been solved yet. We would like to have our application services expose metrics that could be used for monitoring, auto-scaling and analytics. In this blog post, I would like to present to you one of the proposals to solve the application metrics which suggests leveraging <a href="https://prometheus.io/">Prometheus</a>.</p>

<!-- more -->


<p>Our application consists of multiple services that are deployed on multiple machines. Currently, our applicaton is deployed on-premise or as a managed offering on the virtual machines in AWS. We&rsquo;re also working on containerizing the application services to achieve higher density and better manageability when deploying into the AWS cloud. Some of our services are so light-weight that we&rsquo;re going to turn them into Lambda functions in the future to further reduce the operational costs. Thus, a solution for application metrics should be able to work in the serverless environment, too.</p>

<p>As of now, we deploy <a href="https://www.icinga.com/">Icinga</a> along with our application to provide system-level monitoring. Icinga collects the information about the nodes and checks that our services are still running. However, we don&rsquo;t collect any application-level metrics that would allow us to better assess the performance of our system. For example, we would like to know how many requests are processed per second, average request latency, request error rate, what are the depths of the internal queues and so forth. Application metrics would be a welcome input to the auto-scaling decisions and we would like to feed them into our analytics engine as well.</p>

<h2>Getting to know Prometheus</h2>

<p><img class="right" src="/images/posts/prometheus_logo.png" width="130" height="130"></p>

<p>Before jumping in and implementing our own solution for metrics collection and perhaps reinventing the wheel we started shopping around. It seemed to us, that Prometheus monitoring solution was gaining a lot of momentum in recent times. So, we took a closer look at Prometheus and this is what we found:</p>

<ol>
<li>Prometheus is an open-source monitoring solution hosted by <a href="https://www.cncf.io/">CNCF</a> - a foundation that hosts Kubernetes as well. Many companies use and contribute to Prometheus.</li>
<li>The <a href="https://prometheus.io/docs/introduction/overview/">architecture</a> of Prometheus is easy to understand and is modular. While Prometheus provides modules for metrics collection, alerts and Web UI, we would not have to use all of them.</li>
<li>Prometheus is a pull-based monitoring system. Each monitored target has to expose Prometheus formatted metrics. By default, targets make the metrics endpoint available at <a href="http://target/metrics.">http://target/metrics.</a> Prometheus periodically scrapes the metrics exposed by the targets.</li>
<li>Our services would need to expose the application metrics in the <a href="https://prometheus.io/docs/instrumenting/exposition_formats/">Prometheus format</a>. There are actually two formats available: a simple text format and protobufs. There are instrumentation <a href="https://prometheus.io/docs/instrumenting/clientlibs/">libraries</a> for Java, C++ and other languages, to gather the metrics and expose them in the Prometheus format.</li>
<li>The text-based Prometheus metrics format is so simple that it could be collected by other monitoring systems like Nagios or Icinga. Exposing metrics in the Prometheus format doesn&rsquo;t really mandate using Prometheus server for monitoring.</li>
<li>There&rsquo;s a Prometheus <a href="https://github.com/prometheus/jmx_exporter">jmx_exporter</a> library to convert the JMX MBeans data into Prometheus format. This would come in handy for gathering Tomcat metrics, for example.</li>
<li><a href="http://metrics.dropwizard.io/">Dropwizard metrics</a> is a popular Java instrumentation library. For instance, Vert.x toolkit can report its <a href="http://vertx.io/docs/vertx-dropwizard-metrics/java/">internal metrics</a> using the Dropwizard metrics library and there are other frameworks that supports it. Prometheus comes with a <a href="https://github.com/prometheus/client_java/tree/master/simpleclient_dropwizard">simpleclient_dropwizard</a> library that can make Dropwizard metrics available to Prometheus monitoring.</li>
<li>To prevent unauthorized access, the metric targets would need to be protected using TLS in combination with client certs, bearer token or HTTP basic authentication.</li>
<li>Prometheus pulls the metrics from the monitored targets. In addition, Prometheus comes with a <a href="https://prometheus.io/docs/practices/pushing/">Pushgateway</a> where clients can push their metrics to. However, as noted in the Prometheus documentation: <em>Usually, the only valid use case for the Pushgateway is for capturing the outcome of a service-level batch job</em>. Hence, Pushgateway would not work for aggregating metrics pushed by the Lambda functions.</li>
<li>In addition to application-level metrics, system-level metrics can be collected by Prometheus as well thanks to the <a href="https://github.com/prometheus/node_exporter">node_exporter</a>.</li>
<li>Prometheus is a great fit for dynamic environments like clouds and container clusters due to its discovery capabilities. In AWS, operator attaches tags to VMs and based on that Prometheus can discover them and start monitoring them automatically. The same principle works for container clusters like Kubernetes, too. One has to add annotations to pods and Prometheus will discover them automatically.</li>
<li>Prometheus makes the collected metrics available for querying via an <a href="https://prometheus.io/docs/querying/api/">HTTP API</a>. We could retrieve the metrics using this API in order to feed them into our analytics engine.</li>
<li>There is a great <a href="https://prometheus.io/docs/practices/naming/">guide</a> that would help us when designing our custom metrics.</li>
<li>Prometheus is written in Go and comes in a form of statically-linked binaries. This makes the installation of Prometheus a breeze.</li>
</ol>


<h2>Instrumenting Java applications</h2>

<p>In order to gather application metrics and to make them available to the Prometheus monitoring system, we would need to instrument our application services using Prometheus libraries. To get a clear idea, we created a proof-of-concept Java application instrumented using Prometheus. You can find it on <a href="https://github.com/noseka1/prometheus-poc">GitHub</a>.</p>

<p>Alternatively, we are thinking about leveraging Dropwizard metrics library for instrumentation. The Dropwizard metrics library is rather popular and is not connected with any particular monitoring solution. We would still be able to expose the Dropwizard metrics to Prometheus using a wrapper <a href="https://github.com/prometheus/client_java/tree/master/simpleclient_dropwizard">simpleclient_dropwizard</a>.</p>

<h2>Monitoring AWS Lambda functions</h2>

<p>AWS Lambda functions are extremely short-lived processes. Prometheus won&rsquo;t be able to pull the application metrics from them. Instead, Lambdas will have to push their metrics to Prometheus. At the first glance, we thought that the Prometheus Pushgateway could help here, however, reading the Pushgateway&rsquo;s documentation more carefully we found that <em>the Pushgateway is explicitly not an aggregator or distributed counter but rather a metrics cache</em>. And that&rsquo;s a problem, as we would like to count how many Lambda instances are being invoked per second and so on.</p>

<p>At the moment, we can see two approaches how to make the monitoring of Lambda functions work with Prometheus. Either, push the application metrics from the Lambda functions using a StatsD client. Prometheus&#8217; <a href="https://github.com/prometheus/statsd_exporter">statsd_exporter</a> would play a role of a StastD server and make the metrics available to Prometheus. Or, the second approach would be to create our own metrics aggregator that would receive the metrics from Lambda functions in the Prometheus format, aggregate them and expose them to the Prometheus server.</p>

<h2>Alternatives</h2>

<p>Besides using Prometheus, we were also thinking about other solutions for application metrics. As we already deploy Icinga for the system-level monitoring, it would make sense to use it for application metrics, too. We really like Icinga, it&rsquo;s a great monitoring software. Unfortunately, Icinga is based on the node and services model where a statically configured set of nodes are running services on them. This doesn&rsquo;t really fit with the modern containerized deployments where containers are dynamically scheduled on the cluster nodes and are also scaled up and down. Also, Prometheus server supports all sorts of metric queries and aggregations. Icinga is lacking this feature altogether. That&rsquo;s why we&rsquo;re leaning towards replacing Icinga with Prometheus for system-level as well as application-level monitoring.</p>

<p><a href="http://www.hawkular.org/">Hawkular</a> seems to be another modern monitoring project we would like to take a closer look at. In contrast to Prometheus project which is developed by many parties, it seems that Hawkular project is mostly driven by Red Hat.</p>

<h2>Conclusion</h2>

<p>Prometheus is a modern monitoring system. It was the first system we evaluated as we were trying to find a good solution for application metrics. In addition to application-level metrics, we could use Prometheus to collect system-level metrics as well. This would make Prometheus a single monitoring solution for our application. The only bigger issue for us is the absence of the AWS Lambda monitoring story.</p>

<p>If you have an application that you deliver on-premise as well as in the cloud, how did you solve the application metrics collection and monitoring? Is Prometheus a good way to go? Please, leave your comments below.</p>
]]></content>
  </entry>

  <entry>
    <title type="html"><![CDATA[Ansible Certification EX407]]></title>
    <link href="http://alesnosek.com/blog/2017/09/03/ansible-certification-ex407/"/>
    <updated>2017-09-03T22:47:06-07:00</updated>
    <id>http://alesnosek.com/blog/2017/09/03/ansible-certification-ex407</id>
    <content type="html"><![CDATA[<p>Last week I passed the Red Hat certification exam <a href="https://www.redhat.com/en/services/training/ex407-red-hat-certificate-expertise-ansible-automation">EX407 Red Hat Certificate of Expertise in Ansible Automation</a>. In this blog post, I&rsquo;d like to share some of my experience with you.</p>

<!-- more -->


<p><img class="right" src="/images/posts/ansible_logo.png" width="80" height="80"></p>

<p><img class="right" src="/images/posts/redhat_logo.png" width="130" height="130"></p>

<p>In addition to the Ansible certification, Red Hat offers a certification for Puppet as well, <a href="https://www.redhat.com/en/services/training/ex405-red-hat-certificate-expertise-configuration-management-puppet">EX405 Red Hat Certificate of Expertise in Configuration Management with Puppet</a>. I was using Puppet in the years 2008/2009. Back then, Puppet was a state of the art configuration management tool that did a great job for us. However, later on Ansible showed up and I quickly realized that the problems that we were solving with Puppet could have been more easily solved with Ansible. In our company, we switched from Puppet to Ansible in 2014 and have never looked back. As I gained much more experience using Ansible, I decided to go for the Ansible certification. For you, perhaps the Puppet certification would be more interesting.</p>

<p>It is the way of testing, that makes the Red Hat certification exams so enjoyable for me. Red Hat certification exams are purely practical. In the exam, you&rsquo;ll be given a list of requirements and an access to one or more virtual machines. Your goal is to configure the virtual machines based on the requirements.</p>

<p>The Ansible exam focused on writing Ansible playbooks, working with Ansible inventories including dynamic inventories, running ad-hoc Ansible commands, leveraging Ansible facts, creating Jinja2 templates, error handling, playbook tags, downloading a role from Ansible Galaxy and using Ansible vault to secure passwords.</p>

<p>To prepare for the exam, I used the online course Automation with Ansible I (DO407R) that is included in my <a href="https://www.redhat.com/en/services/training/learning-subscription">Red Hat Learning Subscription</a>.</p>

<p>Overall, I didn&rsquo;t find the exam too difficult. There were 4 hours of time provided for the exam. I was able to complete all my tasks 1 hour and 20 minutes before the exam end while reaching the maximum score of 300 points.</p>

<p>If you&rsquo;ve kept reading my blog post until this point, here are three basic exam tips for you:</p>

<ol>
<li>Use the <code>ansible-playbook --limit</code> parameter, to try out your playbook against a single host first before applying it to all your hosts.</li>
<li>Make sure you can run Ansible modules as ad-hoc commands. They can come handy when you want to undo the effects of running an erroneous playbook.</li>
<li>Use <code>ansible-doc --list</code> to look up a module that can achieve your goal and <code>ansible-doc &lt;module&gt;</code> to learn about the module parameters and example usage.</li>
</ol>


<p>Passing the Ansible exam brought me to the rank of Red Hat Certified Architect Level II. The current list of my Red Hat certifications can be found on the <a href="https://www.redhat.com/rhtapps/certification/verify/?certId=160-216-727">Verify a Red Hat Certified Professional</a> website.</p>

<p>Are you working on your Red Hat certification and would like to share your experience? I would always like to hear from you! Feel free to leave your comments below.</p>
]]></content>
  </entry>

</feed>
